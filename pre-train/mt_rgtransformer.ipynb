{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/datadrive/data_cs/'\n",
    "batch_size = 128\n",
    "batch_num  = 128\n",
    "epoch_num  = 1000\n",
    "samp_num   = 128 - 1\n",
    "\n",
    "device = torch.device(\"cuda:2\")\n",
    "graph = dill.load(open(data_dir + 'graph.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = {t: True for t in graph.times if t != None and t < 2015}\n",
    "valid_range = {t: True for t in graph.times if t != None and t >= 2015  and t <= 2016}\n",
    "test_range  = {t: True for t in graph.times if t != None and t > 2016}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_dim, n_hid, num_types, num_relations, n_heads, n_layers, dropout = 0.3):\n",
    "        super(GNN, self).__init__()\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.num_types = num_types\n",
    "        self.in_dim    = in_dim\n",
    "        self.n_hid     = n_hid\n",
    "        self.aggregat_ws   = nn.ModuleList()\n",
    "        self.drop          = nn.Dropout(dropout)\n",
    "        for t in range(num_types):\n",
    "            self.aggregat_ws.append(nn.Linear(in_dim, n_hid))\n",
    "        for l in range(n_layers):\n",
    "            self.gcs.append(RAGCNConv(n_hid, n_hid, num_types, num_relations, n_heads, dropout))\n",
    "    def set_device(self, device):\n",
    "        self.device = device\n",
    "        for gc in self.gcs:\n",
    "            gc.device = device\n",
    "    def forward(self, node_feature, node_type, edge_time, edge_index, edge_type):\n",
    "        res = torch.zeros(node_feature.size(0), self.n_hid).to(node_feature.device)\n",
    "        for t_id in range(self.num_types):\n",
    "            aggregat_w = self.aggregat_ws[t_id]\n",
    "            idx = (node_type == t_id)\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            res[idx] = torch.tanh(aggregat_w(node_feature[idx]))\n",
    "        meta_xs = self.drop(res)\n",
    "        del res\n",
    "        for gc in self.gcs:\n",
    "            meta_xs = gc(meta_xs, node_type, edge_index, edge_type, edge_time)\n",
    "        return meta_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sample(size, num, pos_id):\n",
    "    res = {}\n",
    "    while len(res) != num:\n",
    "        s = np.random.choice(size)\n",
    "        if s in res or s in pos_id:\n",
    "            continue\n",
    "        res[s] = True\n",
    "    return list(res.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_loss(gnn, predictor, rem_edges, node_feature, node_type, node_time, edge_index, edge_type, node_dict, samp_num, target_size):\n",
    "    positive_target_ids, source_ids = rem_edges[:,0].reshape(-1, 1), rem_edges[:,1] + node_dict[source_type][0]\n",
    "    negative_target_ids = np.array([neg_sample(target_size, pos_id, samp_num) for pos_id in positive_target_ids])\n",
    "    target_ids = np.concatenate((positive_target_ids, negative_target_ids), axis=-1) + node_dict[target_type][0]\n",
    "\n",
    "    node_emb = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                node_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "    source_emb = node_emb[source_ids].repeat(1, target_ids.shape[1]\\\n",
    "                                      ).view(target_ids.shape[0] * target_ids.shape[1], -1)\n",
    "\n",
    "    target_emb = node_emb[target_ids].view(target_ids.shape[0] * target_ids.shape[1], -1)\n",
    "    res = predictor.forward(source_emb, target_emb).view(-1, target_ids.shape[1])\n",
    "    return -torch.log_softmax(res, dim=-1)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_mask(task_set, feature, time, edge_list, batch_size):\n",
    "    store_edges = copy.deepcopy(edge_list)\n",
    "    rem_lists   = []\n",
    "    neg_nums    = []\n",
    "    for target_type, source_type, rel_type in task_set:\n",
    "        edges = np.array(edge_list[target_type][source_type][rel_type])\n",
    "        remn  = min(len(edges)-1, batch_size)\n",
    "        rem_ids = np.random.choice(np.arange(len(edges)), remn, replace = False)\n",
    "        ori_ids = np.array([i for i in range(len(edges)) if i not in rem_ids])\n",
    "        ori_edges = edges[ori_ids]\n",
    "        rem_lists += [edges[rem_ids]]\n",
    "        neg_nums  += [len(time[target_type])]\n",
    "        edge_list[target_type][source_type][rel_type] = list(ori_edges)\n",
    "        edge_list[source_type][target_type]['rev_' + rel_type] = list(np.stack((ori_edges[:,1], ori_edges[:,0])).T)\n",
    "    node_feature, node_type, node_time, edge_index, edge_type, node_dict, _ = to_torch(feature, time, edge_list, graph)\n",
    "    del edge_list\n",
    "    edge_list = store_edges\n",
    "    return neg_nums, rem_lists, node_feature, node_type, node_time, edge_index, edge_type, node_dict\n",
    "def mt_sample(seed, time_range, task_set, sampled_depth = 3, sampled_number = 100, batch_size = batch_size):\n",
    "    np.random.seed(seed)\n",
    "    train_feature, train_time, train_edge_list, _ = \\\n",
    "            sample_subgraph(graph, time_range=train_range, sampled_depth = sampled_depth, sampled_number = sampled_number)\n",
    "    neg_nums, rem_lists, node_feature, node_type, node_time, edge_index, edge_type, node_dict = \\\n",
    "        multi_mask(task_set, train_feature, train_time, train_edge_list, batch_size)\n",
    "    return neg_nums, rem_lists, node_feature, node_type, node_time, edge_index, edge_type, node_dict\n",
    "\n",
    "def prepare_data(pool, process_ids, task_set):\n",
    "    jobs = []\n",
    "    for process_id in process_ids[:-1]:\n",
    "        p = pool.apply_async(mt_sample, args=(np.random.randint(2**32 - 1), train_range, task_set))\n",
    "        jobs.append(p)\n",
    "    p = pool.apply_async(mt_sample, args=(np.random.randint(2**32 - 1), valid_range, task_set))\n",
    "    jobs.append(p)\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = graph.get_types()\n",
    "cand_list = list(graph.edge_list['field']['paper']['PF_in_L1'])\n",
    "gnn = GNN(in_dim = len(graph.node_feature['paper']['emb'][0]) + 401, n_hid = 256, num_types = len(types), \\\n",
    "          num_relations = len(graph.get_meta_graph()) + 1, n_heads = 8, n_layers = 3).to(device)\n",
    "fp_predictor = Matcher(256, 8).to(device)\n",
    "vp_predictor = Matcher(256, 8).to(device)\n",
    "pp_predictor = Matcher(256, 8).to(device)\n",
    "ap_predictor = Matcher(256, 8).to(device)\n",
    "ai_predictor = Matcher(256, 8).to(device)\n",
    "model = nn.Sequential(gnn, fp_predictor, vp_predictor, pp_predictor, ap_predictor, ai_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 1000, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stats = []\n",
    "models = [fp_predictor, vp_predictor, pp_predictor, ap_predictor, ai_predictor]\n",
    "task_set = [['field', 'paper', 'PF_in_L2'],\\\n",
    "            ['venue', 'paper', 'PV_Conference'],\\\n",
    "            ['paper', 'paper', 'PP_cite'],\\\n",
    "            ['paper', 'author', 'AP_write_first'],\\\n",
    "            ['affiliation', 'author', 'in']]\n",
    "\n",
    "pool = mp.Pool(4)\n",
    "process_ids = np.arange(batch_num // 4)\n",
    "st = time.time()\n",
    "jobs = prepare_data(pool, process_ids, task_set)\n",
    "train_step = 1500\n",
    "best_val   = 0\n",
    "\n",
    "for epoch in np.arange(epoch_num)+1:\n",
    "    train_data = [job.get() for job in jobs[:-1]]\n",
    "    valid_data = jobs[-1].get()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    pool = mp.Pool(4)\n",
    "    jobs = prepare_data(pool, process_ids, task_set)\n",
    "    et = time.time()\n",
    "    print('Data Preparation: %.1fs' % (et - st))\n",
    "    \n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "    for neg_nums, rem_lists, node_feature, node_type, node_time, edge_index, edge_type, node_dict in train_data:\n",
    "        for target_size, predictor, rem_edges, (target_type, source_type, rel_type) in zip(neg_nums, models, rem_lists, task_set):\n",
    "            '''\n",
    "                Train\n",
    "            '''\n",
    "            positive_target_ids, source_ids = rem_edges[:,0].reshape(-1, 1), rem_edges[:,1] + node_dict[source_type][0]\n",
    "            negative_target_ids = np.array([neg_sample(target_size, samp_num, \\\n",
    "                edge_index[1][edge_index[0] == s_id].tolist() + [pos_id]) for pos_id, s_id in zip(positive_target_ids, source_ids)])\n",
    "            target_ids = np.concatenate((positive_target_ids, negative_target_ids), axis=-1) + node_dict[target_type][0]\n",
    "\n",
    "            node_emb = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                        node_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "            source_emb = node_emb[source_ids].repeat(1, target_ids.shape[1]\\\n",
    "                                              ).view(target_ids.shape[0] * target_ids.shape[1], -1)\n",
    "\n",
    "            target_emb = node_emb[target_ids].view(target_ids.shape[0] * target_ids.shape[1], -1)\n",
    "            res = predictor.forward(source_emb, target_emb, pair=True).view(-1, target_ids.shape[1])\n",
    "            loss = -torch.log_softmax(res, dim=-1)[:,0].mean()\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.2)\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            train_losses += [loss.cpu().detach().tolist()]\n",
    "            train_step += 1\n",
    "            scheduler.step(train_step)\n",
    "    '''\n",
    "        Valid\n",
    "    '''\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        neg_nums, rem_lists, node_feature, node_type, node_time, edge_index, edge_type, node_dict = valid_data\n",
    "        valid_losses = []\n",
    "        valid_accs   = []\n",
    "        for target_size, predictor, rem_edges, (target_type, source_type, rel_type) in zip(neg_nums, models, rem_lists, task_set):\n",
    "            '''\n",
    "                Valid\n",
    "            '''\n",
    "            positive_target_ids, source_ids = rem_edges[:,0].reshape(-1, 1), rem_edges[:,1] + node_dict[source_type][0]\n",
    "            negative_target_ids = np.array([neg_sample(target_size, samp_num, \\\n",
    "                edge_index[1][edge_index[0] == s_id].tolist() + [pos_id]) for pos_id, s_id in zip(positive_target_ids, source_ids)])\n",
    "            target_ids = np.concatenate((positive_target_ids, negative_target_ids), axis=-1) + node_dict[target_type][0]\n",
    "\n",
    "            node_emb = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                        node_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "            source_emb = node_emb[source_ids].repeat(1, target_ids.shape[1]\\\n",
    "                                              ).view(target_ids.shape[0] * target_ids.shape[1], -1)\n",
    "\n",
    "            target_emb = node_emb[target_ids].view(target_ids.shape[0] * target_ids.shape[1], -1)\n",
    "            res = predictor.forward(source_emb, target_emb, pair=True).view(-1, target_ids.shape[1])\n",
    "            valid_losses +=  [-torch.log_softmax(res, dim=-1)[:,0]]\n",
    "            s = (res.argmax(dim=1) == 0).sum()\n",
    "            valid_accs += [s.tolist() / batch_size]\n",
    "        valid_losses = torch.cat(valid_losses).cpu().detach().tolist()\n",
    "        s = (res.argmax(dim=1) == 0).sum()\n",
    "        st = time.time()\n",
    "        print((\"Epoch: %d (%.1fs)  LR: %.5f Train Loss: %f  Valid Loss: %f  Valid Acc: %f\") % \\\n",
    "              (epoch, (st-et), optimizer.param_groups[0]['lr'], np.average(train_losses), np.average(valid_losses), np.average(valid_accs)))\n",
    "        if np.average(valid_accs) > best_val:\n",
    "            best_val = np.average(valid_accs)\n",
    "            torch.save(gnn, './save/mt_model.pt')\n",
    "        stats += [[train_losses, valid_losses]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = torch.load('./save/mt_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55749031"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 0\n",
    "for s in graph.edge_list:\n",
    "    for t in graph.edge_list[s]:\n",
    "        for e in graph.edge_list[s][t]:\n",
    "            for i in graph.edge_list[s][t][e]:\n",
    "                size += len(graph.edge_list[s][t][e][i])\n",
    "size\n",
    "size = 0\n",
    "for s in graph.node_feature:\n",
    "    size += len(graph.node_feature[s])\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1116163"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 0\n",
    "for s in graph.node_feature:\n",
    "    size += len(graph.node_feature[s])\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Conference', 'Journal', 'Patent', 'Repository'], dtype='<U10')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(list(graph.node_feature['venue']['attr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attr</th>\n",
       "      <th>citation</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>node_emb</th>\n",
       "      <th>type</th>\n",
       "      <th>emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>Patent</td>\n",
       "      <td>0</td>\n",
       "      <td>2764484637</td>\n",
       "      <td>international journal of chemical sciences</td>\n",
       "      <td>[0.740164, -0.589690, -0.403322, 0.115058, 0.1...</td>\n",
       "      <td>venue</td>\n",
       "      <td>[-0.1699792742729187, -1.8855005502700806, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        attr  citation          id  \\\n",
       "1324  Patent         0  2764484637   \n",
       "\n",
       "                                            name  \\\n",
       "1324  international journal of chemical sciences   \n",
       "\n",
       "                                               node_emb   type  \\\n",
       "1324  [0.740164, -0.589690, -0.403322, 0.115058, 0.1...  venue   \n",
       "\n",
       "                                                    emb  \n",
       "1324  [-0.1699792742729187, -1.8855005502700806, -0....  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.node_feature['venue'][graph.node_feature['venue']['attr']=='Patent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1116163, 5574903, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "topconf = graph.node_feature['venue'].nlargest(256, 'citation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list(graph.node_feature['venue'][graph.node_feature['venue']['name'] == 'WWW']['emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world wide web\n",
      "WSDM\n",
      "CIKM\n",
      "ICWSM\n",
      "KDD\n",
      "ieee transactions on knowledge and data engineering\n",
      "SIGIR\n",
      "AAAI\n",
      "proceedings of the vldb endowment\n",
      "HICSS\n",
      "ISWC\n",
      "information processing and management\n",
      "ieee intelligent systems\n",
      "lecture notes in computer science\n",
      "IUI\n",
      "SIGMOD\n",
      "knowledge and information systems\n",
      "ICDM\n",
      "journal of web semantics\n",
      "CHI\n",
      "IJCAI\n",
      "SAC\n",
      "acm transactions on information systems\n",
      "ICDE\n",
      "journal of the association for information science and technology\n",
      "EDBT\n",
      "arxiv cryptography and security\n",
      "international journal of human computer studies international journal of man machine studies\n",
      "acm transactions on computer human interaction\n",
      "sigkdd explorations\n",
      "scientific programming\n",
      "NDSS\n",
      "decision support systems\n",
      "FSE\n",
      "computer supported cooperative work\n",
      "journal of artificial intelligence research\n",
      "autonomous agents and multi agent systems\n",
      "information sciences\n",
      "arxiv distributed parallel and cluster computing\n",
      "CCS\n",
      "personal and ubiquitous computing\n",
      "evolutionary computation\n",
      "NAACL\n",
      "ACSAC\n",
      "international journal of geographical information science\n",
      "COLING\n",
      "SIGCOMM\n",
      "ai magazine\n",
      "MSR\n",
      "S&P\n"
     ]
    }
   ],
   "source": [
    "cos = euclidean_distances(res, list(topconf['emb']))[0]\n",
    "for li in np.argsort(cos)[:50]:\n",
    "    print(list(topconf['name'])[li])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "KDD\n",
      "ICCV\n",
      "CIKM\n",
      "NSDI\n",
      "ICDM\n",
      "MM\n",
      "ICCAD\n",
      "ICDCS\n",
      "GLOBECOM\n",
      "VTC\n",
      "CHI\n",
      "MOBICOM\n",
      "world wide web\n",
      "MobiHoc\n",
      "HPCA\n",
      "SIGMOD\n",
      "ieee transactions on computer aided design of integrated circuits and systems\n",
      "acm sigarch computer architecture news\n",
      "UIST\n",
      "DAC\n",
      "CCS\n",
      "MobiSys\n",
      "autonomous agents and multi agent systems\n",
      "SIGGRAPH\n",
      "ICSE\n",
      "ICDE\n",
      "sigmetrics performance evaluation review\n",
      "SenSys\n",
      "ICIP\n",
      "ICC\n",
      "NAACL\n",
      "SIGIR\n",
      "INFOCOM\n",
      "ieee wireless communications\n",
      "HICSS\n",
      "operating systems review\n",
      "DATE\n",
      "IJCAI\n",
      "ICML\n",
      "NeurIPS\n",
      "ECCV\n",
      "ICASSP\n",
      "ACL\n",
      "EMNLP\n",
      "CVPR\n",
      "AAAI\n",
      "SECURITY\n",
      "journal of the association for information science and technology\n",
      "CRYPTO\n",
      "IPDPS\n",
      "----------------------------------------------------------------------------------------------------\n",
      "KDD\n",
      "ICCV\n",
      "CIKM\n",
      "NSDI\n",
      "ICDM\n",
      "MM\n",
      "ICCAD\n",
      "ICDCS\n",
      "GLOBECOM\n",
      "VTC\n",
      "CHI\n",
      "MOBICOM\n",
      "world wide web\n",
      "MobiHoc\n",
      "HPCA\n",
      "SIGMOD\n",
      "ieee transactions on computer aided design of integrated circuits and systems\n",
      "acm sigarch computer architecture news\n",
      "UIST\n",
      "DAC\n",
      "CCS\n",
      "MobiSys\n",
      "SIGGRAPH\n",
      "autonomous agents and multi agent systems\n",
      "SenSys\n",
      "sigmetrics performance evaluation review\n",
      "ICIP\n",
      "ICSE\n",
      "ICDE\n",
      "SIGIR\n",
      "ICC\n",
      "HICSS\n",
      "ieee wireless communications\n",
      "operating systems review\n",
      "DATE\n",
      "NAACL\n",
      "INFOCOM\n",
      "IPDPS\n",
      "acm sigsoft software engineering notes\n",
      "IJCAI\n",
      "SECURITY\n",
      "ieee micro\n",
      "IROS\n",
      "OSDI\n",
      "journal of the association for information science and technology\n",
      "ICML\n",
      "ECCV\n",
      "proceedings of the vldb endowment\n",
      "NeurIPS\n",
      "ICASSP\n",
      "====================================================================================================\n",
      "KDD\n",
      "NSDI\n",
      "CHI\n",
      "SenSys\n",
      "ICDCS\n",
      "AAAI\n",
      "ACL\n",
      "ICC\n",
      "GLOBECOM\n",
      "ICASSP\n",
      "ECCV\n",
      "ICML\n",
      "MM\n",
      "autonomous agents and multi agent systems\n",
      "ICCV\n",
      "CIKM\n",
      "SIGGRAPH\n",
      "MobiSys\n",
      "arxiv computer vision and pattern recognition\n",
      "INFOCOM\n",
      "NAACL\n",
      "OSDI\n",
      "MOBICOM\n",
      "sigmetrics performance evaluation review\n",
      "NeurIPS\n",
      "SECURITY\n",
      "ieee wireless communications\n",
      "ieee micro\n",
      "CRYPTO\n",
      "DAC\n",
      "IPDPS\n",
      "VTC\n",
      "IJCAI\n",
      "HICSS\n",
      "USENIX ATC\n",
      "MobiHoc\n",
      "ICDM\n",
      "acm sigarch computer architecture news\n",
      "ICDE\n",
      "lecture notes in computer science\n",
      "CCS\n",
      "SIGIR\n",
      "ICCAD\n",
      "CVPR\n",
      "EMNLP\n",
      "HPCA\n",
      "acm sigsoft software engineering notes\n",
      "SIGMOD\n",
      "world wide web\n",
      "proceedings of the vldb endowment\n",
      "----------------------------------------------------------------------------------------------------\n",
      "KDD\n",
      "NSDI\n",
      "CHI\n",
      "SenSys\n",
      "ICDCS\n",
      "ICASSP\n",
      "GLOBECOM\n",
      "AAAI\n",
      "ICC\n",
      "ACL\n",
      "ECCV\n",
      "ICML\n",
      "MM\n",
      "ICCV\n",
      "autonomous agents and multi agent systems\n",
      "SIGGRAPH\n",
      "MobiSys\n",
      "CIKM\n",
      "arxiv computer vision and pattern recognition\n",
      "NAACL\n",
      "SECURITY\n",
      "CRYPTO\n",
      "OSDI\n",
      "IPDPS\n",
      "INFOCOM\n",
      "NeurIPS\n",
      "MOBICOM\n",
      "ieee wireless communications\n",
      "sigmetrics performance evaluation review\n",
      "DAC\n",
      "ieee micro\n",
      "acm sigarch computer architecture news\n",
      "EMNLP\n",
      "VTC\n",
      "CCS\n",
      "ICDM\n",
      "ICCAD\n",
      "SIGIR\n",
      "ICDE\n",
      "HPCA\n",
      "USENIX ATC\n",
      "MobiHoc\n",
      "IJCAI\n",
      "HICSS\n",
      "lecture notes in computer science\n",
      "acm sigsoft software engineering notes\n",
      "proceedings of the vldb endowment\n",
      "the vldb journal\n",
      "DATE\n",
      "IROS\n",
      "====================================================================================================\n",
      "1\n",
      "KDD\n",
      "ICDM\n",
      "ICCV\n",
      "CIKM\n",
      "HICSS\n",
      "SIGIR\n",
      "GLOBECOM\n",
      "CHI\n",
      "SIGMOD\n",
      "journal of machine learning research\n",
      "ICCAD\n",
      "CCS\n",
      "MOBICOM\n",
      "MobiSys\n",
      "MM\n",
      "ieee journal on selected areas in communications\n",
      "OSDI\n",
      "sigmetrics performance evaluation review\n",
      "ICDCS\n",
      "ieee transactions on parallel and distributed systems\n",
      "ICSE\n",
      "ICDE\n",
      "DAC\n",
      "ieee transactions on mobile computing\n",
      "VTC\n",
      "UIST\n",
      "NSDI\n",
      "acm sigarch computer architecture news\n",
      "SIGGRAPH\n",
      "ICIP\n",
      "NAACL\n",
      "INFOCOM\n",
      "ICC\n",
      "acm sigsoft software engineering notes\n",
      "ieee wireless communications\n",
      "DATE\n",
      "HPCA\n",
      "IJCAI\n",
      "IROS\n",
      "ieee transactions on vehicular technology\n",
      "IPDPS\n",
      "ECCV\n",
      "SECURITY\n",
      "ICML\n",
      "NeurIPS\n",
      "ACL\n",
      "CVPR\n",
      "ICASSP\n",
      "EMNLP\n",
      "CRYPTO\n",
      "----------------------------------------------------------------------------------------------------\n",
      "KDD\n",
      "ICDM\n",
      "ICCV\n",
      "CIKM\n",
      "HICSS\n",
      "SIGIR\n",
      "GLOBECOM\n",
      "SIGMOD\n",
      "CHI\n",
      "journal of machine learning research\n",
      "CCS\n",
      "ICCAD\n",
      "MobiSys\n",
      "MOBICOM\n",
      "MM\n",
      "OSDI\n",
      "ieee journal on selected areas in communications\n",
      "sigmetrics performance evaluation review\n",
      "ICDCS\n",
      "ICSE\n",
      "ieee transactions on parallel and distributed systems\n",
      "ieee transactions on mobile computing\n",
      "ICDE\n",
      "DAC\n",
      "NSDI\n",
      "UIST\n",
      "acm sigarch computer architecture news\n",
      "VTC\n",
      "ICIP\n",
      "SIGGRAPH\n",
      "acm sigsoft software engineering notes\n",
      "ieee wireless communications\n",
      "INFOCOM\n",
      "DATE\n",
      "IROS\n",
      "ICC\n",
      "HPCA\n",
      "NAACL\n",
      "ieee transactions on vehicular technology\n",
      "IPDPS\n",
      "MobiHoc\n",
      "IJCAI\n",
      "SECURITY\n",
      "ICML\n",
      "ECCV\n",
      "NeurIPS\n",
      "ACL\n",
      "ICASSP\n",
      "CVPR\n",
      "EMNLP\n",
      "====================================================================================================\n",
      "KDD\n",
      "ICDE\n",
      "ICSE\n",
      "EMNLP\n",
      "CIKM\n",
      "NeurIPS\n",
      "lecture notes in computer science\n",
      "IROS\n",
      "INFOCOM\n",
      "the vldb journal\n",
      "ECCV\n",
      "NAACL\n",
      "ICDCS\n",
      "AAAI\n",
      "ICCV\n",
      "MM\n",
      "proceedings of the vldb endowment\n",
      "ICASSP\n",
      "ieee transactions on mobile computing\n",
      "SIGMOD\n",
      "ICC\n",
      "ACL\n",
      "ieee journal on selected areas in communications\n",
      "IJCAI\n",
      "arxiv computer vision and pattern recognition\n",
      "MobiSys\n",
      "CVPR\n",
      "UIST\n",
      "ICCAD\n",
      "DAC\n",
      "CRYPTO\n",
      "SIGIR\n",
      "ieee transactions on parallel and distributed systems\n",
      "sigmetrics performance evaluation review\n",
      "CHI\n",
      "DATE\n",
      "CCS\n",
      "HICSS\n",
      "ICML\n",
      "MOBICOM\n",
      "ICIP\n",
      "SIGGRAPH\n",
      "SECURITY\n",
      "VTC\n",
      "acm sigsoft software engineering notes\n",
      "GLOBECOM\n",
      "ieee transactions on vehicular technology\n",
      "ICDM\n",
      "HPCA\n",
      "ieee wireless communications\n",
      "----------------------------------------------------------------------------------------------------\n",
      "KDD\n",
      "ICDE\n",
      "ICSE\n",
      "EMNLP\n",
      "CIKM\n",
      "NeurIPS\n",
      "INFOCOM\n",
      "IROS\n",
      "lecture notes in computer science\n",
      "NAACL\n",
      "ECCV\n",
      "the vldb journal\n",
      "ICDCS\n",
      "ICCV\n",
      "MM\n",
      "proceedings of the vldb endowment\n",
      "ieee transactions on mobile computing\n",
      "ICC\n",
      "ieee journal on selected areas in communications\n",
      "arxiv computer vision and pattern recognition\n",
      "MobiSys\n",
      "IJCAI\n",
      "CRYPTO\n",
      "SIGIR\n",
      "sigmetrics performance evaluation review\n",
      "ieee transactions on parallel and distributed systems\n",
      "CCS\n",
      "ICASSP\n",
      "ICML\n",
      "AAAI\n",
      "MOBICOM\n",
      "UIST\n",
      "ACL\n",
      "CVPR\n",
      "HICSS\n",
      "DATE\n",
      "SIGMOD\n",
      "DAC\n",
      "SECURITY\n",
      "SIGGRAPH\n",
      "ICCAD\n",
      "VTC\n",
      "acm sigsoft software engineering notes\n",
      "ICDM\n",
      "ICIP\n",
      "journal of machine learning research\n",
      "OSDI\n",
      "CHI\n",
      "NSDI\n",
      "ieee transactions on vehicular technology\n",
      "====================================================================================================\n",
      "2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-238-e3d0b7b0b424>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         feature, times, edge_list, _ = sample_subgraph(graph, {t: True for t in graph.times if t != None}, \\\n\u001b[0;32m---> 12\u001b[0;31m                         inp = {'venue': vids}, sampled_depth = 4, sampled_number = 256)\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mnode_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_dict\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mto_torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/heter/utils.py\u001b[0m in \u001b[0;36msample_subgraph\u001b[0;34m(graph, time_range, sampled_depth, sampled_number, inp)\u001b[0m\n\u001b[1;32m    292\u001b[0m                             \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0meach\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mexist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0madjacancy\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                         '''\n\u001b[0;32m--> 294\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtesrt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m                             \u001b[0medge_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrelation_type\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_ser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_ser\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gnn.to(device)\n",
    "gnn.eval()\n",
    "topconf = graph.node_feature['venue'].nlargest(100, 'citation')\n",
    "with torch.no_grad():\n",
    "    _time = 2000\n",
    "    vids = np.stack([graph.node_feature['venue'].nlargest(100, 'citation').index.values, np.repeat([_time], 100)]).T\n",
    "    conf_emb1 = []\n",
    "    conf_emb2 = []\n",
    "    for i in range(10):\n",
    "        print(i)\n",
    "        feature, times, edge_list, _ = sample_subgraph(graph, {t: True for t in graph.times if t != None}, \\\n",
    "                        inp = {'venue': vids}, sampled_depth = 4, sampled_number = 256)\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict = \\\n",
    "                    to_torch(feature, times, edge_list, graph)\n",
    "        venue_ids = np.arange(len(vids)) + node_dict['venue'][0]\n",
    "\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type = node_feature.to(device), node_type.to(device), \\\n",
    "                              edge_time.to(device), edge_index.to(device), edge_type.to(device)\n",
    "        res = torch.zeros(node_feature.size(0), gnn.n_hid).to(node_feature.device)\n",
    "        for t_id in range(gnn.num_types):\n",
    "            aggregat_w = gnn.aggregat_ws[t_id]\n",
    "            idx = (node_type == t_id)\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            res[idx] = torch.tanh(aggregat_w(node_feature[idx]))\n",
    "        meta_xs = gnn.drop(res)\n",
    "        del res\n",
    "        meta_xs = gnn.gcs[0](meta_xs, node_type, edge_index, edge_type, edge_time)\n",
    "        emb = meta_xs[venue_ids].cpu().detach().numpy()\n",
    "        conf_emb1 += [emb]\n",
    "        cos = cosine_similarity(emb)\n",
    "        for li in np.argsort(-cos[list(topconf['name']).index('KDD')])[:50]:\n",
    "            print(list(topconf['name'])[li])\n",
    "        print('-' * 100)\n",
    "        dis = euclidean_distances(emb)\n",
    "        for li in np.argsort(dis[list(topconf['name']).index('KDD')])[:50]:\n",
    "            print(list(topconf['name'])[li])\n",
    "        print('=' * 100)\n",
    "        \n",
    "        \n",
    "        meta_xs = gnn.gcs[1](meta_xs, node_type, edge_index, edge_type, edge_time)\n",
    "        emb = meta_xs[venue_ids].cpu().detach().numpy()\n",
    "        conf_emb2 += [emb]\n",
    "        cos = cosine_similarity(emb)\n",
    "        for li in np.argsort(-cos[list(topconf['name']).index('KDD')])[:50]:\n",
    "            print(list(topconf['name'])[li])\n",
    "        print('-' * 100)\n",
    "        dis = euclidean_distances(emb)\n",
    "        for li in np.argsort(dis[list(topconf['name']).index('KDD')])[:50]:\n",
    "            print(list(topconf['name'])[li])\n",
    "        print('=' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coss = [euclidean_distances(emb) for emb in conf_emb1]\n",
    "cos = np.average(coss, axis=0)\n",
    "for li in np.argsort(cos[list(topconf['name']).index('ACL')])[:50]:\n",
    "    print(list(topconf['name'])[li])\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACL\n",
      "INFOCOM\n",
      "ECCV\n",
      "GLOBECOM\n",
      "ICSE\n",
      "CVPR\n",
      "ICIP\n",
      "SIGMOD\n",
      "EMNLP\n",
      "NeurIPS\n",
      "ICCV\n",
      "NAACL\n",
      "FSE\n",
      "AAAI\n",
      "VTC\n",
      "CAV\n",
      "SECURITY\n",
      "SIGIR\n",
      "ICDE\n",
      "CCS\n",
      "ICC\n",
      "IJCAI\n",
      "EDBT\n",
      "MICCAI\n",
      "CIKM\n",
      "ECOOP\n",
      "SIGGRAPH\n",
      "COLING\n",
      "ICRA\n",
      "ICML\n",
      "KDD\n",
      "ICDCS\n",
      "data and knowledge engineering\n",
      "MM\n",
      "GECCO\n",
      "DAC\n",
      "DATE\n",
      "ICCAD\n",
      "INTERSPEECH\n",
      "ASILOMAR\n",
      "MOBICOM\n",
      "ICLR\n",
      "IUI\n",
      "NSDI\n",
      "UAI\n",
      "ICDM\n",
      "HICSS\n",
      "ACSAC\n",
      "ISSTA\n",
      "IPDPS\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "coss = [euclidean_distances(emb) for emb in conf_emb2]\n",
    "cos = np.average(coss, axis=0)\n",
    "for li in np.argsort(cos[list(topconf['name']).index('ACL')])[:50]:\n",
    "    print(list(topconf['name'])[li])\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_emb = {2010: [conf_emb1, conf_emb2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_emb[1990] = [conf_emb1, conf_emb2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_emb[2030] = [conf_emb1, conf_emb2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.44682163,  1.4439737 ,  1.0461857 , ...,  1.0276611 ,\n",
       "         -0.5696738 , -0.7640809 ],\n",
       "        [-0.44682163,  1.2189132 ,  0.80924225, ...,  1.4106448 ,\n",
       "         -0.56967384, -0.7640809 ],\n",
       "        [-0.44682163,  1.7692353 ,  0.7775029 , ...,  0.61222756,\n",
       "         -0.49836862, -0.7178318 ],\n",
       "        ...,\n",
       "        [-0.44682163, -0.10202482,  0.07401577, ...,  0.37724358,\n",
       "         -0.51938236, -0.7640809 ],\n",
       "        [-0.44682163, -0.00676589, -0.32257503, ...,  0.21618128,\n",
       "         -0.50861377, -0.76408094],\n",
       "        [-0.44682163, -0.5664511 , -0.03994929, ..., -0.2181938 ,\n",
       "         -0.5206225 , -0.7640809 ]], dtype=float32),\n",
       " array([[-0.4534359 ,  1.0715052 ,  0.79899216, ...,  0.41144317,\n",
       "         -0.49090075, -0.7148243 ],\n",
       "        [-0.4534359 ,  0.7887458 ,  0.21565858, ...,  1.555398  ,\n",
       "         -0.4794289 , -0.7148243 ],\n",
       "        [-0.4534359 ,  1.0116353 ,  1.191432  , ...,  0.8996734 ,\n",
       "         -0.43086213, -0.7148243 ],\n",
       "        ...,\n",
       "        [-0.4534359 , -0.29021427, -0.19674638, ..., -0.00305312,\n",
       "         -0.44862723, -0.7148243 ],\n",
       "        [-0.4534359 , -0.574738  ,  0.27449092, ..., -0.31337222,\n",
       "         -0.45634955, -0.71482444],\n",
       "        [-0.4534359 , -0.823768  , -1.0513588 , ..., -0.88283527,\n",
       "         -0.24251582,  1.623608  ]], dtype=float32),\n",
       " array([[-0.42583793,  1.6975662 ,  2.1587713 , ...,  1.6335089 ,\n",
       "         -0.55710846, -0.71338975],\n",
       "        [-0.42583793,  1.4486269 ,  0.45830402, ...,  1.1562824 ,\n",
       "         -0.55710846, -0.71338975],\n",
       "        [-0.42583793,  0.5237587 ,  0.33017716, ...,  0.46156895,\n",
       "         -0.47797412, -0.71338975],\n",
       "        ...,\n",
       "        [-0.42583793, -0.31843445,  0.21802601, ...,  0.22272861,\n",
       "         -0.49832642, -0.71338975],\n",
       "        [-0.42583793, -0.52304685, -0.03604461, ..., -0.33822387,\n",
       "         -0.49750972, -0.7133898 ],\n",
       "        [-0.42583793, -0.65739626, -0.67755103, ..., -0.8996472 ,\n",
       "         -0.37133053,  0.50023174]], dtype=float32),\n",
       " array([[-0.4695803 ,  0.3386923 ,  0.743664  , ...,  0.06436691,\n",
       "         -0.51737696, -0.5673633 ],\n",
       "        [-0.4695803 ,  1.6667984 ,  0.45691118, ...,  1.4434178 ,\n",
       "         -0.517377  , -0.70459294],\n",
       "        [-0.4695803 ,  1.268206  ,  0.8735851 , ...,  0.8665181 ,\n",
       "         -0.45545265, -0.70459294],\n",
       "        ...,\n",
       "        [-0.4695803 ,  0.1411532 ,  0.5700022 , ...,  0.06418652,\n",
       "         -0.45884407, -0.70459294],\n",
       "        [-0.4695803 ,  0.0360794 , -0.27568448, ...,  0.40223524,\n",
       "         -0.4713681 , -0.70459294],\n",
       "        [ 0.0886291 , -0.82895553,  0.83677304, ..., -0.86616087,\n",
       "         -0.49671996, -0.7045928 ]], dtype=float32),\n",
       " array([[-0.47517365,  0.9634371 ,  0.27660772, ...,  0.66955006,\n",
       "         -0.5283795 , -0.66385317],\n",
       "        [-0.47517365,  1.6708682 ,  0.7754936 , ...,  1.7063676 ,\n",
       "         -0.5283796 , -0.7154763 ],\n",
       "        [-0.47517365, -0.08441885,  0.74614155, ...,  0.07584525,\n",
       "         -0.4669474 , -0.7154763 ],\n",
       "        ...,\n",
       "        [-0.47517365, -0.23918645,  0.05115178, ...,  0.3139268 ,\n",
       "         -0.48059988, -0.7154763 ],\n",
       "        [-0.06976037, -0.82190794,  0.726038  , ..., -0.84158516,\n",
       "         -0.49449182, -0.71547633],\n",
       "        [-0.47517365, -0.10735336, -0.35365015, ...,  0.15314811,\n",
       "         -0.46639276, -0.7154763 ]], dtype=float32),\n",
       " array([[-0.46603727,  0.5860731 ,  0.8944342 , ...,  0.45477724,\n",
       "         -0.53453845, -0.7110727 ],\n",
       "        [-0.46603727,  0.65594715,  1.0862893 , ...,  1.2729958 ,\n",
       "         -0.5345385 , -0.7110727 ],\n",
       "        [-0.46603727,  0.39956555,  0.57574284, ...,  0.6560359 ,\n",
       "         -0.48807627, -0.7110727 ],\n",
       "        ...,\n",
       "        [-0.46603727, -0.72928745,  0.5458392 , ...,  0.11299581,\n",
       "         -0.4741916 , -0.7110727 ],\n",
       "        [-0.46603727, -0.76508796,  0.32175612, ..., -0.1349116 ,\n",
       "         -0.49708682, -0.7110727 ],\n",
       "        [ 0.41078085, -0.8490899 ,  0.66696537, ..., -0.88417006,\n",
       "         -0.5125569 , -0.7110727 ]], dtype=float32),\n",
       " array([[-0.445911  ,  1.0390935 ,  0.8140012 , ...,  0.32725075,\n",
       "         -0.5725773 , -0.75134   ],\n",
       "        [-0.445911  ,  0.35276902, -0.2618085 , ...,  0.48901135,\n",
       "         -0.57257736, -0.75134   ],\n",
       "        [-0.445911  ,  0.5196224 ,  1.1004753 , ...,  0.6924957 ,\n",
       "         -0.5371549 , -0.75134   ],\n",
       "        ...,\n",
       "        [-0.445911  , -0.34951144, -0.08021159, ..., -0.090335  ,\n",
       "         -0.5251037 , -0.75134   ],\n",
       "        [ 0.07810295, -0.8277264 ,  0.5116831 , ..., -0.8264473 ,\n",
       "         -0.542477  , -0.7513401 ],\n",
       "        [-0.445911  , -0.60581195, -0.02170897, ..., -0.232182  ,\n",
       "         -0.52592903, -0.75134   ]], dtype=float32),\n",
       " array([[-0.46085292,  0.9329751 ,  1.0432392 , ...,  0.6868785 ,\n",
       "         -0.55465746, -0.72431624],\n",
       "        [-0.46085292,  1.4973142 ,  0.8383584 , ...,  1.4204729 ,\n",
       "         -0.55465746, -0.72431624],\n",
       "        [-0.46085292,  0.9776527 ,  2.1230946 , ...,  1.2187235 ,\n",
       "         -0.50548106, -0.72431624],\n",
       "        ...,\n",
       "        [-0.46085292, -0.54898894,  0.10724358, ...,  0.0204628 ,\n",
       "         -0.50705975, -0.72431624],\n",
       "        [-0.46085292,  0.5603766 ,  0.2625694 , ...,  1.2333368 ,\n",
       "         -0.5041313 , -0.72431624],\n",
       "        [-0.46085292, -0.78287715,  0.15389101, ..., -0.40361464,\n",
       "         -0.51548934, -0.72431624]], dtype=float32),\n",
       " array([[-0.43951595,  1.4273432 ,  0.7796055 , ...,  0.94143015,\n",
       "         -0.52379316, -0.715576  ],\n",
       "        [-0.43951595,  1.5341688 ,  0.8322817 , ...,  1.5634454 ,\n",
       "         -0.5237932 , -0.715576  ],\n",
       "        [-0.43951595,  0.76873237,  0.07944281, ...,  0.4554584 ,\n",
       "         -0.46105525, -0.7155761 ],\n",
       "        ...,\n",
       "        [-0.43951595,  0.6679388 ,  0.55983293, ...,  0.3136969 ,\n",
       "         -0.46364126, -0.5569694 ],\n",
       "        [-0.43951595,  0.04837767, -0.16298431, ...,  0.10908953,\n",
       "         -0.47486556, -0.7155761 ],\n",
       "        [-0.43951595, -0.8494959 ,  0.26643506, ..., -0.40877548,\n",
       "         -0.483845  , -0.715576  ]], dtype=float32),\n",
       " array([[-0.4363901 ,  0.608009  ,  0.40583408, ...,  0.40010053,\n",
       "         -0.53441626, -0.6851815 ],\n",
       "        [-0.4363901 ,  0.39144027,  0.29610446, ...,  1.3434185 ,\n",
       "         -0.5208259 , -0.6851815 ],\n",
       "        [-0.4363901 ,  0.6957225 ,  0.7368742 , ...,  0.51867026,\n",
       "         -0.46623018, -0.6851815 ],\n",
       "        ...,\n",
       "        [-0.4363901 , -0.27959508, -0.18852241, ...,  0.19989562,\n",
       "         -0.48351264, -0.6851815 ],\n",
       "        [-0.23562422, -0.85404176,  0.2647679 , ..., -0.87707293,\n",
       "         -0.4997825 , -0.6851815 ],\n",
       "        [ 0.38760525, -0.85404176,  0.6798421 , ..., -0.8789201 ,\n",
       "         -0.50103545, -0.6851815 ]], dtype=float32)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_emb[1990][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict = {j: i for i, j in enumerate(graph.get_types())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_dict = {e[2]: i for i, e in enumerate(graph.get_meta_graph())}\n",
    "edge_dict['self'] = len(edge_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "_s = 0\n",
    "for i in graph.edge_list['author']['affiliation']['rev_in']:\n",
    "    _s += len(graph.edge_list['author']['affiliation']['rev_in'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "612872"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "510189"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph.edge_list['author']['affiliation']['rev_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn, _ = torch.load('../paper-field-L2/save/rgt_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affiliation author in\n",
      "tensor(0.9698, device='cuda:1', grad_fn=<MeanBackward0>)\n",
      "affiliation affiliation IPI_coauthor\n",
      "tensor(0.9237, device='cuda:1', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, j, k in graph.get_meta_graph():\n",
    "    if i == 'affiliation':\n",
    "        print(i,j,k)\n",
    "        print(gnn.gcs[0].relation_ws[node_dict[i]][edge_dict[k]][node_dict[j]].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper venue rev_PV_Conference\n",
    "tensor(1.0509, device='cuda:1', grad_fn=<MeanBackward0>)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
