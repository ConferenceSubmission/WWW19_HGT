{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/datadrive/data/'\n",
    "batch_size = 256\n",
    "batch_num  = 128\n",
    "epoch_num  = 1000\n",
    "samp_num   = 7\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "graph = dill.load(open(data_dir + 'graph.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = {t: True for t in graph.times if t != None and t <= 2015}\n",
    "valid_range = {t: True for t in graph.times if t != None and t > 2015  and t < 2018}\n",
    "test_range  = {t: True for t in graph.times if t != None and t >= 2018}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_hid, num_types, num_relations, n_heads, n_layers, device, dropout = 0.3):\n",
    "        super(GNN, self).__init__()\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.adapt = nn.Linear(n_hid, n_hid)\n",
    "        self.n_hid = n_hid\n",
    "        for l in range(n_layers):\n",
    "            self.gcs.append(RAGCNConv(n_hid, n_hid, num_types, num_relations, n_heads, device, dropout))\n",
    "    def set_device(self, device):\n",
    "        self.device = device\n",
    "        for gc in self.gcs:\n",
    "            gc.device = device\n",
    "    def forward(self, node_feature, node_type, edge_time, edge_index, edge_type):\n",
    "        meta_xs = F.elu(self.adapt(node_feature))\n",
    "        for gc in self.gcs:\n",
    "            meta_xs = gc(meta_xs, node_type, edge_index, edge_type, edge_time)\n",
    "        return meta_xs\n",
    "    def __repr__(self):\n",
    "        return '{}(n_hid={}, n_layers={})'.format(\n",
    "            self.__class__.__name__, self.n_hid, len(gcs))\n",
    "    \n",
    "class CPC_Predictor(nn.Module):\n",
    "    def __init__(self, n_hid, num_types, num_relations, n_heads, n_layers, device, dropout = 0.3):\n",
    "        super(CPC_Predictor, self).__init__()\n",
    "        self.gnn = GNN(n_hid, num_types, num_relations, n_heads, n_layers, device, dropout)\n",
    "        self.matcher = MatchConv(n_hid, n_hid, num_types, num_relations, n_heads, device, dropout)\n",
    "        self.score   = nn.Linear(n_heads, 1)\n",
    "        \n",
    "    def forward(self, pos_paper_ids, neg_paper_ids, node_feature, node_type, edge_time, edge_index, edge_type):\n",
    "        meta_xs = self.gnn(node_feature, node_type, edge_time, edge_index, edge_type)\n",
    "        meta_xs = self.matcher(meta_xs, node_type, edge_index, edge_type)\n",
    "        pos_res = self.score(meta_xs[pos_paper_ids])\n",
    "        neg_res = self.score(meta_xs[neg_paper_ids])\n",
    "        neg_res = neg_res.view(pos_res.size(0), neg_res.size(0) // pos_res.size(0))\n",
    "        return torch.cat([pos_res, neg_res], dim=-1)\n",
    "\n",
    "def gen_negative_sample(dim1, dim2, num):\n",
    "    res = []\n",
    "    for i in range(num):\n",
    "        res += [[np.random.randint(dim1), np.random.randint(dim2)]]\n",
    "    return res\n",
    "\n",
    "def neg_sample(size, pos_id, num):\n",
    "    res = {}\n",
    "    while len(res) != num:\n",
    "        s = np.random.choice(size)\n",
    "        if s in res or s == pos_id:\n",
    "            continue\n",
    "        res[s] = True\n",
    "    return list(res.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch(feature, time, edge_list):\n",
    "    ser = 0\n",
    "    node_dict = {}\n",
    "    for t in feature:\n",
    "        if t != 'fake_paper':\n",
    "            node_dict[t] = [0, ser]\n",
    "            ser += 1\n",
    "    node_dict['fake_paper'] = [0, node_dict['paper'][1]]\n",
    "    node_num = 0\n",
    "    node_feature = []\n",
    "    node_type    = []\n",
    "    node_time    = []\n",
    "    for t in feature:\n",
    "        node_dict[t][0] = node_num\n",
    "        node_feature += list(feature[t])\n",
    "        node_time    += list(time[t])\n",
    "        node_type    += [node_dict[t][1] for _ in range(len(feature[t]))]\n",
    "        node_num     += len(feature[t])\n",
    "    edge_index = []\n",
    "    edge_type  = []\n",
    "    edge_time  = []\n",
    "    edge_dict  = {}\n",
    "    for target_type in edge_list:\n",
    "        for source_type in edge_list[target_type]:\n",
    "            for relation_type in edge_list[target_type][source_type]:\n",
    "                if relation_type not in edge_dict:\n",
    "                    edge_dict[relation_type] = len(edge_dict)\n",
    "                for ti, si in edge_list[target_type][source_type][relation_type]:\n",
    "                    sid, tid = si + node_dict[source_type][0], ti + node_dict[target_type][0]\n",
    "                    edge_index += [[sid, tid]]\n",
    "                    edge_type  += [edge_dict[relation_type]]   \n",
    "                    edge_time  += [node_time[tid] - node_time[sid] + 120]\n",
    "    node_feature = torch.FloatTensor(node_feature).to(device)\n",
    "    node_type    = torch.LongTensor(node_type).to(device)\n",
    "    edge_time    = torch.LongTensor(edge_time).to(device)\n",
    "    edge_index   = torch.LongTensor(edge_index).to(device).t()\n",
    "    edge_type    = torch.LongTensor(edge_type).to(device)\n",
    "    return node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict\n",
    "\n",
    "def mask(feature, time, edge_list, batch_size, target_type, source_type, rel_type):\n",
    "    edges = np.array(edge_list[target_type][source_type][rel_type])\n",
    "    rem_ids = np.random.choice(np.arange(len(edges)), batch_size, replace = False)\n",
    "    ori_ids = np.array([i for i in range(len(edges)) if i not in rem_ids])\n",
    "    ori_edges = edges[ori_ids]\n",
    "    rem_edges = edges[rem_ids]\n",
    "\n",
    "    edge_list[target_type][source_type][rel_type] = list(ori_edges)\n",
    "    edge_list[source_type][target_type]['rev_' + rel_type] = list(np.stack((ori_edges[:,1], ori_edges[:,0])).T)\n",
    "    node_feature, node_type, node_time, edge_index, edge_type, node_dict, _ = to_torch(feature, time, edge_list)\n",
    "    edge_list[target_type][source_type][rel_type] = list(edges)\n",
    "    edge_list[source_type][target_type]['rev_' + rel_type] = list(np.stack((edges[:,1], edges[:,0])).T)\n",
    "    return rem_edges, node_feature, node_type, node_time, edge_index, edge_type, node_dict\n",
    "\n",
    "def edge_loss(gnn, predictor, feature, time, edge_list, batch_size, target_type, source_type, rel_type):\n",
    "    rem_edges, node_feature, node_type, node_time, edge_index, edge_type, node_dict = \\\n",
    "            mask(feature, time, edge_list, batch_size, target_type, source_type, rel_type)\n",
    "    target_size = len(feature[target_type])\n",
    "    positive_target_ids, source_ids = rem_edges[:,0].reshape(-1, 1), rem_edges[:,1] + node_dict[source_type][0]\n",
    "    negative_target_ids = np.array([neg_sample(target_size, pos_id, samp_num - 1) for pos_id in positive_target_ids])\n",
    "    target_ids = np.concatenate((positive_target_ids, negative_target_ids), axis=-1) + node_dict[target_type][0]\n",
    "\n",
    "    node_emb = gnn.forward(node_feature, node_type, node_time, edge_index, edge_type)\n",
    "    source_emb = node_emb[source_ids].repeat(1, target_ids.shape[1]\\\n",
    "                                      ).view(target_ids.shape[0] * target_ids.shape[1], -1)\n",
    "    \n",
    "    target_emb = node_emb[target_ids].view(target_ids.shape[0] * target_ids.shape[1], -1)\n",
    "    res = predictor.forward(source_emb, target_emb).view(-1, target_ids.shape[1])\n",
    "    return -torch.log_softmax(res, dim=-1)[:,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "batch_num  = 128\n",
    "epoch_num  = 100\n",
    "samp_num   = 7\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "train_feature, train_time, train_edge_list = \\\n",
    "        graph.sample_subgraph(time_range=train_range, sampled_depth = 4, sampled_number = 64)\n",
    "node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict  = \\\n",
    "        to_torch(train_feature, train_time, train_edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = GNN(n_hid = 400, num_types = len(node_dict), num_relations = len(edge_dict), \\\n",
    "                       n_heads = 4, n_layers = 2, device = device, dropout = 0.5).to(device)\n",
    "fp_predictor = NTN(200, 200, 4).to(device)\n",
    "vp_predictor = NTN(200, 200, 4).to(device)\n",
    "pp_predictor = NTN(200, 200, 4).to(device)\n",
    "ap_predictor = NTN(200, 200, 4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(list(fp_predictor.parameters()) + list(vp_predictor.parameters()) + \\\n",
    "                             list(pp_predictor.parameters()) + list(ap_predictor.parameters()) + list(gnn.parameters()))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 2000, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  LR: 0.00099 Train Loss: 1.940670  Valid Loss: [1.2019134759902954, 2.1395201683044434, 1.9515711069107056, 1.9282042980194092]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RAGCNConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type TDW. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RelTemporalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2  LR: 0.00096 Train Loss: 1.697782  Valid Loss: [1.7943284511566162, 1.992253303527832, 1.956581950187683, 1.9190278053283691]\n",
      "Epoch: 3  LR: 0.00091 Train Loss: 1.679364  Valid Loss: [1.0076727867126465, 2.341188907623291, 1.9548419713974, 1.8858458995819092]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RAGCNConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type TDW. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RelTemporalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4  LR: 0.00085 Train Loss: 1.653797  Valid Loss: [1.3943597078323364, 2.197718620300293, 1.9615848064422607, 1.867427110671997]\n",
      "Epoch: 5  LR: 0.00077 Train Loss: 1.644723  Valid Loss: [1.5225019454956055, 2.1239171028137207, 1.946450114250183, 1.897566556930542]\n",
      "Epoch: 6  LR: 0.00068 Train Loss: 1.616090  Valid Loss: [1.135371208190918, 2.09604811668396, 1.9305073022842407, 1.8914133310317993]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RAGCNConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type TDW. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RelTemporalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7  LR: 0.00059 Train Loss: 1.596987  Valid Loss: [0.9930037260055542, 2.379387140274048, 1.8653638362884521, 1.8962191343307495]\n",
      "Epoch: 8  LR: 0.00049 Train Loss: 1.569882  Valid Loss: [1.0702203512191772, 2.2987213134765625, 2.0838184356689453, 1.888795256614685]\n",
      "Epoch: 9  LR: 0.00039 Train Loss: 1.558523  Valid Loss: [1.2430710792541504, 2.6062722206115723, 1.9731640815734863, 1.9110252857208252]\n",
      "Epoch: 10  LR: 0.00029 Train Loss: 1.547903  Valid Loss: [0.6451696157455444, 2.0382494926452637, 1.932870626449585, 1.8846811056137085]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RAGCNConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type TDW. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RelTemporalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11  LR: 0.00021 Train Loss: 1.525456  Valid Loss: [0.9846047163009644, 2.2114388942718506, 1.9248523712158203, 1.8778998851776123]\n",
      "Epoch: 12  LR: 0.00014 Train Loss: 1.506042  Valid Loss: [0.844925045967102, 2.3784337043762207, 1.8391391038894653, 1.8840439319610596]\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "train_step = 0\n",
    "best_val   = 100\n",
    "models = [gnn, fp_predictor, vp_predictor, pp_predictor, ap_predictor]\n",
    "task_set = [[fp_predictor, 'field', 'paper', 'PF_in'],\\\n",
    "            [vp_predictor, 'venue', 'paper', 'PV_in'],\\\n",
    "            [pp_predictor, 'paper', 'paper', 'PP_cite'],\\\n",
    "            [ap_predictor, 'paper', 'author', 'AP_write']]\n",
    "for epoch in np.arange(epoch_num)+1:\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    torch.cuda.empty_cache()\n",
    "    for m in models:\n",
    "        m.train()\n",
    "    for out_batch in np.arange(batch_num // 4) + 1:\n",
    "        train_feature, train_time, train_edge_list = \\\n",
    "            graph.sample_subgraph(time_range=train_range, sampled_depth = 5, sampled_number = 64)\n",
    "        for predictor, target_type, source_type, rel_type in task_set:\n",
    "            '''\n",
    "                Train\n",
    "            '''\n",
    "            loss = edge_loss(gnn, predictor, train_feature, train_time, train_edge_list, batch_size, target_type, source_type, rel_type)\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            train_losses += [loss.cpu().detach().tolist()]\n",
    "            train_step += 1\n",
    "            scheduler.step(train_step)\n",
    "    '''\n",
    "        Valid\n",
    "    '''\n",
    "    for m in models:\n",
    "        m.eval()\n",
    "    valid_feature, valid_time, valid_edge_list = \\\n",
    "        graph.sample_subgraph(time_range=valid_range, sampled_depth = 5, sampled_number = 64)\n",
    "    for predictor, target_type, source_type, rel_type in task_set:\n",
    "        '''\n",
    "            Train\n",
    "        '''\n",
    "        loss = edge_loss(gnn, predictor, valid_feature, valid_time, valid_edge_list, batch_size, target_type, source_type, rel_type)\n",
    "        valid_losses += [loss.cpu().detach().tolist()]\n",
    "    print((\"Epoch: %d  LR: %.5f Train Loss: %f  Valid Loss: %s\") % \\\n",
    "          (epoch, optimizer.param_groups[0]['lr'], np.average(train_losses), valid_losses))\n",
    "    '''\n",
    "        Test\n",
    "    '''\n",
    "#     test_feature, test_time, test_edge_list = \\\n",
    "#         graph.sample_subgraph(time_range=test_range, sampled_depth = 5, sampled_number = 128)\n",
    "#     res = []\n",
    "#     for i in range(21):\n",
    "#         ratio = i / 20.\n",
    "#         loss, pred = cpc_loss(test_feature, test_time, test_edge_list, cpc_predictor, batch_size, samp_num, ratio = ratio)\n",
    "#         for i in loss.detach().cpu().tolist():\n",
    "#             res += [[ratio, i]]\n",
    "#         del loss, pred\n",
    "#     sb.lineplot(x=\"ratio\", y=\"loss\",\n",
    "#                  data=pd.DataFrame(res, columns = ['ratio', 'loss']))\n",
    "#     plt.show()\n",
    "    stats += [[train_losses, valid_losses]]\n",
    "    if np.average(valid_losses) < best_val:\n",
    "        best_val = np.average(valid_losses)\n",
    "        torch.save(gnn, './save/mt_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
