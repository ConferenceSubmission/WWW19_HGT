{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/datadrive/data_cs/'\n",
    "batch_size = 64\n",
    "batch_num  = 128\n",
    "epoch_num  = 1000\n",
    "samp_num   = 64 - 1\n",
    "\n",
    "device = torch.device(\"cuda:2\")\n",
    "graph = dill.load(open(data_dir + 'graph.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.edge_list = edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(graph, open(data_dir + 'graph.pk', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = {t: True for t in graph.times if t != None and t < 2015}\n",
    "valid_range = {t: True for t in graph.times if t != None and t >= 2015  and t <= 2016}\n",
    "test_range  = {t: True for t in graph.times if t != None and t > 2016}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_dim, n_hid, num_types, num_relations, n_heads, n_layers, dropout = 0.3):\n",
    "        super(GNN, self).__init__()\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.num_types = num_types\n",
    "        self.in_dim    = in_dim\n",
    "        self.n_hid     = n_hid\n",
    "        self.aggregat_ws   = nn.ModuleList()\n",
    "        self.drop          = nn.Dropout(dropout)\n",
    "        for t in range(num_types):\n",
    "            self.aggregat_ws.append(nn.Linear(in_dim, n_hid))\n",
    "        for l in range(n_layers):\n",
    "            self.gcs.append(RAGCNConv(n_hid, n_hid, num_types, num_relations, n_heads, dropout))\n",
    "    def set_device(self, device):\n",
    "        self.device = device\n",
    "        for gc in self.gcs:\n",
    "            gc.device = device\n",
    "    def forward(self, node_feature, node_type, edge_time, edge_index, edge_type):\n",
    "        res = torch.zeros(node_feature.size(0), self.n_hid).to(node_feature.device)\n",
    "        for t_id in range(self.num_types):\n",
    "            aggregat_w = self.aggregat_ws[t_id]\n",
    "            idx = (node_type == t_id)\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            res[idx] = torch.tanh(aggregat_w(node_feature[idx]))\n",
    "        meta_xs = self.drop(res)\n",
    "        del res\n",
    "        for gc in self.gcs:\n",
    "            meta_xs = gc(meta_xs, node_type, edge_index, edge_type, edge_time)\n",
    "        return meta_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_mask(task_set, feature, time, edge_list, batch_size):\n",
    "    rem_lists   = []\n",
    "    neg_nums    = []\n",
    "    ori_lists   = []\n",
    "    for target_type, source_type, rel_type in task_set:\n",
    "        edges = np.array(edge_list[target_type][source_type][rel_type])\n",
    "        '''\n",
    "            edge: (target_id, source_id)\n",
    "        '''\n",
    "        ori_lists += [edges]\n",
    "        remn  = min((len(edges)-1) // 2, batch_size)\n",
    "        rem_ids = np.random.choice(np.arange(len(edges)), remn, replace = False)\n",
    "        lft_ids = np.array([i for i in range(len(edges)) if i not in rem_ids])\n",
    "        lft_edges = edges[lft_ids]\n",
    "        rem_lists += [edges[rem_ids]]\n",
    "        neg_nums  += [len(time[target_type])]\n",
    "        edge_list[target_type][source_type][rel_type] = list(lft_edges)\n",
    "        edge_list[source_type][target_type]['rev_' + rel_type] = list(np.stack((lft_edges[:,1], lft_edges[:,0])).T)\n",
    "    node_feature, node_type, node_time, edge_index, edge_type, node_dict, _ = to_torch(feature, time, edge_list, graph)\n",
    "    return neg_nums, rem_lists, node_feature, node_type, node_time, edge_index, edge_type, node_dict, ori_lists\n",
    "def mt_sample(seed, time_range, task_set, sampled_depth = 3, sampled_number = 100, batch_size = batch_size):\n",
    "    np.random.seed(seed)\n",
    "    train_feature, train_time, train_edge_list, _ = \\\n",
    "            sample_subgraph(graph, time_range=train_range, sampled_depth = sampled_depth, sampled_number = sampled_number)\n",
    "    return multi_mask(task_set, train_feature, train_time, train_edge_list, batch_size)\n",
    "\n",
    "def prepare_data(pool, process_ids, task_set):\n",
    "    jobs = []\n",
    "    for process_id in process_ids[:-1]:\n",
    "        p = pool.apply_async(mt_sample, args=(np.random.randint(2**32 - 1), train_range, task_set))\n",
    "        jobs.append(p)\n",
    "    p = pool.apply_async(mt_sample, args=(np.random.randint(2**32 - 1), valid_range, task_set))\n",
    "    jobs.append(p)\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = graph.get_types()\n",
    "gnn = GNN(in_dim = len(graph.node_feature['paper']['emb'][0]) + 401, n_hid = 256, num_types = len(types), \\\n",
    "          num_relations = len(graph.get_meta_graph()) + 1, n_heads = 8, n_layers = 3).to(device)\n",
    "fp_predictor = Matcher(256, 8).to(device)\n",
    "vp_predictor = Matcher(256, 8).to(device)\n",
    "pp_predictor = Matcher(256, 8).to(device)\n",
    "ap_predictor = Matcher(256, 8).to(device)\n",
    "ai_predictor = Matcher(256, 8).to(device)\n",
    "model = nn.Sequential(gnn, fp_predictor, vp_predictor, pp_predictor, ap_predictor, ai_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 1000, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 156.5s\n",
      "Epoch: 1 (571.6s)  LR: 0.00073 Train Loss: 7.193311  Valid Loss: 4.212434  Valid Acc: 0.037500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RAGCNConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RelTemporalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 3.1s\n",
      "Epoch: 2 (584.3s)  LR: 0.00091 Train Loss: 4.982474  Valid Loss: 3.974452  Valid Acc: 0.068750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RAGCNConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RelTemporalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 3.0s\n",
      "Epoch: 3 (569.8s)  LR: 0.00100 Train Loss: 4.332902  Valid Loss: 3.829248  Valid Acc: 0.028125\n",
      "Data Preparation: 2.7s\n",
      "Epoch: 4 (599.3s)  LR: 0.00096 Train Loss: 4.035325  Valid Loss: 3.711625  Valid Acc: 0.078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RAGCNConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RelTemporalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 3.7s\n",
      "Epoch: 5 (557.2s)  LR: 0.00082 Train Loss: 3.832633  Valid Loss: 3.623439  Valid Acc: 0.059375\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 6 (572.6s)  LR: 0.00061 Train Loss: 3.657501  Valid Loss: 3.664063  Valid Acc: 0.059375\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 7 (583.7s)  LR: 0.00037 Train Loss: 3.621540  Valid Loss: 3.524964  Valid Acc: 0.053125\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 8 (576.9s)  LR: 0.00016 Train Loss: 3.543070  Valid Loss: 3.515449  Valid Acc: 0.062500\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 9 (589.3s)  LR: 0.00003 Train Loss: 3.500578  Valid Loss: 3.551407  Valid Acc: 0.040625\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 10 (571.6s)  LR: 0.00001 Train Loss: 3.501643  Valid Loss: 3.506605  Valid Acc: 0.071875\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 11 (564.4s)  LR: 0.00010 Train Loss: 3.553417  Valid Loss: 3.338795  Valid Acc: 0.109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RAGCNConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RelTemporalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 3.2s\n",
      "Epoch: 12 (580.0s)  LR: 0.00029 Train Loss: 3.489405  Valid Loss: 3.462789  Valid Acc: 0.071875\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 13 (587.8s)  LR: 0.00052 Train Loss: 3.506860  Valid Loss: 3.220992  Valid Acc: 0.100000\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 14 (564.2s)  LR: 0.00075 Train Loss: 3.522716  Valid Loss: 3.394563  Valid Acc: 0.075000\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 15 (579.2s)  LR: 0.00093 Train Loss: 3.512809  Valid Loss: 3.296485  Valid Acc: 0.068750\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 16 (572.0s)  LR: 0.00100 Train Loss: 3.525861  Valid Loss: 3.317275  Valid Acc: 0.062500\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 17 (584.1s)  LR: 0.00096 Train Loss: 3.501419  Valid Loss: 3.494350  Valid Acc: 0.078125\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 18 (582.4s)  LR: 0.00081 Train Loss: 3.526415  Valid Loss: 3.309427  Valid Acc: 0.056250\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 19 (571.8s)  LR: 0.00059 Train Loss: 3.494969  Valid Loss: 3.510880  Valid Acc: 0.062500\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 20 (582.2s)  LR: 0.00035 Train Loss: 3.470497  Valid Loss: 3.503358  Valid Acc: 0.062500\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 21 (577.1s)  LR: 0.00014 Train Loss: 3.454088  Valid Loss: 3.424884  Valid Acc: 0.078125\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 22 (571.8s)  LR: 0.00002 Train Loss: 3.442096  Valid Loss: 3.282188  Valid Acc: 0.084375\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 23 (567.4s)  LR: 0.00001 Train Loss: 3.458178  Valid Loss: 3.407202  Valid Acc: 0.084375\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 24 (571.7s)  LR: 0.00012 Train Loss: 3.441255  Valid Loss: 3.268255  Valid Acc: 0.103125\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 25 (554.2s)  LR: 0.00031 Train Loss: 3.459111  Valid Loss: 3.404702  Valid Acc: 0.075000\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 26 (565.5s)  LR: 0.00055 Train Loss: 3.417018  Valid Loss: 3.492512  Valid Acc: 0.071875\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 27 (572.5s)  LR: 0.00077 Train Loss: 3.432364  Valid Loss: 3.347802  Valid Acc: 0.071875\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 28 (568.2s)  LR: 0.00094 Train Loss: 3.459103  Valid Loss: 3.482022  Valid Acc: 0.065625\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 29 (568.7s)  LR: 0.00100 Train Loss: 3.458530  Valid Loss: 3.283861  Valid Acc: 0.071875\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 30 (535.8s)  LR: 0.00095 Train Loss: 3.519088  Valid Loss: 3.368577  Valid Acc: 0.068750\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 31 (578.2s)  LR: 0.00079 Train Loss: 3.431163  Valid Loss: 3.495645  Valid Acc: 0.053125\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 32 (572.4s)  LR: 0.00056 Train Loss: 3.434181  Valid Loss: 3.311097  Valid Acc: 0.081250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 33 (562.5s)  LR: 0.00032 Train Loss: 3.418094  Valid Loss: 3.490495  Valid Acc: 0.059375\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 34 (550.5s)  LR: 0.00013 Train Loss: 3.380412  Valid Loss: 3.253126  Valid Acc: 0.084375\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 35 (577.3s)  LR: 0.00001 Train Loss: 3.406133  Valid Loss: 3.172058  Valid Acc: 0.087500\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 36 (572.7s)  LR: 0.00002 Train Loss: 3.342638  Valid Loss: 3.217535  Valid Acc: 0.081250\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 37 (567.8s)  LR: 0.00013 Train Loss: 3.367016  Valid Loss: 3.314999  Valid Acc: 0.103125\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 38 (561.3s)  LR: 0.00033 Train Loss: 3.375493  Valid Loss: 3.307641  Valid Acc: 0.068750\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 39 (582.9s)  LR: 0.00057 Train Loss: 3.349510  Valid Loss: 3.412805  Valid Acc: 0.065625\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 40 (585.9s)  LR: 0.00079 Train Loss: 3.355363  Valid Loss: 3.221232  Valid Acc: 0.062500\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 41 (564.8s)  LR: 0.00095 Train Loss: 3.388296  Valid Loss: 3.466752  Valid Acc: 0.068750\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 42 (577.4s)  LR: 0.00100 Train Loss: 3.429086  Valid Loss: 3.550910  Valid Acc: 0.078125\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 43 (586.6s)  LR: 0.00093 Train Loss: 3.480443  Valid Loss: 3.474206  Valid Acc: 0.062500\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 44 (564.7s)  LR: 0.00077 Train Loss: 3.452209  Valid Loss: 3.289847  Valid Acc: 0.109375\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 45 (577.2s)  LR: 0.00054 Train Loss: 3.430757  Valid Loss: 3.331893  Valid Acc: 0.106250\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 46 (565.2s)  LR: 0.00030 Train Loss: 3.433563  Valid Loss: 3.366231  Valid Acc: 0.078125\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 47 (574.6s)  LR: 0.00011 Train Loss: 3.394294  Valid Loss: 3.302012  Valid Acc: 0.103125\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 48 (594.8s)  LR: 0.00001 Train Loss: 3.368190  Valid Loss: 3.501574  Valid Acc: 0.062500\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 49 (573.9s)  LR: 0.00002 Train Loss: 3.345199  Valid Loss: 3.384143  Valid Acc: 0.084375\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 50 (568.9s)  LR: 0.00015 Train Loss: 3.378633  Valid Loss: 3.385327  Valid Acc: 0.090625\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 51 (574.4s)  LR: 0.00035 Train Loss: 3.363710  Valid Loss: 3.297566  Valid Acc: 0.090625\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 52 (570.7s)  LR: 0.00059 Train Loss: 3.367818  Valid Loss: 3.346414  Valid Acc: 0.078125\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 53 (559.8s)  LR: 0.00081 Train Loss: 3.377626  Valid Loss: 3.488184  Valid Acc: 0.078125\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 54 (584.4s)  LR: 0.00096 Train Loss: 3.417003  Valid Loss: 3.455742  Valid Acc: 0.062500\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 55 (573.2s)  LR: 0.00100 Train Loss: 3.416952  Valid Loss: 3.406851  Valid Acc: 0.081250\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 56 (579.5s)  LR: 0.00092 Train Loss: 3.382409  Valid Loss: 3.423908  Valid Acc: 0.078125\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 57 (575.4s)  LR: 0.00075 Train Loss: 3.398800  Valid Loss: 3.307461  Valid Acc: 0.118750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RAGCNConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RelTemporalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 3.1s\n",
      "Epoch: 58 (572.3s)  LR: 0.00052 Train Loss: 3.425837  Valid Loss: 3.308850  Valid Acc: 0.084375\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 59 (568.0s)  LR: 0.00028 Train Loss: 3.357271  Valid Loss: 3.157411  Valid Acc: 0.137500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RAGCNConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RelTemporalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 3.3s\n",
      "Epoch: 60 (551.5s)  LR: 0.00010 Train Loss: 3.356827  Valid Loss: 3.056056  Valid Acc: 0.115625\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 61 (560.8s)  LR: 0.00001 Train Loss: 3.342914  Valid Loss: 3.184962  Valid Acc: 0.053125\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 62 (578.4s)  LR: 0.00003 Train Loss: 3.331947  Valid Loss: 3.301248  Valid Acc: 0.071875\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 63 (582.9s)  LR: 0.00016 Train Loss: 3.324354  Valid Loss: 3.068801  Valid Acc: 0.109375\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 64 (568.9s)  LR: 0.00038 Train Loss: 3.311912  Valid Loss: 3.339999  Valid Acc: 0.065625\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 65 (588.7s)  LR: 0.00062 Train Loss: 3.293715  Valid Loss: 3.308818  Valid Acc: 0.106250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 66 (565.0s)  LR: 0.00083 Train Loss: 3.367169  Valid Loss: 3.395196  Valid Acc: 0.100000\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 67 (551.4s)  LR: 0.00097 Train Loss: 3.409278  Valid Loss: 3.317049  Valid Acc: 0.090625\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 68 (580.0s)  LR: 0.00100 Train Loss: 3.412192  Valid Loss: 3.248329  Valid Acc: 0.068750\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 69 (577.8s)  LR: 0.00091 Train Loss: 3.386412  Valid Loss: 3.513410  Valid Acc: 0.056250\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 70 (590.0s)  LR: 0.00073 Train Loss: 3.387790  Valid Loss: 3.369448  Valid Acc: 0.093750\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 71 (577.0s)  LR: 0.00049 Train Loss: 3.366691  Valid Loss: 3.258278  Valid Acc: 0.100000\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 72 (566.3s)  LR: 0.00026 Train Loss: 3.370811  Valid Loss: 3.104429  Valid Acc: 0.106250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 73 (569.7s)  LR: 0.00008 Train Loss: 3.347014  Valid Loss: 3.443632  Valid Acc: 0.075000\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 74 (580.8s)  LR: 0.00000 Train Loss: 3.310461  Valid Loss: 3.263129  Valid Acc: 0.100000\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 75 (558.5s)  LR: 0.00004 Train Loss: 3.364279  Valid Loss: 3.121371  Valid Acc: 0.090625\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 76 (572.6s)  LR: 0.00018 Train Loss: 3.335964  Valid Loss: 3.264327  Valid Acc: 0.103125\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 77 (566.2s)  LR: 0.00040 Train Loss: 3.334670  Valid Loss: 3.301938  Valid Acc: 0.103125\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 78 (571.2s)  LR: 0.00064 Train Loss: 3.349520  Valid Loss: 3.218007  Valid Acc: 0.081250\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 79 (561.4s)  LR: 0.00085 Train Loss: 3.393353  Valid Loss: 3.153164  Valid Acc: 0.093750\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 80 (568.5s)  LR: 0.00098 Train Loss: 3.377281  Valid Loss: 3.324982  Valid Acc: 0.065625\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 81 (565.9s)  LR: 0.00099 Train Loss: 3.363045  Valid Loss: 3.137959  Valid Acc: 0.121875\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 82 (569.1s)  LR: 0.00090 Train Loss: 3.380411  Valid Loss: 3.475844  Valid Acc: 0.084375\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 83 (572.1s)  LR: 0.00071 Train Loss: 3.396198  Valid Loss: 3.476881  Valid Acc: 0.081250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 84 (563.5s)  LR: 0.00047 Train Loss: 3.378912  Valid Loss: 3.353006  Valid Acc: 0.075000\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 85 (583.6s)  LR: 0.00024 Train Loss: 3.329605  Valid Loss: 3.327781  Valid Acc: 0.068750\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 86 (564.0s)  LR: 0.00007 Train Loss: 3.380275  Valid Loss: 3.359061  Valid Acc: 0.081250\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 87 (586.2s)  LR: 0.00000 Train Loss: 3.353990  Valid Loss: 3.265946  Valid Acc: 0.112500\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 88 (579.1s)  LR: 0.00005 Train Loss: 3.349940  Valid Loss: 3.336866  Valid Acc: 0.084375\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 89 (555.3s)  LR: 0.00020 Train Loss: 3.383198  Valid Loss: 3.418299  Valid Acc: 0.078125\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 90 (566.8s)  LR: 0.00042 Train Loss: 3.347406  Valid Loss: 3.300667  Valid Acc: 0.075000\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 91 (594.9s)  LR: 0.00066 Train Loss: 3.309460  Valid Loss: 3.134913  Valid Acc: 0.109375\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 92 (572.0s)  LR: 0.00086 Train Loss: 3.362048  Valid Loss: 3.617475  Valid Acc: 0.050000\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 93 (561.4s)  LR: 0.00098 Train Loss: 3.388258  Valid Loss: 3.410309  Valid Acc: 0.059375\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 94 (568.6s)  LR: 0.00099 Train Loss: 3.469795  Valid Loss: 3.397609  Valid Acc: 0.065625\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 95 (580.9s)  LR: 0.00088 Train Loss: 3.489547  Valid Loss: 3.535739  Valid Acc: 0.075000\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 96 (580.2s)  LR: 0.00068 Train Loss: 3.456897  Valid Loss: 3.530022  Valid Acc: 0.046875\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 97 (577.1s)  LR: 0.00045 Train Loss: 3.483800  Valid Loss: 3.318555  Valid Acc: 0.103125\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 98 (587.0s)  LR: 0.00022 Train Loss: 3.420656  Valid Loss: 3.585683  Valid Acc: 0.034375\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 99 (579.7s)  LR: 0.00006 Train Loss: 3.415547  Valid Loss: 3.571957  Valid Acc: 0.075000\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 100 (572.1s)  LR: 0.00000 Train Loss: 3.429789  Valid Loss: 3.184906  Valid Acc: 0.081250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 101 (570.7s)  LR: 0.00006 Train Loss: 3.419804  Valid Loss: 3.423610  Valid Acc: 0.093750\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 102 (580.2s)  LR: 0.00022 Train Loss: 3.399971  Valid Loss: 3.437326  Valid Acc: 0.084375\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 103 (574.9s)  LR: 0.00045 Train Loss: 3.430197  Valid Loss: 3.397721  Valid Acc: 0.078125\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 104 (568.7s)  LR: 0.00068 Train Loss: 3.426123  Valid Loss: 3.501920  Valid Acc: 0.084375\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 105 (575.0s)  LR: 0.00088 Train Loss: 3.457088  Valid Loss: 3.581487  Valid Acc: 0.040625\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 106 (580.1s)  LR: 0.00099 Train Loss: 3.476152  Valid Loss: 3.613094  Valid Acc: 0.084375\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 107 (574.1s)  LR: 0.00098 Train Loss: 3.490692  Valid Loss: 3.323928  Valid Acc: 0.096875\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 108 (597.8s)  LR: 0.00086 Train Loss: 3.455110  Valid Loss: 3.506398  Valid Acc: 0.065625\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 109 (571.0s)  LR: 0.00066 Train Loss: 3.485347  Valid Loss: 3.288070  Valid Acc: 0.081250\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 110 (580.6s)  LR: 0.00042 Train Loss: 3.410859  Valid Loss: 3.344115  Valid Acc: 0.103125\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 111 (562.9s)  LR: 0.00020 Train Loss: 3.419382  Valid Loss: 3.190885  Valid Acc: 0.109375\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 112 (578.3s)  LR: 0.00005 Train Loss: 3.369225  Valid Loss: 3.439789  Valid Acc: 0.075000\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 113 (556.1s)  LR: 0.00000 Train Loss: 3.388855  Valid Loss: 3.480479  Valid Acc: 0.065625\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 114 (585.8s)  LR: 0.00007 Train Loss: 3.362070  Valid Loss: 3.346876  Valid Acc: 0.081250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 115 (571.8s)  LR: 0.00024 Train Loss: 3.387041  Valid Loss: 3.405501  Valid Acc: 0.118750\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 116 (571.8s)  LR: 0.00047 Train Loss: 3.368913  Valid Loss: 3.272759  Valid Acc: 0.068750\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 117 (542.0s)  LR: 0.00071 Train Loss: 3.394451  Valid Loss: 3.475484  Valid Acc: 0.062500\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 118 (526.0s)  LR: 0.00090 Train Loss: 3.445240  Valid Loss: 3.393607  Valid Acc: 0.071875\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 119 (531.0s)  LR: 0.00099 Train Loss: 3.415614  Valid Loss: 3.550590  Valid Acc: 0.068750\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 120 (521.6s)  LR: 0.00098 Train Loss: 3.514859  Valid Loss: 3.633447  Valid Acc: 0.075000\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 121 (526.2s)  LR: 0.00085 Train Loss: 3.488722  Valid Loss: 3.455807  Valid Acc: 0.087500\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 122 (544.2s)  LR: 0.00064 Train Loss: 3.451893  Valid Loss: 3.470840  Valid Acc: 0.068750\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 123 (546.8s)  LR: 0.00040 Train Loss: 3.409315  Valid Loss: 3.320809  Valid Acc: 0.071875\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 124 (518.2s)  LR: 0.00018 Train Loss: 3.445581  Valid Loss: 3.458694  Valid Acc: 0.106250\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 125 (548.1s)  LR: 0.00004 Train Loss: 3.424709  Valid Loss: 3.193162  Valid Acc: 0.096875\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 126 (539.3s)  LR: 0.00000 Train Loss: 3.416424  Valid Loss: 3.352217  Valid Acc: 0.087500\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 127 (536.9s)  LR: 0.00008 Train Loss: 3.421812  Valid Loss: 3.199792  Valid Acc: 0.056250\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 128 (535.1s)  LR: 0.00026 Train Loss: 3.425830  Valid Loss: 3.075396  Valid Acc: 0.112500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 3.4s\n",
      "Epoch: 129 (546.6s)  LR: 0.00049 Train Loss: 3.412929  Valid Loss: 3.390638  Valid Acc: 0.078125\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 130 (529.5s)  LR: 0.00073 Train Loss: 3.420853  Valid Loss: 3.507815  Valid Acc: 0.087500\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 131 (533.1s)  LR: 0.00091 Train Loss: 3.427823  Valid Loss: 3.274329  Valid Acc: 0.068750\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 132 (540.2s)  LR: 0.00100 Train Loss: 3.424181  Valid Loss: 3.505622  Valid Acc: 0.081250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 133 (545.4s)  LR: 0.00097 Train Loss: 3.429288  Valid Loss: 3.520567  Valid Acc: 0.065625\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 134 (533.6s)  LR: 0.00083 Train Loss: 3.412040  Valid Loss: 3.335769  Valid Acc: 0.106250\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 135 (550.2s)  LR: 0.00062 Train Loss: 3.386748  Valid Loss: 3.329660  Valid Acc: 0.062500\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 136 (538.6s)  LR: 0.00038 Train Loss: 3.403004  Valid Loss: 3.120133  Valid Acc: 0.093750\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 137 (518.8s)  LR: 0.00016 Train Loss: 3.397347  Valid Loss: 3.368299  Valid Acc: 0.090625\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 138 (529.1s)  LR: 0.00003 Train Loss: 3.361157  Valid Loss: 3.369500  Valid Acc: 0.096875\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 139 (537.1s)  LR: 0.00001 Train Loss: 3.350383  Valid Loss: 3.343307  Valid Acc: 0.087500\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 140 (513.8s)  LR: 0.00010 Train Loss: 3.389465  Valid Loss: 3.350345  Valid Acc: 0.075000\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 141 (549.1s)  LR: 0.00028 Train Loss: 3.344842  Valid Loss: 3.189771  Valid Acc: 0.096875\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 142 (599.2s)  LR: 0.00052 Train Loss: 3.372937  Valid Loss: 3.410728  Valid Acc: 0.043750\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 143 (565.8s)  LR: 0.00075 Train Loss: 3.407316  Valid Loss: 3.449280  Valid Acc: 0.081250\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 144 (596.8s)  LR: 0.00092 Train Loss: 3.407868  Valid Loss: 3.236538  Valid Acc: 0.065625\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 145 (591.4s)  LR: 0.00100 Train Loss: 3.387298  Valid Loss: 3.269250  Valid Acc: 0.084375\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 146 (597.4s)  LR: 0.00096 Train Loss: 3.403464  Valid Loss: 3.282093  Valid Acc: 0.087500\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 147 (552.4s)  LR: 0.00081 Train Loss: 3.396007  Valid Loss: 3.166520  Valid Acc: 0.106250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 148 (575.4s)  LR: 0.00059 Train Loss: 3.422596  Valid Loss: 3.404517  Valid Acc: 0.081250\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 149 (578.6s)  LR: 0.00035 Train Loss: 3.384012  Valid Loss: 3.131011  Valid Acc: 0.078125\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 150 (574.5s)  LR: 0.00015 Train Loss: 3.376794  Valid Loss: 3.037259  Valid Acc: 0.137500\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 151 (573.0s)  LR: 0.00002 Train Loss: 3.340999  Valid Loss: 3.407967  Valid Acc: 0.078125\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 152 (590.5s)  LR: 0.00001 Train Loss: 3.338032  Valid Loss: 3.209874  Valid Acc: 0.087500\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 153 (584.4s)  LR: 0.00011 Train Loss: 3.365206  Valid Loss: 3.368161  Valid Acc: 0.093750\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 154 (565.1s)  LR: 0.00030 Train Loss: 3.388552  Valid Loss: 3.341565  Valid Acc: 0.084375\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 155 (583.4s)  LR: 0.00054 Train Loss: 3.375523  Valid Loss: 3.214960  Valid Acc: 0.084375\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 156 (572.4s)  LR: 0.00077 Train Loss: 3.387047  Valid Loss: 3.328003  Valid Acc: 0.078125\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 157 (565.5s)  LR: 0.00093 Train Loss: 3.404163  Valid Loss: 3.329267  Valid Acc: 0.090625\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 158 (592.8s)  LR: 0.00100 Train Loss: 3.408919  Valid Loss: 3.166445  Valid Acc: 0.121875\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 159 (560.0s)  LR: 0.00095 Train Loss: 3.429732  Valid Loss: 3.306530  Valid Acc: 0.071875\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 160 (581.3s)  LR: 0.00079 Train Loss: 3.381958  Valid Loss: 3.443991  Valid Acc: 0.056250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 161 (576.9s)  LR: 0.00057 Train Loss: 3.407792  Valid Loss: 3.186157  Valid Acc: 0.103125\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 162 (567.7s)  LR: 0.00033 Train Loss: 3.400856  Valid Loss: 3.529100  Valid Acc: 0.081250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 163 (577.3s)  LR: 0.00013 Train Loss: 3.344079  Valid Loss: 3.214606  Valid Acc: 0.103125\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 164 (593.6s)  LR: 0.00002 Train Loss: 3.355242  Valid Loss: 3.427020  Valid Acc: 0.071875\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 165 (565.8s)  LR: 0.00001 Train Loss: 3.386210  Valid Loss: 3.345453  Valid Acc: 0.087500\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 166 (579.1s)  LR: 0.00013 Train Loss: 3.388727  Valid Loss: 3.389131  Valid Acc: 0.078125\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 167 (582.2s)  LR: 0.00032 Train Loss: 3.352817  Valid Loss: 3.182050  Valid Acc: 0.103125\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 168 (572.4s)  LR: 0.00056 Train Loss: 3.339826  Valid Loss: 3.214150  Valid Acc: 0.062500\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 169 (583.2s)  LR: 0.00079 Train Loss: 3.331577  Valid Loss: 3.354731  Valid Acc: 0.087500\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 170 (557.8s)  LR: 0.00095 Train Loss: 3.385846  Valid Loss: 3.489065  Valid Acc: 0.053125\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 171 (578.8s)  LR: 0.00100 Train Loss: 3.402918  Valid Loss: 3.439828  Valid Acc: 0.090625\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 172 (571.2s)  LR: 0.00094 Train Loss: 3.376153  Valid Loss: 3.294825  Valid Acc: 0.081250\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 173 (573.0s)  LR: 0.00077 Train Loss: 3.383714  Valid Loss: 3.213203  Valid Acc: 0.087500\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 174 (558.6s)  LR: 0.00055 Train Loss: 3.369048  Valid Loss: 3.428578  Valid Acc: 0.112500\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 175 (588.4s)  LR: 0.00031 Train Loss: 3.345230  Valid Loss: 3.248787  Valid Acc: 0.087500\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 176 (571.8s)  LR: 0.00012 Train Loss: 3.389038  Valid Loss: 3.335476  Valid Acc: 0.068750\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 177 (591.5s)  LR: 0.00001 Train Loss: 3.320294  Valid Loss: 3.305098  Valid Acc: 0.087500\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 178 (568.9s)  LR: 0.00002 Train Loss: 3.348027  Valid Loss: 3.254050  Valid Acc: 0.087500\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 179 (608.0s)  LR: 0.00014 Train Loss: 3.321159  Valid Loss: 3.407222  Valid Acc: 0.109375\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 180 (578.2s)  LR: 0.00035 Train Loss: 3.326698  Valid Loss: 3.318455  Valid Acc: 0.109375\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 181 (597.7s)  LR: 0.00059 Train Loss: 3.342991  Valid Loss: 3.331236  Valid Acc: 0.090625\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 182 (572.2s)  LR: 0.00081 Train Loss: 3.363947  Valid Loss: 3.085478  Valid Acc: 0.096875\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 183 (591.3s)  LR: 0.00096 Train Loss: 3.359079  Valid Loss: 3.430425  Valid Acc: 0.065625\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 184 (567.2s)  LR: 0.00100 Train Loss: 3.396812  Valid Loss: 3.473784  Valid Acc: 0.062500\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 185 (563.6s)  LR: 0.00093 Train Loss: 3.402640  Valid Loss: 3.533488  Valid Acc: 0.065625\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 186 (579.6s)  LR: 0.00075 Train Loss: 3.390465  Valid Loss: 3.296850  Valid Acc: 0.075000\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 187 (563.8s)  LR: 0.00052 Train Loss: 3.405754  Valid Loss: 3.171653  Valid Acc: 0.081250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 188 (577.9s)  LR: 0.00029 Train Loss: 3.364811  Valid Loss: 3.409421  Valid Acc: 0.075000\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 189 (582.6s)  LR: 0.00010 Train Loss: 3.340332  Valid Loss: 3.306531  Valid Acc: 0.121875\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 190 (575.4s)  LR: 0.00001 Train Loss: 3.351285  Valid Loss: 3.171743  Valid Acc: 0.115625\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 191 (571.9s)  LR: 0.00003 Train Loss: 3.375024  Valid Loss: 3.300052  Valid Acc: 0.103125\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 192 (569.9s)  LR: 0.00016 Train Loss: 3.359988  Valid Loss: 3.261914  Valid Acc: 0.096875\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 193 (590.4s)  LR: 0.00037 Train Loss: 3.302856  Valid Loss: 3.282885  Valid Acc: 0.093750\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 194 (584.6s)  LR: 0.00061 Train Loss: 3.338515  Valid Loss: 3.156031  Valid Acc: 0.093750\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 195 (578.3s)  LR: 0.00082 Train Loss: 3.352255  Valid Loss: 3.327480  Valid Acc: 0.078125\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 196 (567.7s)  LR: 0.00096 Train Loss: 3.395277  Valid Loss: 3.191773  Valid Acc: 0.140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RAGCNConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RelTemporalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 3.3s\n",
      "Epoch: 197 (579.0s)  LR: 0.00100 Train Loss: 3.386912  Valid Loss: 3.348182  Valid Acc: 0.078125\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 198 (580.4s)  LR: 0.00091 Train Loss: 3.376766  Valid Loss: 3.404176  Valid Acc: 0.090625\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 199 (566.9s)  LR: 0.00073 Train Loss: 3.392063  Valid Loss: 3.289126  Valid Acc: 0.090625\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 200 (560.5s)  LR: 0.00050 Train Loss: 3.378901  Valid Loss: 3.106820  Valid Acc: 0.100000\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 201 (567.6s)  LR: 0.00027 Train Loss: 3.341902  Valid Loss: 3.436722  Valid Acc: 0.093750\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 202 (569.3s)  LR: 0.00009 Train Loss: 3.348789  Valid Loss: 3.137346  Valid Acc: 0.115625\n",
      "Data Preparation: 3.6s\n",
      "Epoch: 203 (581.4s)  LR: 0.00000 Train Loss: 3.323877  Valid Loss: 3.181622  Valid Acc: 0.078125\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 204 (554.3s)  LR: 0.00004 Train Loss: 3.390142  Valid Loss: 3.315120  Valid Acc: 0.087500\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 205 (602.6s)  LR: 0.00018 Train Loss: 3.304029  Valid Loss: 3.304127  Valid Acc: 0.078125\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 206 (578.2s)  LR: 0.00039 Train Loss: 3.365895  Valid Loss: 3.405040  Valid Acc: 0.081250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 207 (575.8s)  LR: 0.00063 Train Loss: 3.345958  Valid Loss: 3.352144  Valid Acc: 0.112500\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 208 (565.0s)  LR: 0.00084 Train Loss: 3.363618  Valid Loss: 3.297865  Valid Acc: 0.087500\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 209 (571.0s)  LR: 0.00097 Train Loss: 3.372484  Valid Loss: 3.295772  Valid Acc: 0.081250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 210 (576.8s)  LR: 0.00099 Train Loss: 3.344511  Valid Loss: 3.246143  Valid Acc: 0.075000\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 211 (583.5s)  LR: 0.00090 Train Loss: 3.353716  Valid Loss: 3.285258  Valid Acc: 0.134375\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 212 (578.0s)  LR: 0.00071 Train Loss: 3.347081  Valid Loss: 3.351229  Valid Acc: 0.081250\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 213 (567.1s)  LR: 0.00048 Train Loss: 3.391846  Valid Loss: 3.156158  Valid Acc: 0.103125\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 214 (603.6s)  LR: 0.00025 Train Loss: 3.324673  Valid Loss: 3.386605  Valid Acc: 0.093750\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 215 (595.2s)  LR: 0.00007 Train Loss: 3.273897  Valid Loss: 3.076459  Valid Acc: 0.087500\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 216 (567.6s)  LR: 0.00000 Train Loss: 3.303337  Valid Loss: 3.225029  Valid Acc: 0.084375\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 217 (566.8s)  LR: 0.00005 Train Loss: 3.345182  Valid Loss: 3.253208  Valid Acc: 0.103125\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 218 (576.1s)  LR: 0.00019 Train Loss: 3.320357  Valid Loss: 3.378146  Valid Acc: 0.087500\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 219 (580.3s)  LR: 0.00041 Train Loss: 3.286449  Valid Loss: 3.291262  Valid Acc: 0.100000\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 220 (587.2s)  LR: 0.00065 Train Loss: 3.307460  Valid Loss: 3.235488  Valid Acc: 0.096875\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 221 (585.7s)  LR: 0.00086 Train Loss: 3.353414  Valid Loss: 3.268917  Valid Acc: 0.084375\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 222 (574.9s)  LR: 0.00098 Train Loss: 3.335525  Valid Loss: 3.105772  Valid Acc: 0.096875\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 223 (580.8s)  LR: 0.00099 Train Loss: 3.349145  Valid Loss: 3.178061  Valid Acc: 0.106250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 224 (593.0s)  LR: 0.00089 Train Loss: 3.308692  Valid Loss: 3.444393  Valid Acc: 0.084375\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 225 (568.4s)  LR: 0.00069 Train Loss: 3.347699  Valid Loss: 3.155257  Valid Acc: 0.121875\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 226 (597.6s)  LR: 0.00045 Train Loss: 3.332466  Valid Loss: 3.244795  Valid Acc: 0.115625\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 227 (568.8s)  LR: 0.00023 Train Loss: 3.315984  Valid Loss: 3.142075  Valid Acc: 0.100000\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 228 (586.7s)  LR: 0.00006 Train Loss: 3.303101  Valid Loss: 3.121515  Valid Acc: 0.118750\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 229 (572.9s)  LR: 0.00000 Train Loss: 3.275404  Valid Loss: 3.336108  Valid Acc: 0.084375\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 230 (575.5s)  LR: 0.00006 Train Loss: 3.278426  Valid Loss: 3.350355  Valid Acc: 0.071875\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 231 (578.9s)  LR: 0.00021 Train Loss: 3.299002  Valid Loss: 3.144172  Valid Acc: 0.087500\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 232 (585.5s)  LR: 0.00044 Train Loss: 3.297955  Valid Loss: 3.300443  Valid Acc: 0.087500\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 233 (586.9s)  LR: 0.00068 Train Loss: 3.303596  Valid Loss: 3.217053  Valid Acc: 0.125000\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 234 (589.1s)  LR: 0.00088 Train Loss: 3.280707  Valid Loss: 3.310222  Valid Acc: 0.084375\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 235 (569.6s)  LR: 0.00099 Train Loss: 3.329364  Valid Loss: 3.334587  Valid Acc: 0.081250\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 236 (574.4s)  LR: 0.00098 Train Loss: 3.343687  Valid Loss: 3.324452  Valid Acc: 0.103125\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 237 (559.3s)  LR: 0.00087 Train Loss: 3.369283  Valid Loss: 3.409056  Valid Acc: 0.046875\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 238 (556.0s)  LR: 0.00067 Train Loss: 3.332468  Valid Loss: 3.185599  Valid Acc: 0.106250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 239 (577.5s)  LR: 0.00043 Train Loss: 3.314314  Valid Loss: 3.371594  Valid Acc: 0.059375\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 240 (576.1s)  LR: 0.00021 Train Loss: 3.282023  Valid Loss: 3.274810  Valid Acc: 0.081250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 241 (585.5s)  LR: 0.00005 Train Loss: 3.277647  Valid Loss: 3.073445  Valid Acc: 0.103125\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 242 (581.4s)  LR: 0.00000 Train Loss: 3.292903  Valid Loss: 3.268660  Valid Acc: 0.084375\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 243 (577.6s)  LR: 0.00007 Train Loss: 3.276230  Valid Loss: 3.309899  Valid Acc: 0.093750\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 244 (571.7s)  LR: 0.00023 Train Loss: 3.317157  Valid Loss: 3.207460  Valid Acc: 0.071875\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 245 (597.2s)  LR: 0.00046 Train Loss: 3.309849  Valid Loss: 3.155822  Valid Acc: 0.087500\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 246 (561.1s)  LR: 0.00070 Train Loss: 3.289816  Valid Loss: 3.243753  Valid Acc: 0.096875\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 247 (588.8s)  LR: 0.00089 Train Loss: 3.302313  Valid Loss: 3.418000  Valid Acc: 0.071875\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 248 (578.8s)  LR: 0.00099 Train Loss: 3.305529  Valid Loss: 3.350660  Valid Acc: 0.046875\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 249 (582.7s)  LR: 0.00098 Train Loss: 3.312232  Valid Loss: 3.347284  Valid Acc: 0.106250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 250 (569.4s)  LR: 0.00085 Train Loss: 3.376391  Valid Loss: 3.378994  Valid Acc: 0.071875\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 251 (579.2s)  LR: 0.00065 Train Loss: 3.351060  Valid Loss: 3.237047  Valid Acc: 0.081250\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 252 (588.3s)  LR: 0.00041 Train Loss: 3.348874  Valid Loss: 3.310331  Valid Acc: 0.075000\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 253 (582.1s)  LR: 0.00019 Train Loss: 3.341424  Valid Loss: 3.061302  Valid Acc: 0.071875\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 254 (598.6s)  LR: 0.00004 Train Loss: 3.296535  Valid Loss: 3.389375  Valid Acc: 0.081250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 255 (566.6s)  LR: 0.00000 Train Loss: 3.325035  Valid Loss: 3.285049  Valid Acc: 0.087500\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 256 (581.1s)  LR: 0.00008 Train Loss: 3.289458  Valid Loss: 3.057330  Valid Acc: 0.112500\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 257 (567.3s)  LR: 0.00025 Train Loss: 3.323522  Valid Loss: 3.423417  Valid Acc: 0.081250\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 258 (584.1s)  LR: 0.00048 Train Loss: 3.283710  Valid Loss: 3.411300  Valid Acc: 0.078125\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 259 (568.3s)  LR: 0.00072 Train Loss: 3.348175  Valid Loss: 3.263322  Valid Acc: 0.100000\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 260 (540.2s)  LR: 0.00090 Train Loss: 3.323607  Valid Loss: 3.343766  Valid Acc: 0.084375\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 261 (534.2s)  LR: 0.00100 Train Loss: 3.307723  Valid Loss: 3.448364  Valid Acc: 0.062500\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 262 (538.4s)  LR: 0.00097 Train Loss: 3.370446  Valid Loss: 3.280675  Valid Acc: 0.071875\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 263 (549.3s)  LR: 0.00084 Train Loss: 3.309067  Valid Loss: 3.309087  Valid Acc: 0.084375\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 264 (525.0s)  LR: 0.00062 Train Loss: 3.373572  Valid Loss: 3.593887  Valid Acc: 0.050000\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 265 (545.8s)  LR: 0.00038 Train Loss: 3.295783  Valid Loss: 3.168077  Valid Acc: 0.096875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 3.1s\n",
      "Epoch: 266 (540.1s)  LR: 0.00017 Train Loss: 3.333254  Valid Loss: 3.247920  Valid Acc: 0.090625\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 267 (541.1s)  LR: 0.00003 Train Loss: 3.294555  Valid Loss: 3.290626  Valid Acc: 0.096875\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 268 (516.2s)  LR: 0.00000 Train Loss: 3.320540  Valid Loss: 3.061470  Valid Acc: 0.115625\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 269 (545.4s)  LR: 0.00009 Train Loss: 3.317026  Valid Loss: 3.172194  Valid Acc: 0.109375\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 270 (528.3s)  LR: 0.00027 Train Loss: 3.300713  Valid Loss: 3.121057  Valid Acc: 0.128125\n",
      "Data Preparation: 3.6s\n",
      "Epoch: 271 (549.9s)  LR: 0.00051 Train Loss: 3.314428  Valid Loss: 3.116534  Valid Acc: 0.103125\n",
      "Data Preparation: 3.6s\n",
      "Epoch: 272 (566.4s)  LR: 0.00074 Train Loss: 3.286019  Valid Loss: 3.257643  Valid Acc: 0.109375\n",
      "Data Preparation: 3.6s\n",
      "Epoch: 273 (538.6s)  LR: 0.00092 Train Loss: 3.338220  Valid Loss: 3.050644  Valid Acc: 0.121875\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 274 (528.9s)  LR: 0.00100 Train Loss: 3.339646  Valid Loss: 3.214152  Valid Acc: 0.087500\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 275 (520.4s)  LR: 0.00096 Train Loss: 3.355218  Valid Loss: 3.261716  Valid Acc: 0.081250\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 276 (533.5s)  LR: 0.00082 Train Loss: 3.342781  Valid Loss: 3.134486  Valid Acc: 0.121875\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 277 (510.3s)  LR: 0.00060 Train Loss: 3.325271  Valid Loss: 3.451520  Valid Acc: 0.084375\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 278 (538.9s)  LR: 0.00036 Train Loss: 3.326490  Valid Loss: 3.146678  Valid Acc: 0.093750\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 279 (532.4s)  LR: 0.00015 Train Loss: 3.293678  Valid Loss: 3.320541  Valid Acc: 0.081250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 280 (545.0s)  LR: 0.00003 Train Loss: 3.310500  Valid Loss: 3.146363  Valid Acc: 0.143750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RAGCNConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type RelTemporalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 3.4s\n",
      "Epoch: 281 (546.5s)  LR: 0.00001 Train Loss: 3.296891  Valid Loss: 3.284798  Valid Acc: 0.131250\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 282 (555.1s)  LR: 0.00011 Train Loss: 3.266341  Valid Loss: 3.208198  Valid Acc: 0.121875\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 283 (532.1s)  LR: 0.00029 Train Loss: 3.293627  Valid Loss: 3.152039  Valid Acc: 0.087500\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 284 (534.8s)  LR: 0.00053 Train Loss: 3.285497  Valid Loss: 3.104111  Valid Acc: 0.096875\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 285 (543.7s)  LR: 0.00076 Train Loss: 3.293271  Valid Loss: 3.422727  Valid Acc: 0.090625\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 286 (538.1s)  LR: 0.00093 Train Loss: 3.319184  Valid Loss: 3.382679  Valid Acc: 0.100000\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 287 (538.5s)  LR: 0.00100 Train Loss: 3.307029  Valid Loss: 3.329804  Valid Acc: 0.087500\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 288 (543.8s)  LR: 0.00095 Train Loss: 3.332467  Valid Loss: 3.124009  Valid Acc: 0.140625\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 289 (551.0s)  LR: 0.00080 Train Loss: 3.335779  Valid Loss: 3.331523  Valid Acc: 0.134375\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 290 (546.1s)  LR: 0.00058 Train Loss: 3.348355  Valid Loss: 3.084157  Valid Acc: 0.121875\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 291 (552.1s)  LR: 0.00034 Train Loss: 3.328036  Valid Loss: 3.254441  Valid Acc: 0.084375\n",
      "Data Preparation: 3.6s\n",
      "Epoch: 292 (556.8s)  LR: 0.00014 Train Loss: 3.286837  Valid Loss: 3.101809  Valid Acc: 0.093750\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 293 (542.3s)  LR: 0.00002 Train Loss: 3.284857  Valid Loss: 3.117564  Valid Acc: 0.087500\n",
      "Data Preparation: 3.5s\n",
      "Epoch: 294 (563.7s)  LR: 0.00001 Train Loss: 3.298311  Valid Loss: 3.346773  Valid Acc: 0.087500\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 295 (592.5s)  LR: 0.00012 Train Loss: 3.262700  Valid Loss: 3.160199  Valid Acc: 0.078125\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 296 (582.7s)  LR: 0.00032 Train Loss: 3.310535  Valid Loss: 3.045604  Valid Acc: 0.118750\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 297 (591.9s)  LR: 0.00056 Train Loss: 3.333132  Valid Loss: 3.410871  Valid Acc: 0.081250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 298 (583.2s)  LR: 0.00078 Train Loss: 3.359865  Valid Loss: 3.426922  Valid Acc: 0.115625\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 299 (562.5s)  LR: 0.00094 Train Loss: 3.358980  Valid Loss: 3.220303  Valid Acc: 0.087500\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 300 (581.1s)  LR: 0.00100 Train Loss: 3.345201  Valid Loss: 3.295468  Valid Acc: 0.100000\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 301 (581.4s)  LR: 0.00094 Train Loss: 3.351945  Valid Loss: 3.348598  Valid Acc: 0.087500\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 302 (580.1s)  LR: 0.00078 Train Loss: 3.390225  Valid Loss: 3.488763  Valid Acc: 0.075000\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 303 (590.0s)  LR: 0.00056 Train Loss: 3.367094  Valid Loss: 3.393349  Valid Acc: 0.103125\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 304 (591.0s)  LR: 0.00032 Train Loss: 3.356984  Valid Loss: 3.350853  Valid Acc: 0.075000\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 305 (576.8s)  LR: 0.00012 Train Loss: 3.351114  Valid Loss: 3.324933  Valid Acc: 0.087500\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 306 (570.4s)  LR: 0.00001 Train Loss: 3.342337  Valid Loss: 3.300666  Valid Acc: 0.115625\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 307 (584.4s)  LR: 0.00002 Train Loss: 3.332175  Valid Loss: 3.278024  Valid Acc: 0.096875\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 308 (584.4s)  LR: 0.00014 Train Loss: 3.321185  Valid Loss: 3.391544  Valid Acc: 0.087500\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 309 (564.2s)  LR: 0.00034 Train Loss: 3.346531  Valid Loss: 3.390327  Valid Acc: 0.081250\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 310 (578.3s)  LR: 0.00058 Train Loss: 3.358111  Valid Loss: 3.217956  Valid Acc: 0.078125\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 311 (568.8s)  LR: 0.00080 Train Loss: 3.337060  Valid Loss: 3.341675  Valid Acc: 0.071875\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 312 (588.7s)  LR: 0.00095 Train Loss: 3.325570  Valid Loss: 3.185807  Valid Acc: 0.093750\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 313 (590.8s)  LR: 0.00100 Train Loss: 3.344535  Valid Loss: 3.427820  Valid Acc: 0.075000\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 314 (578.4s)  LR: 0.00093 Train Loss: 3.330908  Valid Loss: 3.254938  Valid Acc: 0.087500\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 315 (581.2s)  LR: 0.00076 Train Loss: 3.368243  Valid Loss: 3.172584  Valid Acc: 0.096875\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 316 (591.8s)  LR: 0.00053 Train Loss: 3.351465  Valid Loss: 3.384021  Valid Acc: 0.112500\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 317 (588.0s)  LR: 0.00029 Train Loss: 3.359393  Valid Loss: 3.418563  Valid Acc: 0.078125\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 318 (568.3s)  LR: 0.00011 Train Loss: 3.336591  Valid Loss: 3.230686  Valid Acc: 0.118750\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 319 (587.3s)  LR: 0.00001 Train Loss: 3.309446  Valid Loss: 3.315261  Valid Acc: 0.090625\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 320 (594.3s)  LR: 0.00003 Train Loss: 3.324441  Valid Loss: 3.264987  Valid Acc: 0.100000\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 321 (583.3s)  LR: 0.00015 Train Loss: 3.312809  Valid Loss: 3.306764  Valid Acc: 0.084375\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 322 (591.0s)  LR: 0.00036 Train Loss: 3.317413  Valid Loss: 3.260249  Valid Acc: 0.100000\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 323 (594.1s)  LR: 0.00060 Train Loss: 3.304358  Valid Loss: 3.223530  Valid Acc: 0.100000\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 324 (591.0s)  LR: 0.00082 Train Loss: 3.304835  Valid Loss: 3.299741  Valid Acc: 0.071875\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 325 (587.5s)  LR: 0.00096 Train Loss: 3.358402  Valid Loss: 3.460611  Valid Acc: 0.103125\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 326 (581.5s)  LR: 0.00100 Train Loss: 3.364502  Valid Loss: 3.386028  Valid Acc: 0.078125\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 327 (570.9s)  LR: 0.00092 Train Loss: 3.386838  Valid Loss: 3.252548  Valid Acc: 0.100000\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 328 (598.1s)  LR: 0.00074 Train Loss: 3.355149  Valid Loss: 3.186197  Valid Acc: 0.106250\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 329 (578.0s)  LR: 0.00051 Train Loss: 3.384052  Valid Loss: 3.178689  Valid Acc: 0.087500\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 330 (573.9s)  LR: 0.00027 Train Loss: 3.350580  Valid Loss: 3.326377  Valid Acc: 0.065625\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 331 (586.4s)  LR: 0.00009 Train Loss: 3.323444  Valid Loss: 3.271532  Valid Acc: 0.109375\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 332 (576.0s)  LR: 0.00000 Train Loss: 3.345876  Valid Loss: 3.192263  Valid Acc: 0.115625\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 333 (582.4s)  LR: 0.00003 Train Loss: 3.312837  Valid Loss: 3.302196  Valid Acc: 0.068750\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 334 (578.4s)  LR: 0.00017 Train Loss: 3.335994  Valid Loss: 3.126225  Valid Acc: 0.118750\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 335 (587.6s)  LR: 0.00038 Train Loss: 3.308492  Valid Loss: 3.147400  Valid Acc: 0.115625\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 336 (576.7s)  LR: 0.00062 Train Loss: 3.332124  Valid Loss: 3.149150  Valid Acc: 0.100000\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 337 (585.7s)  LR: 0.00084 Train Loss: 3.316927  Valid Loss: 3.416772  Valid Acc: 0.078125\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 338 (583.3s)  LR: 0.00097 Train Loss: 3.343126  Valid Loss: 3.284655  Valid Acc: 0.084375\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 339 (593.4s)  LR: 0.00100 Train Loss: 3.346364  Valid Loss: 3.171690  Valid Acc: 0.118750\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 340 (582.5s)  LR: 0.00090 Train Loss: 3.363219  Valid Loss: 3.369943  Valid Acc: 0.068750\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 341 (586.7s)  LR: 0.00072 Train Loss: 3.367630  Valid Loss: 3.371375  Valid Acc: 0.050000\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 342 (604.1s)  LR: 0.00048 Train Loss: 3.336581  Valid Loss: 3.184691  Valid Acc: 0.106250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 343 (572.7s)  LR: 0.00025 Train Loss: 3.328241  Valid Loss: 3.348680  Valid Acc: 0.106250\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 344 (569.5s)  LR: 0.00008 Train Loss: 3.318246  Valid Loss: 3.427919  Valid Acc: 0.106250\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 345 (573.1s)  LR: 0.00000 Train Loss: 3.323333  Valid Loss: 3.343588  Valid Acc: 0.100000\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 346 (586.0s)  LR: 0.00004 Train Loss: 3.315393  Valid Loss: 3.252717  Valid Acc: 0.090625\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 347 (567.1s)  LR: 0.00019 Train Loss: 3.338870  Valid Loss: 3.402808  Valid Acc: 0.071875\n",
      "Data Preparation: 3.3s\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "models = [fp_predictor, vp_predictor, pp_predictor, ap_predictor, ai_predictor]\n",
    "task_set = [['field', 'paper', 'PF_in_L2'],\\\n",
    "            ['venue', 'paper', 'PV_Conference'],\\\n",
    "            ['paper', 'paper', 'PP_cite'],\\\n",
    "            ['paper', 'author', 'AP_write_first'],\\\n",
    "            ['affiliation', 'author', 'in']]\n",
    "\n",
    "pool = mp.Pool(4)\n",
    "process_ids = np.arange(batch_num // 4)\n",
    "st = time.time()\n",
    "jobs = prepare_data(pool, process_ids, task_set)\n",
    "train_step = 1500\n",
    "best_val   = 0\n",
    "\n",
    "for epoch in np.arange(epoch_num)+1:\n",
    "    train_data = [job.get() for job in jobs[:-1]]\n",
    "    valid_data = jobs[-1].get()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    pool = mp.Pool(4)\n",
    "    jobs = prepare_data(pool, process_ids, task_set)\n",
    "    et = time.time()\n",
    "    print('Data Preparation: %.1fs' % (et - st))\n",
    "    \n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "    for neg_nums, rem_lists, node_feature, node_type, node_time, edge_index, edge_type, node_dict, ori_lists in train_data:\n",
    "        for target_size, predictor, rem_edges, ori_edges, (target_type, source_type, rel_type) in \\\n",
    "                zip(neg_nums, models, rem_lists, ori_lists, task_set):\n",
    "            '''\n",
    "                Train\n",
    "            '''\n",
    "            sn = np.min([target_size-1, samp_num])\n",
    "            positive_target_ids, source_ids = rem_edges[:,0].reshape(-1, 1), rem_edges[:,1]\n",
    "            negative_target_ids = np.array([neg_sample(target_size, sn, \\\n",
    "                edges[0][edges[1] == s_id].tolist()) for edges, s_id in zip(ori_edges, source_ids)])\n",
    "            target_ids = np.concatenate((positive_target_ids, negative_target_ids), axis=-1) + node_dict[target_type][0]\n",
    "\n",
    "            node_emb = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                        node_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "            source_emb = node_emb[source_ids].repeat(1, target_ids.shape[1]\\\n",
    "                                              ).view(target_ids.shape[0] * target_ids.shape[1], -1)\n",
    "\n",
    "            target_emb = node_emb[target_ids].view(target_ids.shape[0] * target_ids.shape[1], -1)\n",
    "            res = predictor.forward(source_emb, target_emb, pair=True).view(-1, target_ids.shape[1])\n",
    "            loss = -torch.log_softmax(res, dim=-1)[:,0].mean()\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.2)\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            train_losses += [loss.cpu().detach().tolist()]\n",
    "            train_step += 1\n",
    "            scheduler.step(train_step)\n",
    "    '''\n",
    "        Valid\n",
    "    '''\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        neg_nums, rem_lists, node_feature, node_type, node_time, edge_index, edge_type, node_dict, ori_lists = valid_data\n",
    "        valid_losses = []\n",
    "        valid_accs   = []\n",
    "        for target_size, predictor, rem_edges, ori_edges, (target_type, source_type, rel_type) in \\\n",
    "                zip(neg_nums, models, rem_lists, ori_lists, task_set):\n",
    "            '''\n",
    "                Valid\n",
    "            '''\n",
    "            positive_target_ids, source_ids = rem_edges[:,0].reshape(-1, 1), rem_edges[:,1]\n",
    "            negative_target_ids = np.array([neg_sample(target_size, sn, \\\n",
    "                edges[0][edges[1] == s_id].tolist()) for edges, s_id in zip(ori_edges, source_ids)])\n",
    "            target_ids = np.concatenate((positive_target_ids, negative_target_ids), axis=-1) + node_dict[target_type][0]\n",
    "\n",
    "            node_emb = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                        node_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "            source_emb = node_emb[source_ids].repeat(1, target_ids.shape[1]\\\n",
    "                                              ).view(target_ids.shape[0] * target_ids.shape[1], -1)\n",
    "\n",
    "            target_emb = node_emb[target_ids].view(target_ids.shape[0] * target_ids.shape[1], -1)\n",
    "            res = predictor.forward(source_emb, target_emb, pair=True).view(-1, target_ids.shape[1])\n",
    "            valid_losses +=  [-torch.log_softmax(res, dim=-1)[:,0]]\n",
    "            s = (res.argmax(dim=1) == 0).sum()\n",
    "            valid_accs += [s.tolist() / batch_size]\n",
    "        valid_losses = torch.cat(valid_losses).cpu().detach().tolist()\n",
    "        s = (res.argmax(dim=1) == 0).sum()\n",
    "        st = time.time()\n",
    "        print((\"Epoch: %d (%.1fs)  LR: %.5f Train Loss: %f  Valid Loss: %f  Valid Acc: %f\") % \\\n",
    "              (epoch, (st-et), optimizer.param_groups[0]['lr'], np.average(train_losses), np.average(valid_losses), np.average(valid_accs)))\n",
    "        if np.average(valid_accs) > best_val:\n",
    "            best_val = np.average(valid_accs)\n",
    "            torch.save(gnn, './save/mt_model.pt')\n",
    "        stats += [[train_losses, valid_losses]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_sample(size, num, pos_id):\n",
    "    s = np.arange(size)\n",
    "    np.random.shuffle(s)\n",
    "    return [si for si in s if si not in pos_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-50bfb00dadeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m             ori_edges[:, 0][ori_edges[:, 1] == s_id].tolist()) for  s_id in source_ids])\n\u001b[1;32m     15\u001b[0m         \u001b[0msource_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_ids\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnode_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtarget_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_target_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_target_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnode_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         node_emb = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    neg_nums, rem_lists, node_feature, node_type, node_time, edge_index, edge_type, node_dict, ori_lists = valid_data\n",
    "    valid_losses = []\n",
    "    valid_accs   = []\n",
    "    for target_size, predictor, rem_edges, ori_edges, (target_type, source_type, rel_type) in \\\n",
    "            zip(neg_nums, models, rem_lists, ori_lists, task_set):\n",
    "        '''\n",
    "            Valid\n",
    "        '''\n",
    "        positive_target_ids, source_ids = rem_edges[:,0].reshape(-1, 1), rem_edges[:,1]\n",
    "        negative_target_ids = np.array([neg_sample(target_size, sn, \\\n",
    "            ori_edges[:, 0][ori_edges[:, 1] == s_id].tolist()) for  s_id in source_ids])\n",
    "        \n",
    "        \n",
    "        sn = np.min([len(_id) for _id in negative_target_ids] + [samp_num])\n",
    "        source_ids = source_ids + node_dict[source_type][0]\n",
    "        target_ids = np.concatenate((positive_target_ids, negative_target_ids), axis=-1) + node_dict[target_type][0]\n",
    "        \n",
    "        node_emb = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                    node_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "        source_emb = node_emb[source_ids].repeat(1, target_ids.shape[1]\\\n",
    "                                          ).view(target_ids.shape[0] * target_ids.shape[1], -1)\n",
    "\n",
    "        target_emb = node_emb[target_ids].view(target_ids.shape[0] * target_ids.shape[1], -1)\n",
    "        res = predictor.forward(source_emb, target_emb, pair=True).view(-1, target_ids.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = torch.load('./save/mt_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "for s in graph.edge_list:\n",
    "    for t in graph.edge_list[s]:\n",
    "        for e in graph.edge_list[s][t]:\n",
    "            for i in graph.edge_list[s][t][e]:\n",
    "                size += len(graph.edge_list[s][t][e][i])\n",
    "size\n",
    "size = 0\n",
    "for s in graph.node_feature:\n",
    "    size += len(graph.node_feature[s])\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 0\n",
    "for s in graph.node_feature:\n",
    "    size += len(graph.node_feature[s])\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "470"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(list(graph.node_feature['venue']['attr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.node_feature['venue'][graph.node_feature['venue']['attr']=='Patent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1116163, 5574903, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topconf = graph.node_feature['venue'].nlargest(256, 'citation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list(graph.node_feature['venue'][graph.node_feature['venue']['name'] == 'WWW']['emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = euclidean_distances(res, list(topconf['emb']))[0]\n",
    "for li in np.argsort(cos)[:50]:\n",
    "    print(list(topconf['name'])[li])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn.to(device)\n",
    "gnn.eval()\n",
    "topconf = graph.node_feature['venue'].nlargest(100, 'citation')\n",
    "with torch.no_grad():\n",
    "    _time = 2000\n",
    "    vids = np.stack([graph.node_feature['venue'].nlargest(100, 'citation').index.values, np.repeat([_time], 100)]).T\n",
    "    conf_emb1 = []\n",
    "    conf_emb2 = []\n",
    "    for i in range(10):\n",
    "        print(i)\n",
    "        feature, times, edge_list, _ = sample_subgraph(graph, {t: True for t in graph.times if t != None}, \\\n",
    "                        inp = {'venue': vids}, sampled_depth = 4, sampled_number = 256)\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict = \\\n",
    "                    to_torch(feature, times, edge_list, graph)\n",
    "        venue_ids = np.arange(len(vids)) + node_dict['venue'][0]\n",
    "\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type = node_feature.to(device), node_type.to(device), \\\n",
    "                              edge_time.to(device), edge_index.to(device), edge_type.to(device)\n",
    "        res = torch.zeros(node_feature.size(0), gnn.n_hid).to(node_feature.device)\n",
    "        for t_id in range(gnn.num_types):\n",
    "            aggregat_w = gnn.aggregat_ws[t_id]\n",
    "            idx = (node_type == t_id)\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            res[idx] = torch.tanh(aggregat_w(node_feature[idx]))\n",
    "        meta_xs = gnn.drop(res)\n",
    "        del res\n",
    "        meta_xs = gnn.gcs[0](meta_xs, node_type, edge_index, edge_type, edge_time)\n",
    "        emb = meta_xs[venue_ids].cpu().detach().numpy()\n",
    "        conf_emb1 += [emb]\n",
    "        cos = cosine_similarity(emb)\n",
    "        for li in np.argsort(-cos[list(topconf['name']).index('KDD')])[:50]:\n",
    "            print(list(topconf['name'])[li])\n",
    "        print('-' * 100)\n",
    "        dis = euclidean_distances(emb)\n",
    "        for li in np.argsort(dis[list(topconf['name']).index('KDD')])[:50]:\n",
    "            print(list(topconf['name'])[li])\n",
    "        print('=' * 100)\n",
    "        \n",
    "        \n",
    "        meta_xs = gnn.gcs[1](meta_xs, node_type, edge_index, edge_type, edge_time)\n",
    "        emb = meta_xs[venue_ids].cpu().detach().numpy()\n",
    "        conf_emb2 += [emb]\n",
    "        cos = cosine_similarity(emb)\n",
    "        for li in np.argsort(-cos[list(topconf['name']).index('KDD')])[:50]:\n",
    "            print(list(topconf['name'])[li])\n",
    "        print('-' * 100)\n",
    "        dis = euclidean_distances(emb)\n",
    "        for li in np.argsort(dis[list(topconf['name']).index('KDD')])[:50]:\n",
    "            print(list(topconf['name'])[li])\n",
    "        print('=' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coss = [euclidean_distances(emb) for emb in conf_emb1]\n",
    "cos = np.average(coss, axis=0)\n",
    "for li in np.argsort(cos[list(topconf['name']).index('ACL')])[:50]:\n",
    "    print(list(topconf['name'])[li])\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coss = [euclidean_distances(emb) for emb in conf_emb2]\n",
    "cos = np.average(coss, axis=0)\n",
    "for li in np.argsort(cos[list(topconf['name']).index('ACL')])[:50]:\n",
    "    print(list(topconf['name'])[li])\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_emb = {2010: [conf_emb1, conf_emb2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_emb[1990] = [conf_emb1, conf_emb2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_emb[2030] = [conf_emb1, conf_emb2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_emb[1990][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict = {j: i for i, j in enumerate(graph.get_types())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_dict = {e[2]: i for i, e in enumerate(graph.get_meta_graph())}\n",
    "edge_dict['self'] = len(edge_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_s = 0\n",
    "for i in graph.edge_list['author']['affiliation']['rev_in']:\n",
    "    _s += len(graph.edge_list['author']['affiliation']['rev_in'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(graph.edge_list['author']['affiliation']['rev_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn, _ = torch.load('../paper-field-L2/save/rgt_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, j, k in graph.get_meta_graph():\n",
    "    if i == 'affiliation':\n",
    "        print(i,j,k)\n",
    "        print(gnn.gcs[0].relation_ws[node_dict[i]][edge_dict[k]][node_dict[j]].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper venue rev_PV_Conference\n",
    "tensor(1.0509, device='cuda:1', grad_fn=<MeanBackward0>)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
