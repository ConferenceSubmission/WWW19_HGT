{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "from model import *\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/datadrive/data_cs/'\n",
    "batch_size = 256\n",
    "batch_num  = 128\n",
    "epoch_num  = 200\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "graph = dill.load(open(data_dir + 'graph.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = {t: True for t in graph.times if t != None and t < 2015}\n",
    "valid_range = {t: True for t in graph.times if t != None and t >= 2015  and t <= 2016}\n",
    "test_range  = {t: True for t in graph.times if t != None and t > 2016}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(graph.node_feature['paper']['time'])\n",
    "# tot = 0\n",
    "# for t in train_range:\n",
    "#     tot += np.sum(graph.node_feature['paper']['time'] == t)\n",
    "# print(tot)\n",
    "# tot = 0\n",
    "# for t in valid_range:\n",
    "#     tot += np.sum(graph.node_feature['paper']['time'] == t)\n",
    "# print(tot)\n",
    "# tot = 0\n",
    "# for t in test_range:\n",
    "#     tot += np.sum(graph.node_feature['paper']['time'] == t)\n",
    "# print(tot)\n",
    "# l = [377082, 77467, 89695]\n",
    "# l = np.array(l)\n",
    "# l / np.sum(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_dim, n_hid, num_types, num_relations, n_heads, n_layers, dropout = 0.3):\n",
    "        super(GNN, self).__init__()\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.num_types = num_types\n",
    "        self.in_dim    = in_dim\n",
    "        self.n_hid     = n_hid\n",
    "        self.aggregat_ws   = nn.ModuleList()\n",
    "        self.drop          = nn.Dropout(dropout)\n",
    "        for t in range(num_types):\n",
    "            self.aggregat_ws.append(nn.Linear(in_dim, n_hid))\n",
    "        for l in range(n_layers):\n",
    "            self.gcs.append(RAGCNConv(n_hid, n_hid, num_types, num_relations, n_heads, dropout))\n",
    "    def set_device(self, device):\n",
    "        self.device = device\n",
    "        for gc in self.gcs:\n",
    "            gc.device = device\n",
    "    def forward(self, node_feature, node_type, edge_time, edge_index, edge_type):\n",
    "        res = torch.zeros(node_feature.size(0), self.n_hid).to(node_feature.device)\n",
    "        for t_id in range(self.num_types):\n",
    "            aggregat_w = self.aggregat_ws[t_id]\n",
    "            idx = (node_type == t_id)\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            res[idx] = torch.tanh(aggregat_w(node_feature[idx]))\n",
    "        meta_xs = self.drop(res)\n",
    "        del res\n",
    "        for gc in self.gcs:\n",
    "            meta_xs = gc(meta_xs, node_type, edge_index, edge_type, edge_time)\n",
    "        return meta_xs\n",
    "    \n",
    "class CPC_Predictor(nn.Module):\n",
    "    def __init__(self, in_dim, n_hid, num_types, num_relations, n_heads, n_layers, dropout = 0.3):\n",
    "        super(CPC_Predictor, self).__init__()\n",
    "        self.gnn = GNN(in_dim, n_hid, num_types, num_relations, n_heads, n_layers, dropout)\n",
    "        self.matcher = nn.Linear(n_hid, n_heads)\n",
    "        self.score   = nn.Linear(n_heads, 1)\n",
    "        \n",
    "    def forward(self, pos_paper_ids, neg_paper_ids, node_feature, node_type, edge_time, edge_index, edge_type):\n",
    "        meta_xs = self.gnn(node_feature, node_type, edge_time, edge_index, edge_type)\n",
    "        meta_xs = self.score(F.elu(self.matcher(meta_xs)))\n",
    "        pos_res = meta_xs[pos_paper_ids]\n",
    "        neg_res = meta_xs[neg_paper_ids]\n",
    "        neg_res = neg_res.view(pos_res.size(0), neg_res.size(0) // pos_res.size(0))\n",
    "        return torch.cat([pos_res, neg_res], dim=-1)\n",
    "\n",
    "def gen_negative_sample(dim1, dim2, num):\n",
    "    res = []\n",
    "    for i in range(num):\n",
    "        res += [[np.random.randint(dim1), np.random.randint(dim2)]]\n",
    "    return res\n",
    "\n",
    "def neg_sample(size, pos_id, num):\n",
    "    res = {}\n",
    "    while len(res) != num:\n",
    "        s = np.random.choice(size)\n",
    "        if s in res or s == pos_id:\n",
    "            continue\n",
    "        res[s] = True\n",
    "    return list(res.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch(feature, time, edge_list, graph):\n",
    "    '''\n",
    "        Transform a sampled sub-graph into pytorch Tensor\n",
    "        node_dict: {node_type: <node_number, node_type_ID>} node_number is used to trace back the nodes in original graph.\n",
    "        edge_dict: {edge_type: edge_type_ID}\n",
    "    '''\n",
    "    node_dict = {}\n",
    "    node_feature = []\n",
    "    node_type    = []\n",
    "    node_time    = []\n",
    "    edge_index   = []\n",
    "    edge_type    = []\n",
    "    edge_time    = []\n",
    "    \n",
    "    node_num = 0\n",
    "    for t in graph.get_types():\n",
    "        type_id = len(node_dict)\n",
    "        if t == 'fake_paper':\n",
    "            type_id = -1\n",
    "        node_dict[t] = [node_num, type_id]\n",
    "        node_num     += len(feature[t])\n",
    "    if 'fake_paper' in node_dict:\n",
    "        node_dict['fake_paper'][1] = node_dict['paper'][1]\n",
    "    \n",
    "    for t in graph.get_types():\n",
    "        node_feature += list(feature[t])\n",
    "        node_time    += list(time[t])\n",
    "        type_id = node_dict[t][1]\n",
    "        node_type    += [type_id for _ in range(len(feature[t]))]\n",
    "        \n",
    "    edge_dict = {e[2]: i for i, e in enumerate(graph.get_meta_graph())}\n",
    "    edge_dict['self'] = len(edge_dict)\n",
    "    for target_type in edge_list:\n",
    "        for source_type in edge_list[target_type]:\n",
    "            for relation_type in edge_list[target_type][source_type]:\n",
    "                for ti, si in edge_list[target_type][source_type][relation_type]:\n",
    "                    sid, tid = si + node_dict[source_type][0], ti + node_dict[target_type][0]\n",
    "                    edge_index += [[sid, tid]]\n",
    "                    edge_type  += [edge_dict[relation_type]]   \n",
    "                    '''\n",
    "                        Our time ranges from 1900 - 2020, largest span is 120.\n",
    "                    '''\n",
    "                    edge_time  += [node_time[tid] - node_time[sid] + 120]\n",
    "    node_feature = torch.FloatTensor(node_feature)\n",
    "    node_type    = torch.LongTensor(node_type)\n",
    "    edge_time    = torch.LongTensor(edge_time)\n",
    "    edge_index   = torch.LongTensor(edge_index).t()\n",
    "    edge_type    = torch.LongTensor(edge_type)\n",
    "    return node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpc_sample(seed, in_feature, in_times, in_edge_list, batch_size, neg_size, ratio):\n",
    "    np.random.seed(seed)\n",
    "    feature, times, edge_list = copy.deepcopy(in_feature), copy.deepcopy(in_times), copy.deepcopy(in_edge_list)\n",
    "    paper_ids = np.random.choice(len(times['paper']), batch_size, replace = False)\n",
    "    shuff_ids = np.random.choice(len(times['paper']), batch_size * neg_size, replace = False)\n",
    "    paper_dict = {p: i for i, p in enumerate(paper_ids)}\n",
    "    feature['fake_paper'] = np.array(feature['paper'])[paper_ids].repeat(neg_size, 0)\n",
    "    times['fake_paper']   = np.array(times['paper'])[paper_ids].repeat(neg_size, 0)\n",
    "    te = edge_list['paper']\n",
    "    for source_type in te:\n",
    "        tes = te[source_type]\n",
    "        for relation_type in tes:\n",
    "            tesr = tes[relation_type]\n",
    "            tesd = defaultdict(lambda: [])\n",
    "            for target_ser, source_ser in tesr:\n",
    "                tesd[target_ser] += [source_ser]\n",
    "            for target_ser in paper_dict:\n",
    "                for j in range(neg_size):\n",
    "                    fake_ser = paper_dict[target_ser] * neg_size + j\n",
    "                    shuf_ser = shuff_ids[fake_ser]\n",
    "                    for source_ser in tesd[target_ser]:\n",
    "                        if relation_type == 'self':\n",
    "                            edge_list['fake_paper']['fake_paper']['self'] += [[fake_ser, fake_ser]]\n",
    "                        elif np.random.random() >= ratio or len(tesd[shuf_ser]) == 0:\n",
    "                            edge_list['fake_paper'][source_type][relation_type] += [[fake_ser, source_ser]]\n",
    "                        else:\n",
    "                            if len(tesd[shuf_ser]) > 0:\n",
    "                                rd_ser = np.random.choice(tesd[shuf_ser])\n",
    "                                edge_list['fake_paper'][source_type][relation_type] += [[fake_ser, rd_ser]]\n",
    "    return feature, times, edge_list, paper_ids\n",
    "\n",
    "def cpc_loss(data, paper_ids, num_fake, model, device):\n",
    "    node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict  = data\n",
    "    pos_paper_ids = paper_ids + node_dict['paper'][0]\n",
    "    neg_paper_ids = np.arange(num_fake) + node_dict['fake_paper'][0]\n",
    "    pred = model.forward(pos_paper_ids, neg_paper_ids, node_feature.to(device), node_type.to(device), \\\n",
    "                         edge_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "    return -torch.log_softmax(pred, dim=-1)[:, 0], pred\n",
    "\n",
    "def random_sample(seed, t_range, ratio, sampled_depth = 4, sampled_number = 100, neg_num = 3, batch_size = 128):\n",
    "    np.random.seed(seed)\n",
    "    feature, _time, edge_list, _ = sample_subgraph(graph, t_range, inp = None, \\\n",
    "                        sampled_depth = sampled_depth, sampled_number = sampled_number)  \n",
    "    fake_feature, fake_time, fake_edge_list, paper_ids = cpc_sample(seed, feature, _time, edge_list, batch_size, neg_num, ratio)\n",
    "    return to_torch(fake_feature, fake_time, fake_edge_list, graph), paper_ids, len(fake_time['fake_paper'])\n",
    "\n",
    "def prepare_data(pool, process_ids):\n",
    "    jobs = []\n",
    "    for process_id in process_ids[:-1]:\n",
    "        p = pool.apply_async(random_sample, args=(np.random.randint(2**32 - 1), train_range, np.random.random() * 0.4 + 0.1))\n",
    "        jobs.append(p)\n",
    "    p = pool.apply_async(random_sample, args=(np.random.randint(2**32 - 1), valid_range, 0.3))\n",
    "    jobs.append(p)\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpc_predictor = CPC_Predictor(in_dim = len(graph.node_feature['paper']['emb'][0]) + 401, n_hid = 256, num_types = len(graph.get_types()), \\\n",
    "                              num_relations = len(graph.get_meta_graph()) + 1, n_heads = 8, n_layers = 3).to(device)\n",
    "optimizer = torch.optim.AdamW(cpc_predictor.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 1000, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"<ipython-input-50-e2bf23d996af>\", line 45, in random_sample\n    return to_torch(fake_feature, fake_time, fake_edge_list, graph), paper_ids, len(fake_time['fake_paper'])\n  File \"<ipython-input-52-28cdb8ce1f71>\", line 41, in to_torch\n    edge_time  += [node_time[tid] - node_time[sid] + 120]\nIndexError: list index out of range\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-bc4f32caa483>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbest_val\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-bc4f32caa483>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbest_val\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "pool = mp.Pool(8)\n",
    "process_ids = np.arange(batch_num // 8)\n",
    "st = time.time()\n",
    "jobs = prepare_data(pool, process_ids)\n",
    "train_step = 1500\n",
    "best_val   = 0\n",
    "for epoch in np.arange(epoch_num)+1:\n",
    "    train_data = [job.get() for job in jobs[:-1]]\n",
    "    valid_data = jobs[-1].get()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    pool = mp.Pool(8)\n",
    "    jobs = prepare_data(pool, process_ids)\n",
    "    et = time.time()\n",
    "    print('Data Preparation: %.1fs' % (et - st))\n",
    "    \n",
    "    train_losses = []\n",
    "    cpc_predictor.train()\n",
    "    torch.cuda.empty_cache()\n",
    "    for data, paper_ids, num_fake in train_data:\n",
    "        loss, pred = cpc_loss(data, paper_ids, num_fake, cpc_predictor, device)\n",
    "        optimizer.zero_grad() \n",
    "        loss.mean().backward()\n",
    "        torch.nn.utils.clip_grad_norm_(cpc_predictor.parameters(), 0.2)\n",
    "        optimizer.step()\n",
    "        train_losses += loss.cpu().detach().tolist()\n",
    "        train_step += 1\n",
    "        scheduler.step(train_step)\n",
    "        del loss, pred\n",
    "        torch.cuda.empty_cache()\n",
    "    '''\n",
    "        Valid\n",
    "    '''\n",
    "    cpc_predictor.eval()\n",
    "    with torch.no_grad():\n",
    "        data, paper_ids, num_fake = valid_data\n",
    "        loss, pred = cpc_loss(data, paper_ids, num_fake, cpc_predictor, device)\n",
    "        valid_losses = loss.cpu().detach().tolist()\n",
    "        s = (pred.argmax(dim=1) == 0).sum()\n",
    "        valid_acc = s.tolist()/batch_size\n",
    "        st = time.time()\n",
    "        print((\"Epoch: %d (%.1fs)  LR: %.5f Train Loss: %f  Valid Loss: %f  Valid Acc: %f\") % \\\n",
    "              (epoch, (st-et), optimizer.param_groups[0]['lr'], np.average(train_losses), np.average(valid_losses), valid_acc))\n",
    "        if valid_acc > best_val:\n",
    "            best_val = valid_acc\n",
    "            torch.save(cpc_predictor.gnn, './save/cpc_model.pt')\n",
    "        stats += [[train_losses, valid_losses]]\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            feature, _time, edge_list = sample_subgraph(graph, test_range, inp = None, sampled_depth = 4, sampled_number = 128)\n",
    "            losses = []\n",
    "            accs   = []\n",
    "            for ratio in np.arange(10) / 10:\n",
    "                fake_feature, fake_time, fake_edge_list, paper_ids = cpc_sample(np.random.randint(2**32 - 1), \\\n",
    "                                                                feature, _time, edge_list, batch_size, 5, ratio)\n",
    "                data = to_torch(fake_feature, fake_time, fake_edge_list, graph)\n",
    "                loss, pred = cpc_loss(data, paper_ids, len(fake_time['fake_paper']), cpc_predictor, device)\n",
    "                for li in loss.tolist():\n",
    "                    losses += [[li, ratio]]\n",
    "                s = (pred.argmax(dim=1) == 0).sum()\n",
    "                acc = s.tolist()/batch_size\n",
    "                accs += [acc]\n",
    "                del loss, pred\n",
    "                torch.cuda.empty_cache()\n",
    "            sb.lineplot(data = pd.DataFrame(losses, columns=['loss', 'ratio']), x = 'ratio', y='loss')\n",
    "            plt.show()\n",
    "            plt.plot(accs)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch(feature, time, edge_list, graph):\n",
    "    '''\n",
    "        Transform a sampled sub-graph into pytorch Tensor\n",
    "        node_dict: {node_type: <node_number, node_type_ID>} node_number is used to trace back the nodes in original graph.\n",
    "        edge_dict: {edge_type: edge_type_ID}\n",
    "    '''\n",
    "    node_dict = {}\n",
    "    node_feature = []\n",
    "    node_type    = []\n",
    "    node_time    = []\n",
    "    edge_index   = []\n",
    "    edge_type    = []\n",
    "    edge_time    = []\n",
    "    \n",
    "    node_num = 0\n",
    "    for t in graph.get_types():\n",
    "        type_id = len(node_dict)\n",
    "        node_dict[t] = [node_num, type_id]\n",
    "        node_num     += len(feature[t])\n",
    "    if 'fake_paper' in feature:\n",
    "        node_dict['fake_paper'] = [node_num, node_dict['paper'][1]]\n",
    "        node_num     += len(feature['fake_paper'])\n",
    "    for t in node_dict:\n",
    "        node_feature += list(feature[t])\n",
    "        node_time    += list(time[t])\n",
    "        type_id = node_dict[t][1]\n",
    "        node_type    += [type_id for _ in range(len(feature[t]))]\n",
    "        \n",
    "    edge_dict = {e[2]: i for i, e in enumerate(graph.get_meta_graph())}\n",
    "    edge_dict['self'] = len(edge_dict)\n",
    "    for target_type in edge_list:\n",
    "        for source_type in edge_list[target_type]:\n",
    "            for relation_type in edge_list[target_type][source_type]:\n",
    "                for ti, si in edge_list[target_type][source_type][relation_type]:\n",
    "                    sid, tid = si + node_dict[source_type][0], ti + node_dict[target_type][0]\n",
    "                    edge_index += [[sid, tid]]\n",
    "                    edge_type  += [edge_dict[relation_type]]   \n",
    "                    '''\n",
    "                        Our time ranges from 1900 - 2020, largest span is 120.\n",
    "                    '''\n",
    "                    edge_time  += [node_time[tid] - node_time[sid] + 120]\n",
    "    node_feature = torch.FloatTensor(node_feature)\n",
    "    node_type    = torch.LongTensor(node_type)\n",
    "    edge_time    = torch.LongTensor(edge_time)\n",
    "    edge_index   = torch.LongTensor(edge_index).t()\n",
    "    edge_type    = torch.LongTensor(edge_type)\n",
    "    return node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for epoch, (train_losses, valid_losses) in enumerate(stats):\n",
    "    for ti in train_losses:\n",
    "        losses += [[ti, 'Train', epoch]]\n",
    "#     for ti in valid_losses:\n",
    "#         losses += [[ti, 'Valid', epoch]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.lineplot(data = pd.DataFrame(losses, columns=['loss', 'Type', 'Epoch']), x = 'Epoch', y='loss', hue='Type')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
