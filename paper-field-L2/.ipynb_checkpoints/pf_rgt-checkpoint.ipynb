{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/datadrive/data/'\n",
    "batch_size = 256\n",
    "batch_num  = 128\n",
    "epoch_num  = 1000\n",
    "samp_num   = 7\n",
    "\n",
    "device = torch.device(\"cuda:2\")\n",
    "graph = dill.load(open(data_dir + 'graph.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = {t: True for t in graph.times if t != None and t <= 2015}\n",
    "valid_range = {t: True for t in graph.times if t != None and t > 2015  and t < 2018}\n",
    "test_range  = {t: True for t in graph.times if t != None and t >= 2018}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pf_sample(seed, papers, pairs, t_range, batch_size, test = False):\n",
    "    np.random.seed(seed)\n",
    "    _time = np.random.choice(list(papers.keys()))\n",
    "    pids = np.array(papers[_time])[np.random.choice(len(papers[_time]), batch_size, replace = False)]\n",
    "    fids = []\n",
    "    edge = defaultdict(lambda: {})\n",
    "    for x_id, p_id in enumerate(pids):\n",
    "        f_ids = pairs[p_id]\n",
    "        for f_id in f_ids:\n",
    "            if f_id not in fids:\n",
    "                fids += [f_id]\n",
    "            edge[x_id][fids.index(f_id)] = True\n",
    "    pids = np.stack([pids, np.repeat([_time], batch_size)]).T\n",
    "    fids = np.stack([fids, np.repeat([_time], len(fids))]).T\n",
    " \n",
    "    feature, times, edge_list = sample_subgraph(graph, t_range, \\\n",
    "                inp = {'paper': pids, 'field': fids}, sampled_depth = 4, sampled_number = 128)\n",
    "\n",
    "    el = []\n",
    "    for i in edge_list['paper']['field']['rev_PF_in']:\n",
    "        if i[0] in edge and i[1] in edge[i[0]]:\n",
    "            continue\n",
    "        el += [i]\n",
    "    edge_list['paper']['field']['rev_PF_in'] = el\n",
    "\n",
    "    el = []\n",
    "    for i in edge_list['field']['paper']['PF_in']:\n",
    "        if i[1] in edge and i[0] in edge[i[1]]:\n",
    "            continue\n",
    "        el += [i]\n",
    "    edge_list['field']['paper']['PF_in'] = el\n",
    "    \n",
    "    \n",
    "    node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict = \\\n",
    "            to_torch(feature, times, edge_list)\n",
    "    '''\n",
    "        Trace the paper_id and field_id by its own index plus the type start index\n",
    "    '''\n",
    "    paper_ids = np.arange(len(pids)) + node_dict['paper'][0]\n",
    "    field_ids = np.arange(len(fids)) + node_dict['field'][0]\n",
    "    ylabel = torch.zeros(batch_size, len(ids))\n",
    "    for x_id, p_id in enumerate(pids[:,0]):\n",
    "        for f_id in pairs[p_id]:\n",
    "            ylabel[x_id][list(ids).index(f_id)] = 1\n",
    "    ylabel /= ylabel.sum(axis=1).view(-1, 1)\n",
    "    return node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel\n",
    "    \n",
    "def prepare_data(pool, process_ids):\n",
    "    jobs = []\n",
    "    for process_id in process_ids[:-1]:\n",
    "        p = pool.apply_async(pf_sample, args=(np.random.randint(2**32 - 1), train_papers, \\\n",
    "                                               train_pairs, train_range, batch_size))\n",
    "        jobs.append(p)\n",
    "    p = pool.apply_async(pf_sample, args=(np.random.randint(2**32 - 1), valid_papers, \\\n",
    "                                           valid_pairs, valid_range, batch_size))\n",
    "    jobs.append(p)\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Paper-Field\n",
    "'''\n",
    "field_dict = dill.load(open(data_dir + 'field_dict.pk', 'rb'))\n",
    "ids = np.array([graph.node_forward['field'][k] for k in field_dict if field_dict[k][0] == 'L1'])\n",
    "\n",
    "paper_ser = {}\n",
    "\n",
    "train_pairs = {}\n",
    "valid_pairs = {}\n",
    "test_pairs  = {}\n",
    "\n",
    "train_papers = {_time: {} for _time in train_range}\n",
    "valid_papers = {_time: {} for _time in valid_range}\n",
    "test_papers  = {_time: {} for _time in test_range}\n",
    "\n",
    "for f_id in ids:\n",
    "    for p_id in graph.edge_list['field']['paper']['PF_in'][f_id]:\n",
    "        _time = graph.edge_list['field']['paper']['PF_in'][f_id][p_id]\n",
    "        if _time in train_range:\n",
    "            if p_id not in train_pairs:\n",
    "                train_pairs[p_id] = []\n",
    "            train_pairs[p_id] += [f_id]\n",
    "            train_papers[_time][p_id] = True\n",
    "        elif _time in valid_range:\n",
    "            if p_id not in valid_pairs:\n",
    "                valid_pairs[p_id] = []\n",
    "            valid_pairs[p_id] += [f_id]\n",
    "            valid_papers[_time][p_id] = True\n",
    "        else:\n",
    "            if p_id not in test_pairs:\n",
    "                test_pairs[p_id] = []\n",
    "            test_pairs[p_id] += [f_id]\n",
    "            test_papers[_time][p_id] = True\n",
    "for _time in list(train_papers.keys()):\n",
    "    if len(train_papers[_time]) < batch_size:\n",
    "        train_papers.pop(_time)\n",
    "    else:\n",
    "        train_papers[_time] = np.array(list(train_papers[_time].keys()))\n",
    "for _time in list(valid_papers.keys()):\n",
    "    if len(valid_papers[_time]) < batch_size:\n",
    "        valid_papers.pop(_time)\n",
    "    else:\n",
    "        valid_papers[_time] = np.array(list(valid_papers[_time].keys()))\n",
    "for _time in list(test_papers.keys()):\n",
    "    if len(test_papers[_time]) < batch_size:\n",
    "        test_papers.pop(_time)\n",
    "    else:\n",
    "        test_papers[_time] = np.array(list(test_papers[_time].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_hid, num_types, num_relations, n_heads, n_layers, device, dropout = 0.3):\n",
    "        super(GNN, self).__init__()\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.adapt = nn.Linear(n_hid, n_hid)\n",
    "        for l in range(n_layers):\n",
    "            self.gcs.append(RAGCNConv(n_hid, n_hid, num_types, num_relations, n_heads, device, dropout))\n",
    "    def set_device(self, device):\n",
    "        self.device = device\n",
    "        for gc in self.gcs:\n",
    "            gc.device = device\n",
    "    def forward(self, node_feature, node_type, edge_time, edge_index, edge_type):\n",
    "        meta_xs = F.elu(self.adapt(node_feature))\n",
    "        for gc in self.gcs:\n",
    "            meta_xs = gc(meta_xs, node_type, edge_index, edge_type, edge_time)\n",
    "        return meta_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = GNN(n_hid = 400, num_types = len(graph.get_types()), num_relations = len(graph.get_meta_graph()) + 1, \\\n",
    "                       n_heads = 4, n_layers = 2, device = device).to(device)\n",
    "classifier = Classifier(400, len(ids)).to(device)\n",
    "model = nn.Sequential(gnn, classifier)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 2000, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 93.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (79.0s)  LR: 0.00016 Train Loss: 3.92  Valid Loss: 3.22  Valid NDCG: 0.4139\n",
      "Data Preparation: 13.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 (79.1s)  LR: 0.00018 Train Loss: 3.25  Valid Loss: 2.83  Valid NDCG: 0.5549\n",
      "Data Preparation: 16.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 (76.7s)  LR: 0.00020 Train Loss: 2.82  Valid Loss: 2.18  Valid NDCG: 0.6965\n",
      "Data Preparation: 13.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 (78.0s)  LR: 0.00022 Train Loss: 2.30  Valid Loss: 1.86  Valid NDCG: 0.7459\n",
      "Data Preparation: 23.5s\n",
      "Epoch: 5 (80.6s)  LR: 0.00024 Train Loss: 2.07  Valid Loss: 1.94  Valid NDCG: 0.7410\n",
      "0.7394852623423421\n",
      "Data Preparation: 25.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 (80.8s)  LR: 0.00026 Train Loss: 1.86  Valid Loss: 1.43  Valid NDCG: 0.8151\n",
      "Data Preparation: 14.1s\n",
      "Epoch: 7 (77.3s)  LR: 0.00028 Train Loss: 1.65  Valid Loss: 1.59  Valid NDCG: 0.8066\n",
      "Data Preparation: 14.8s\n",
      "Epoch: 8 (75.9s)  LR: 0.00030 Train Loss: 1.54  Valid Loss: 1.71  Valid NDCG: 0.7790\n",
      "Data Preparation: 19.8s\n",
      "Epoch: 9 (77.4s)  LR: 0.00032 Train Loss: 1.45  Valid Loss: 1.53  Valid NDCG: 0.8054\n",
      "Data Preparation: 15.4s\n",
      "Epoch: 10 (77.3s)  LR: 0.00035 Train Loss: 1.43  Valid Loss: 1.69  Valid NDCG: 0.7782\n",
      "0.7591223493128415\n",
      "Data Preparation: 25.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 (80.2s)  LR: 0.00037 Train Loss: 1.30  Valid Loss: 1.44  Valid NDCG: 0.8229\n",
      "Data Preparation: 13.6s\n",
      "Epoch: 12 (78.6s)  LR: 0.00039 Train Loss: 1.32  Valid Loss: 1.82  Valid NDCG: 0.7597\n",
      "Data Preparation: 16.9s\n",
      "Epoch: 13 (76.8s)  LR: 0.00041 Train Loss: 1.29  Valid Loss: 1.45  Valid NDCG: 0.8102\n",
      "Data Preparation: 18.2s\n",
      "Epoch: 14 (77.2s)  LR: 0.00044 Train Loss: 1.32  Valid Loss: 1.57  Valid NDCG: 0.8030\n",
      "Data Preparation: 16.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 (80.3s)  LR: 0.00046 Train Loss: 1.18  Valid Loss: 1.26  Valid NDCG: 0.8523\n",
      "0.7931516591965083\n",
      "Data Preparation: 25.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 (76.3s)  LR: 0.00048 Train Loss: 1.17  Valid Loss: 1.11  Valid NDCG: 0.8908\n",
      "Data Preparation: 20.2s\n",
      "Epoch: 17 (77.5s)  LR: 0.00051 Train Loss: 1.17  Valid Loss: 1.37  Valid NDCG: 0.8291\n",
      "Data Preparation: 15.7s\n",
      "Epoch: 18 (90.8s)  LR: 0.00053 Train Loss: 1.10  Valid Loss: 1.32  Valid NDCG: 0.8419\n",
      "Data Preparation: 16.9s\n",
      "Epoch: 19 (80.6s)  LR: 0.00055 Train Loss: 1.13  Valid Loss: 1.26  Valid NDCG: 0.8509\n",
      "Data Preparation: 12.2s\n",
      "Epoch: 20 (77.5s)  LR: 0.00058 Train Loss: 1.11  Valid Loss: 1.92  Valid NDCG: 0.7274\n",
      "0.7939282005421668\n",
      "Data Preparation: 26.0s\n",
      "Epoch: 21 (78.9s)  LR: 0.00060 Train Loss: 1.03  Valid Loss: 1.49  Valid NDCG: 0.8038\n",
      "Data Preparation: 18.7s\n",
      "Epoch: 22 (93.0s)  LR: 0.00062 Train Loss: 1.13  Valid Loss: 1.29  Valid NDCG: 0.8424\n",
      "Data Preparation: 14.7s\n",
      "Epoch: 23 (92.8s)  LR: 0.00065 Train Loss: 1.04  Valid Loss: 1.25  Valid NDCG: 0.8426\n",
      "Data Preparation: 18.6s\n",
      "Epoch: 24 (94.2s)  LR: 0.00067 Train Loss: 1.09  Valid Loss: 1.10  Valid NDCG: 0.8788\n",
      "Data Preparation: 13.3s\n",
      "Epoch: 25 (90.0s)  LR: 0.00069 Train Loss: 1.01  Valid Loss: 1.44  Valid NDCG: 0.8208\n",
      "0.8372872076177761\n",
      "Data Preparation: 29.4s\n",
      "Epoch: 26 (94.5s)  LR: 0.00071 Train Loss: 1.10  Valid Loss: 1.58  Valid NDCG: 0.8068\n",
      "Data Preparation: 16.7s\n",
      "Epoch: 27 (94.4s)  LR: 0.00073 Train Loss: 0.90  Valid Loss: 1.36  Valid NDCG: 0.8396\n",
      "Data Preparation: 17.7s\n",
      "Epoch: 28 (82.3s)  LR: 0.00075 Train Loss: 1.02  Valid Loss: 1.15  Valid NDCG: 0.8500\n",
      "Data Preparation: 19.3s\n",
      "Epoch: 29 (78.5s)  LR: 0.00077 Train Loss: 0.93  Valid Loss: 1.12  Valid NDCG: 0.8693\n",
      "Data Preparation: 16.1s\n",
      "Epoch: 30 (80.3s)  LR: 0.00079 Train Loss: 0.92  Valid Loss: 1.29  Valid NDCG: 0.8487\n",
      "0.8219941322914506\n",
      "Data Preparation: 27.5s\n",
      "Epoch: 31 (83.9s)  LR: 0.00081 Train Loss: 0.93  Valid Loss: 1.42  Valid NDCG: 0.8272\n",
      "Data Preparation: 22.7s\n",
      "Epoch: 32 (95.3s)  LR: 0.00083 Train Loss: 1.00  Valid Loss: 1.37  Valid NDCG: 0.8471\n",
      "Data Preparation: 14.1s\n",
      "Epoch: 33 (93.3s)  LR: 0.00085 Train Loss: 1.01  Valid Loss: 1.12  Valid NDCG: 0.8673\n",
      "Data Preparation: 20.9s\n",
      "Epoch: 34 (93.9s)  LR: 0.00086 Train Loss: 0.98  Valid Loss: 1.51  Valid NDCG: 0.8103\n",
      "Data Preparation: 16.4s\n",
      "Epoch: 35 (90.9s)  LR: 0.00088 Train Loss: 1.01  Valid Loss: 1.16  Valid NDCG: 0.8651\n",
      "0.7579801321725039\n",
      "Data Preparation: 31.5s\n",
      "Epoch: 36 (89.4s)  LR: 0.00090 Train Loss: 1.00  Valid Loss: 1.15  Valid NDCG: 0.8802\n",
      "Data Preparation: 19.9s\n",
      "Epoch: 37 (93.2s)  LR: 0.00091 Train Loss: 0.91  Valid Loss: 1.36  Valid NDCG: 0.8569\n",
      "Data Preparation: 24.5s\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "pool = mp.Pool(8)\n",
    "process_ids = np.arange(batch_num // 8)\n",
    "st = time.time()\n",
    "jobs = prepare_data(pool, process_ids)\n",
    "train_step = 3000\n",
    "best_val   = 0\n",
    "res = []\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "for epoch in np.arange(epoch_num)+1:\n",
    "    '''\n",
    "        Prepare Training and Validation Data\n",
    "    '''\n",
    "    train_data = [job.get() for job in jobs[:-1]]\n",
    "    valid_data = jobs[-1].get()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    pool = mp.Pool(8)\n",
    "    jobs = prepare_data(pool, process_ids)\n",
    "    et = time.time()\n",
    "    print('Data Preparation: %.1fs' % (et - st))\n",
    "    \n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    torch.cuda.empty_cache()\n",
    "    for batch in np.arange(2):\n",
    "        for node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel in train_data:\n",
    "            node_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                                   edge_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "            res  = classifier.forward(node_rep[paper_ids])\n",
    "            loss = criterion(res, ylabel.to(device))\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "            train_losses += [loss.cpu().detach().tolist()]\n",
    "            train_step += 1\n",
    "            scheduler.step(train_step)\n",
    "    '''\n",
    "        Valid\n",
    "    '''\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel = valid_data\n",
    "        node_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                                   edge_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "        res  = classifier.forward(node_rep[paper_ids])\n",
    "        loss = criterion(res, ylabel.to(device))\n",
    "        valid_res = []\n",
    "\n",
    "        for ai, bi in zip(ylabel, res.argsort(descending = True)):\n",
    "            valid_res += [ai[bi].tolist()]\n",
    "        valid_ndcg = np.average([ndcg_at_k(resi, len(resi)) for resi in valid_res])\n",
    "        if valid_ndcg > best_val:\n",
    "            best_val = valid_ndcg\n",
    "            torch.save(model, './save/rgt.pt')\n",
    "        st = time.time()\n",
    "        print((\"Epoch: %d (%.1fs)  LR: %.5f Train Loss: %.2f  Valid Loss: %.2f  Valid NDCG: %.4f\") % \\\n",
    "              (epoch, (st-et), optimizer.param_groups[0]['lr'], np.average(train_losses), loss.cpu().detach().tolist(),\\\n",
    "              valid_ndcg))\n",
    "        stats += [[np.average(train_losses), loss.cpu().detach().tolist()]]\n",
    "        if epoch % 5 == 0:\n",
    "            '''\n",
    "                Test\n",
    "            '''\n",
    "            _time = np.random.choice(list(test_papers.keys()))\n",
    "            node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel = pf_sample(np.random.randint(2 ** 32 - 1), test_papers, \\\n",
    "                                                           test_pairs, test_range, batch_size, test=True)\n",
    "            paper_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                      edge_time.to(device), edge_index.to(device), edge_type.to(device))[paper_ids]\n",
    "            res  = classifier.forward(paper_rep)\n",
    "            test_res = []\n",
    "            for ai, bi in zip(ylabel, res.argsort(descending = True)):\n",
    "                test_res += [ai[bi].tolist()]\n",
    "            test_ndcg = np.average([ndcg_at_k(resi, len(resi)) for resi in test_res])\n",
    "            print(test_ndcg)\n",
    "    del train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = np.array(stats)\n",
    "plt.plot(stats[:,0])\n",
    "plt.plot(stats[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('./save/rgt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "gnn, classifier = best_model\n",
    "with torch.no_grad():\n",
    "    test_res = []\n",
    "    for _ in range(10):\n",
    "        _time = np.random.choice(list(test_papers.keys()))\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel = pf_sample(np.random.randint(2 ** 32 - 1), test_papers, \\\n",
    "                                                       test_pairs, test_range, batch_size, test=True)\n",
    "        paper_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                      edge_time.to(device), edge_index.to(device), edge_type.to(device))[paper_ids]\n",
    "        res = classifier.forward(paper_rep)\n",
    "        for ai, bi in zip(ylabel, res.argsort(descending = True)):\n",
    "            test_res += [ai[bi].tolist()]\n",
    "    test_ndcg = [ndcg_at_k(resi, len(resi)) for resi in test_res]\n",
    "    print(np.average(test_ndcg), np.var(test_ndcg))\n",
    "    test_mrr = mean_reciprocal_rank(test_res)\n",
    "    print(np.average(test_mrr), np.var(test_mrr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
