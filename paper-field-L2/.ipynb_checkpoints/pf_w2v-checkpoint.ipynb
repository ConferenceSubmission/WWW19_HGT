{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/datadrive/data/'\n",
    "graph = dill.load(open(data_dir + 'graph.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = {t: True for t in graph.times if t != None and t <= 2015}\n",
    "valid_range = {t: True for t in graph.times if t != None and (t > 2015) & (t < 2018)}\n",
    "test_range  = {t: True for t in graph.times if t != None and t >= 2018}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Matcher(nn.Module):\n",
    "    def __init__(self, n_hid, dropout = 0.5):\n",
    "        super(Matcher, self).__init__()\n",
    "        self.left_linear    = nn.Linear(n_hid,  n_hid)\n",
    "        self.right_linear   = nn.Linear(n_hid,  n_hid)\n",
    "        self.drop     = nn.Dropout(dropout)\n",
    "        self.mem      = None\n",
    "    def forward(self, x, y, test = False):\n",
    "        if test:\n",
    "            if self.mem != None:\n",
    "                tx = self.mem\n",
    "            else:\n",
    "                tx = self.left_linear(x)\n",
    "                self.mem = tx\n",
    "        else:\n",
    "            tx = self.drop(self.left_linear(x))\n",
    "        ty = self.drop(self.right_linear(y))\n",
    "        return torch.log_softmax(torch.mm(ty, tx.T), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "    return 0.\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / dcg_max\n",
    "\n",
    "def mean_reciprocal_rank(rs):\n",
    "    rs = (np.asarray(r).nonzero()[0] for r in rs)\n",
    "    return np.mean([1. / (r[0] + 1) if r.size else 0. for r in rs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625 0.9473224867337506\n"
     ]
    }
   ],
   "source": [
    "r = [1,1,1, 0, 0, 1, 1, 0]\n",
    "print(mean_reciprocal_rank(r), ndcg_at_k(r, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "batch_num  = 128\n",
    "epoch_num  = 100\n",
    "samp_num   = 7\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "train_feature, train_time, train_edge_list = \\\n",
    "        graph.sample_subgraph(time_range=train_range, sampled_depth = 4, sampled_number = 64)\n",
    "node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict  = \\\n",
    "        to_torch(train_feature, train_time, train_edge_list, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_subgraph(graph, time_range, sampled_depth = 2, sampled_number = 8, inp = False):\n",
    "    '''\n",
    "    Sample Sub-Graph based on a subset of paper (events) and their happening time\n",
    "    '''\n",
    "    def add_budget(te, target_id, target_time, layer_data, budget, maxnum = sampled_depth * sampled_number):\n",
    "        for source_type in te:\n",
    "            tes = te[source_type]\n",
    "            for relation_type in tes:\n",
    "                if relation_type == 'self':\n",
    "                    continue\n",
    "                adl = tes[relation_type][target_id]\n",
    "                if len(adl) < maxnum:\n",
    "                    sampled_ids = list(adl.keys())\n",
    "                else:\n",
    "                    sampled_ids = np.random.choice(list(adl.keys()), maxnum, replace = False)\n",
    "                for source_id in sampled_ids:\n",
    "                    source_time = adl[source_id]\n",
    "                    if source_time == None:\n",
    "                        source_time = target_time\n",
    "                    k = encode(source_id, source_time)\n",
    "                    if source_time not in time_range or k in layer_data[source_type]:\n",
    "                        continue\n",
    "                    budget[source_type][k] += 1. / len(sampled_ids)\n",
    "    def decode(s):\n",
    "        idx = s.find('-')\n",
    "        return np.array([s[:idx], s[idx+1:]], dtype=float)\n",
    "    def encode(i, t):\n",
    "        return '%s-%s' % (i, t)\n",
    "\n",
    "    layer_data  = defaultdict( #target_type\n",
    "                        lambda: {} # {target_id + time}\n",
    "                    )\n",
    "    budget     = defaultdict( #source_type\n",
    "                                lambda: defaultdict(  #source_id + source_time\n",
    "                                    lambda: 0. #sampled_score\n",
    "                            ))\n",
    "    new_layer_adj  = defaultdict( #target_type\n",
    "                                    lambda: defaultdict(  #source_type\n",
    "                                        lambda: defaultdict(  #relation_type\n",
    "                                            lambda: [] #[target_id, source_id]\n",
    "                                )))\n",
    "    \n",
    "    if inp == None:\n",
    "        rand_paper_ids  = np.random.choice(range(len(graph.node_feature['paper'])), sampled_number * 2, replace = False)\n",
    "        rand_paper_time = np.array(list(graph.node_feature['paper'].loc[rand_paper_ids, 'time']))\n",
    "        for _id, _time in zip(rand_paper_ids, rand_paper_time):\n",
    "            if _time not in time_range:\n",
    "                continue\n",
    "            layer_data['paper'][encode(_id, _time)] = len(layer_data['paper'])\n",
    "            add_budget(graph.edge_list['paper'], _id, _time, layer_data, budget)\n",
    "    else:\n",
    "        '''\n",
    "        budget: {_type: [[_id, _time]]}\n",
    "        '''\n",
    "        for _type in inp:\n",
    "            for _id, _time in inp[_type]:\n",
    "                layer_data[_type][encode(_id, _time)] = len(layer_data[_type])\n",
    "                add_budget(graph.edge_list[_type], _id, _time, layer_data, budget)\n",
    "            \n",
    "    for layer in range(sampled_depth):\n",
    "        for source_type in graph.get_types():\n",
    "            keys  = np.array(list(budget[source_type].keys()))\n",
    "            if sampled_number > len(keys):\n",
    "                sampled_ids = np.arange(len(keys))\n",
    "            else:\n",
    "                score = np.array(list(budget[source_type].values()))\n",
    "                score = score / np.sum(score)\n",
    "                sampled_ids = np.random.choice(len(score), sampled_number, p = score, replace = False) \n",
    "            sampled_ids = keys[sampled_ids]\n",
    "            for k in sampled_ids:\n",
    "                source_id, source_time = decode(k)\n",
    "                layer_data[source_type][k] = len(layer_data[source_type])\n",
    "                add_budget(graph.edge_list[source_type], int(source_id), int(source_time), layer_data, budget)\n",
    "            for k in sampled_ids:\n",
    "                budget[source_type].pop(k)\n",
    "                \n",
    "    print('Finish Propagate Nodes')            \n",
    "    \n",
    "    feature = {}\n",
    "    times   = {}\n",
    "    for target_type in layer_data:\n",
    "        idxs  = np.array([decode(key) for key in layer_data[target_type]])\n",
    "        feature[target_type] = list(graph.node_feature[target_type].loc[idxs[:,0], 'w2v'])\n",
    "        times[target_type]   = idxs[:,1]\n",
    "    edge_list = defaultdict( #target_type\n",
    "                        lambda: defaultdict(  #source_type\n",
    "                            lambda: defaultdict(  #relation_type\n",
    "                                lambda: [] # [target_id, source_id] \n",
    "                                    )))\n",
    "    for target_type in layer_data:\n",
    "        for target_id in layer_data[target_type]:\n",
    "            target_ser = layer_data[target_type][target_id]\n",
    "            edge_list[target_type][target_type]['self'] += [[target_ser, target_ser]]\n",
    "    for target_type in graph.edge_list:\n",
    "        te = graph.edge_list[target_type]\n",
    "        for source_type in te:\n",
    "            tes = te[source_type]\n",
    "            for relation_type in tes:\n",
    "                tesr = tes[relation_type]\n",
    "                for target_id in layer_data[target_type]:\n",
    "                    target_ser = layer_data[target_type][target_id]\n",
    "                    for source_id in layer_data[source_type]:\n",
    "                        source_ser = layer_data[source_type][source_id]\n",
    "                        if decode(source_id)[0] in tesr[decode(target_id)[0]]:\n",
    "                            edge_list[target_type][source_type][relation_type] += [[target_ser, source_ser]]\n",
    "    return feature, times, edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Propagate Nodes\n"
     ]
    }
   ],
   "source": [
    "inp = {'paper': np.array([np.arange(10), np.ones(10) * 2018]).T}\n",
    "feature, times, edge_list = sample_subgraph(graph, train_range, inp = inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper': array([2018., 2018., 2018., 2018., 2018., 2018., 2018., 2018., 2018.,\n",
       "        2018., 2014., 2014., 2014., 2011., 2014., 2015., 2013., 2015.,\n",
       "        1991., 1987., 2005., 2015., 1983., 2011., 2007., 2013.]),\n",
       " 'venue': array([1993., 2014., 2014., 2004., 1991., 2013., 2015., 2011., 1965.,\n",
       "        2015., 2010., 1998., 2010., 2011., 2011., 2005.]),\n",
       " 'field': array([2010., 2015., 2004., 1993., 2014., 2010., 2004., 2011., 2004.,\n",
       "        1998., 2011., 2010., 1998., 2015., 2014., 2005.]),\n",
       " 'author': array([2011., 2013., 2015., 2015., 2004., 2013., 1993., 2015., 1993.,\n",
       "        2015., 2015., 2010., 2010., 2011., 2011., 2015.]),\n",
       " 'affiliation': array([2015., 1993., 2015., 2015., 2015., 2011., 2004., 2013., 2010.,\n",
       "        2013., 2011., 2010., 2015., 2013., 2015., 2015.])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-eb8426f91ca4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mtest_sers\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mfield_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_feature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'field'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w2v'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mpaper_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_feature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paper'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper_ser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w2v'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Paper-Field\n",
    "'''\n",
    "field_dict = dill.load(open(data_dir + 'field_dict.pk', 'rb'))\n",
    "ids = np.array([graph.node_forward['field'][k] for k in field_dict if field_dict[k][0] == 'L1'])\n",
    "\n",
    "paper_ser = {}\n",
    "\n",
    "train_pairs = defaultdict(lambda: [])\n",
    "valid_pairs = defaultdict(lambda: [])\n",
    "test_pairs  = defaultdict(lambda: [])\n",
    "\n",
    "train_sers = []\n",
    "valid_sers = []\n",
    "test_sers  = []\n",
    "\n",
    "for f_ser, f_id in enumerate(ids):\n",
    "    for p_id in graph.edge_list['field']['paper']['PF_in'][f_id]:\n",
    "        _time = graph.edge_list['field']['paper']['PF_in'][f_id][p_id]\n",
    "        if _time in train_range:\n",
    "            train_pairs[p_id] += [f_id]\n",
    "            train_sers  += [p_id]\n",
    "        elif _time in valid_range:\n",
    "            valid_pairs[p_id] += [f_id]\n",
    "            valid_sers  += [p_id]\n",
    "        else:\n",
    "            test_pairs[p_id] += [f_id]\n",
    "            test_sers  += [p_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(400).to(device)\n",
    "optimizer = torch.optim.Adam(matcher.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 2000, eta_min=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stats = []\n",
    "train_step = 0\n",
    "best_val   = 0\n",
    "res = []\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "for epoch in np.arange(epoch_num)+1:\n",
    "    '''\n",
    "        Train\n",
    "    '''\n",
    "    matcher.train()\n",
    "    train_losses = []\n",
    "    ylabel = torch.zeros(batch_size, len(field_vecs)).to(device)\n",
    "    for batch in np.arange(batch_num)+1:\n",
    "        train_xids = train_sers[np.random.choice(len(train_sers), batch_size)]\n",
    "        for yl, _id in zip(ylabel, train_xids):\n",
    "            yl[np.array(train_pairs[_id])] = 1\n",
    "        train_ylabel = ylabel / ylabel.sum(dim=-1).view(-1, 1)\n",
    "        train_pvecs = torch.FloatTensor(paper_vecs[train_xids]).to(device)\n",
    "        pred = matcher.forward(field_vecs, train_pvecs)\n",
    "        loss = criterion(pred, train_ylabel)\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses += [loss.cpu().detach().tolist()]\n",
    "        train_step += 1\n",
    "        scheduler.step(train_step)\n",
    "    '''\n",
    "        Valid\n",
    "    '''\n",
    "    matcher.eval()\n",
    "    valid_xids = valid_sers[np.random.choice(len(valid_sers), batch_size)]\n",
    "    ylabel = torch.zeros(batch_size, len(field_vecs)).to(device)\n",
    "    for yl, _id in zip(ylabel, valid_xids):\n",
    "        yl[np.array(valid_pairs[_id])] = 1\n",
    "    valid_ylabel = ylabel / ylabel.sum(dim=-1).view(-1, 1)\n",
    "    valid_pvecs = torch.FloatTensor(paper_vecs[valid_xids]).to(device)\n",
    "    pred = matcher.forward(field_vecs, valid_pvecs)\n",
    "    loss = criterion(pred, valid_ylabel)\n",
    "    valid_res = []\n",
    "    \n",
    "    for ai, bi in zip(ylabel, pred.argsort(descending = True)):\n",
    "        valid_res += [ai[bi].tolist()]\n",
    "        \n",
    "    '''\n",
    "        Test\n",
    "    '''\n",
    "    test_xids = test_sers[np.random.choice(len(test_sers), batch_size)]\n",
    "    ylabel = torch.zeros(batch_size, len(field_vecs)).to(device)\n",
    "    for yl, _id in zip(ylabel, valid_xids):\n",
    "        yl[np.array(valid_pairs[_id])] = 1\n",
    "    test_pvecs = torch.FloatTensor(paper_vecs[test_xids]).to(device)\n",
    "    pred = matcher.forward(field_vecs, test_pvecs)\n",
    "    test_res = []\n",
    "    \n",
    "    for ai, bi in zip(ylabel, pred.argsort(descending = True)):\n",
    "        test_res += [ai[bi].tolist()]\n",
    "    if np.average([ndcg_at_k(resi, 100) for resi in valid_res]) > best_val:\n",
    "        best_val = np.average([ndcg_at_k(resi, len(resi)) for resi in valid_res])\n",
    "        res = [np.average([ndcg_at_k(resi, len(resi)) for resi in test_res]),\\\n",
    "          np.average([ndcg_at_k(resi, 100) for resi in test_res]), np.average([mean_reciprocal_rank(resi) for resi in test_res])]\n",
    "    print((\"Epoch: %d  LR: %.4f Train Loss: %.2f  Valid Loss: %.2f  Valid NDCG: %.4f  Test NDCG: %.4f  Test NDCG@100: %.4f  Test MRR: %.4f\") % \\\n",
    "          (epoch, optimizer.param_groups[0]['lr'], np.average(train_losses), loss.cpu().detach().tolist(),\\\n",
    "          np.average([ndcg_at_k(resi, len(resi)) for resi in valid_res]), np.average([ndcg_at_k(resi, len(resi)) for resi in test_res]),\\\n",
    "          np.average([ndcg_at_k(resi, 100) for resi in test_res]), np.average([mean_reciprocal_rank(resi) for resi in test_res])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
