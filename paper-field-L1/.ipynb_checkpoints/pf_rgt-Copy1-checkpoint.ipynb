{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/datadrive/data_cs/'\n",
    "batch_size = 256\n",
    "batch_num  = 128\n",
    "epoch_num  = 100\n",
    "samp_num   = 7\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "graph = dill.load(open(data_dir + 'graph.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_subgraph(graph, time_range, sampled_depth = 2, sampled_number = 8, inp = None):\n",
    "    '''\n",
    "        Sample Sub-Graph based on the connection of other nodes with currently sampled nodes\n",
    "        We maintain budgets for each node type, indexed by <node_id, time>.\n",
    "        Currently sampled nodes are stored in layer_data.\n",
    "        After nodes are sampled, we construct the sampled adjacancy matrix.\n",
    "    '''\n",
    "    layer_data  = defaultdict( #target_type\n",
    "                        lambda: {} # {target_id: [ser, time]}\n",
    "                    )\n",
    "    budget     = defaultdict( #source_type\n",
    "                                    lambda: defaultdict(  #source_id\n",
    "                                        lambda: [0., 2000] #sampled_score, source_time\n",
    "                            ))\n",
    "    new_layer_adj  = defaultdict( #target_type\n",
    "                                    lambda: defaultdict(  #source_type\n",
    "                                        lambda: defaultdict(  #relation_type\n",
    "                                            lambda: [] #[target_id, source_id]\n",
    "                                )))\n",
    "    '''\n",
    "        For each node being sampled, we find out all its neighborhood, \n",
    "        adding the degree count of these nodes in the budget.\n",
    "        Note that there exist some nodes that have many neighborhoods\n",
    "        (such as fields, venues), for those case, we only consider \n",
    "    '''\n",
    "    def add_budget(te, target_id, target_time, layer_data, budget):\n",
    "        for source_type in te:\n",
    "            tes = te[source_type]\n",
    "            for relation_type in tes:\n",
    "                if relation_type == 'self':\n",
    "                    continue\n",
    "                adl = tes[relation_type][target_id]\n",
    "                if len(adl) < sampled_number:\n",
    "                    sampled_ids = list(adl.keys())\n",
    "                else:\n",
    "                    sampled_ids = np.random.choice(list(adl.keys()), sampled_number, replace = False)\n",
    "                for source_id in sampled_ids:\n",
    "                    source_time = adl[source_id]\n",
    "                    if source_time == None:\n",
    "                        source_time = target_time\n",
    "                    '''\n",
    "                        If the node's time is out of range or already being sampled, skip\n",
    "                        Otherwise, accumulate the normalized degree.\n",
    "                    '''\n",
    "                    if source_time not in time_range or source_id in layer_data[source_type]:\n",
    "                        continue\n",
    "                    budget[source_type][source_id][0] += 1. / len(sampled_ids)\n",
    "                    budget[source_type][source_id][1] = source_time\n",
    "    '''\n",
    "        The encode and decode function is used to index each node\n",
    "        by its node_id and time together. So that a same node with\n",
    "        different timestamps can exist in the sampled graph.\n",
    "    '''\n",
    "    \n",
    "    if inp == None:\n",
    "        _time = np.random.choice(list(time_range.keys()))\n",
    "        res = graph.node_feature['paper'][graph.node_feature['paper']['time'] == _time]\n",
    "        sampn = min(len(res), sampled_number)\n",
    "        rand_paper_ids  = np.random.choice(list(res.index), sampn, replace = False)\n",
    "        '''\n",
    "            First adding the sampled nodes then updating budget.\n",
    "        '''\n",
    "        for _id in rand_paper_ids:\n",
    "            layer_data['paper'][_id] = [len(layer_data['paper']), _time]\n",
    "        for _id in rand_paper_ids:\n",
    "            add_budget(graph.edge_list['paper'], _id, _time, layer_data, budget)\n",
    "    else:\n",
    "        '''\n",
    "            First adding the sampled nodes then updating budget.\n",
    "        '''\n",
    "        for _type in inp:\n",
    "            for _id, _time in inp[_type]:\n",
    "                layer_data[_type][_id] = [len(layer_data[_type]), _time]\n",
    "        for _type in inp:\n",
    "            te = graph.edge_list[_type]\n",
    "            for _id, _time in inp[_type]:\n",
    "                add_budget(te, _id, _time, layer_data, budget)\n",
    "    '''\n",
    "        We recursively expand the sampled graph by sampled_depth.\n",
    "        Each time we sample a fixed number of nodes for each budget,\n",
    "        based on the accumulated degree.\n",
    "    '''\n",
    "    for layer in range(sampled_depth):\n",
    "        sts = list(budget.keys())\n",
    "        for source_type in sts:\n",
    "            te = graph.edge_list[source_type]\n",
    "            keys  = np.array(list(budget[source_type].keys()))\n",
    "            vals  = np.array(list(budget[source_type].values()))\n",
    "            if sampled_number > len(keys):\n",
    "                '''\n",
    "                    Directly sample all the nodes\n",
    "                '''\n",
    "                sampled_ids = np.arange(len(keys))\n",
    "            else:\n",
    "                '''\n",
    "                    Sample based on accumulated degree\n",
    "                '''\n",
    "                score = vals[:,0] ** 2\n",
    "                score = score / np.sum(score)\n",
    "                sampled_ids = np.random.choice(len(score), sampled_number, p = score, replace = False) \n",
    "            sampled_keys = keys[sampled_ids]\n",
    "            sampled_tims = vals[sampled_ids][:, 1]\n",
    "            '''\n",
    "                First adding the sampled nodes then updating budget.\n",
    "            '''\n",
    "            for k, t in zip(sampled_keys, sampled_tims):\n",
    "                layer_data[source_type][k] = [len(layer_data[source_type]), t]\n",
    "            for k, t in zip(sampled_keys, sampled_tims):\n",
    "                add_budget(te, int(k), int(t), layer_data, budget)\n",
    "                budget[source_type].pop(k)    \n",
    "    '''\n",
    "        Prepare feature, time and adjacency matrix for the sampled graph\n",
    "    '''\n",
    "    feature = {}\n",
    "    times   = {}\n",
    "    indxs   = {}\n",
    "    for _type in layer_data:\n",
    "        idxs = []\n",
    "        tims = []\n",
    "        for k in layer_data[_type]:\n",
    "            idxs += [k]\n",
    "            tims += [layer_data[_type][k][1]]\n",
    "        if 'node_emb' in graph.node_feature[_type]:\n",
    "            feature[_type] = np.array(list(graph.node_feature['field'].loc[idxs, 'node_emb']), dtype=np.float)\n",
    "        else:\n",
    "            feature[_type] = np.zeros([len(layer_data[_type]), 400])\n",
    "        feature[_type] = np.concatenate((feature[_type], list(graph.node_feature[_type].loc[idxs, 'emb']),\\\n",
    "            np.log10(np.array(list(graph.node_feature[_type].loc[idxs, 'citation'])).reshape(-1, 1) + 0.01)), axis=1)\n",
    "        indxs[_type] = idxs\n",
    "        times[_type] = tims\n",
    "        \n",
    "    edge_list = defaultdict( #target_type\n",
    "                        lambda: defaultdict(  #source_type\n",
    "                            lambda: defaultdict(  #relation_type\n",
    "                                lambda: [] # [target_id, source_id] \n",
    "                                    )))\n",
    "    for _type in layer_data:\n",
    "        for _key in layer_data[_type]:\n",
    "            _ser = layer_data[_type][_key][0]\n",
    "            edge_list[_type][_type]['self'] += [[_ser, _ser]]\n",
    "    '''\n",
    "        Reconstruct sampled adjacancy matrix by checking whether each\n",
    "        link exist in the original graph\n",
    "    '''\n",
    "    for target_type in graph.edge_list:\n",
    "        te = graph.edge_list[target_type]\n",
    "        for source_type in te:\n",
    "            tes = te[source_type]\n",
    "            for relation_type in tes:\n",
    "                if relation_type in ['APA_coauthor', 'rev_APV_in', 'IPI_coauthor', 'APV_in']:\n",
    "                    continue\n",
    "                tesr = tes[relation_type]\n",
    "                for target_key in layer_data[target_type]:\n",
    "                    target_ser = layer_data[target_type][target_key][0]\n",
    "                    tesrt = tesr[target_key]\n",
    "                    for source_key in layer_data[source_type]:\n",
    "                        source_ser = layer_data[source_type][source_key][0]\n",
    "                        '''\n",
    "                            Check whether each link (target_id, source_id) exist in original adjacancy matrix\n",
    "                        '''\n",
    "                        if source_key in tesrt:\n",
    "                            edge_list[target_type][source_type][relation_type] += [[target_ser, source_ser]]\n",
    "    return feature, times, edge_list, indxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = {t: True for t in graph.times if t != None and t < 2015}\n",
    "valid_range = {t: True for t in graph.times if t != None and t >= 2015  and t <= 2016}\n",
    "test_range  = {t: True for t in graph.times if t != None and t > 2016}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pf_sample(seed, papers, pairs, t_range, batch_size, test = False):\n",
    "    np.random.seed(seed)\n",
    "    _time = np.random.choice(list(papers.keys()))\n",
    "    sampn = min(len(papers[_time]), batch_size)\n",
    "    pids = np.array(papers[_time])[np.random.choice(len(papers[_time]), sampn, replace = False)]\n",
    "    fids = []\n",
    "    edge = defaultdict(lambda: {})\n",
    "    for x_id, p_id in enumerate(pids):\n",
    "        f_ids = pairs[p_id]\n",
    "        for f_id in f_ids:\n",
    "            if f_id not in fids:\n",
    "                fids += [f_id]\n",
    "            edge[x_id][fids.index(f_id)] = True\n",
    "    pids = np.stack([pids, np.repeat([_time], sampn)]).T\n",
    "    fids = np.stack([fids, np.repeat([_time], len(fids))]).T\n",
    " \n",
    "    feature, times, edge_list, _ = sample_subgraph(graph, t_range, \\\n",
    "                inp = {'paper': pids, 'field': fids}, sampled_depth = 3, sampled_number = 100)\n",
    "\n",
    "    el = []\n",
    "    for i in edge_list['paper']['field']['rev_PF_in_L2']:\n",
    "        if i[0] in edge and i[1] in edge[i[0]]:\n",
    "            continue\n",
    "        el += [i]\n",
    "    edge_list['paper']['field']['rev_PF_in_L2'] = el\n",
    "\n",
    "    el = []\n",
    "    for i in edge_list['field']['paper']['PF_in_L2']:\n",
    "        if i[1] in edge and i[0] in edge[i[1]]:\n",
    "            continue\n",
    "        el += [i]\n",
    "    edge_list['field']['paper']['PF_in_L2'] = el\n",
    "    \n",
    "    \n",
    "    node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict = \\\n",
    "            to_torch(feature, times, edge_list, graph)\n",
    "    '''\n",
    "        Trace the paper_id and field_id by its own index plus the type start index\n",
    "    '''\n",
    "    paper_ids = np.arange(len(pids)) + node_dict['paper'][0]\n",
    "    field_ids = np.arange(len(fids)) + node_dict['field'][0]\n",
    "    ylabel = torch.zeros(sampn, len(cand_list))\n",
    "    for x_id, p_id in enumerate(pids[:,0]):\n",
    "        for f_id in pairs[p_id]:\n",
    "            ylabel[x_id][cand_list.index(f_id)] = 1\n",
    "    ylabel /= ylabel.sum(axis=1).view(-1, 1)\n",
    "    return node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel\n",
    "    \n",
    "def prepare_data(pool, process_ids):\n",
    "    jobs = []\n",
    "    for process_id in process_ids[:-1]:\n",
    "        p = pool.apply_async(pf_sample, args=(np.random.randint(2**32 - 1), train_papers, \\\n",
    "                                               train_pairs, train_range, batch_size))\n",
    "        jobs.append(p)\n",
    "    p = pool.apply_async(pf_sample, args=(np.random.randint(2**32 - 1), valid_papers, \\\n",
    "                                           valid_pairs, valid_range, batch_size))\n",
    "    jobs.append(p)\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_dim, n_hid, num_types, num_relations, n_heads, n_layers, dropout = 0.3):\n",
    "        super(GNN, self).__init__()\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.num_types = num_types\n",
    "        self.in_dim    = in_dim\n",
    "        self.n_hid     = n_hid\n",
    "        self.aggregat_ws   = nn.ModuleList()\n",
    "        self.drop          = nn.Dropout(dropout)\n",
    "        for t in range(num_types):\n",
    "            self.aggregat_ws.append(nn.Linear(in_dim, n_hid))\n",
    "        for l in range(n_layers):\n",
    "            self.gcs.append(RAGCNConv(n_hid, n_hid, num_types, num_relations, n_heads, dropout))\n",
    "    def set_device(self, device):\n",
    "        self.device = device\n",
    "        for gc in self.gcs:\n",
    "            gc.device = device\n",
    "    def forward(self, node_feature, node_type, edge_time, edge_index, edge_type):\n",
    "        res = torch.zeros(node_feature.size(0), self.n_hid).to(node_feature.device)\n",
    "        for t_id in range(self.num_types):\n",
    "            aggregat_w = self.aggregat_ws[t_id]\n",
    "            idx = (node_type == t_id)\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            res[idx] = torch.tanh(aggregat_w(node_feature[idx]))\n",
    "        meta_xs = self.drop(res)\n",
    "        del res\n",
    "        for gc in self.gcs:\n",
    "            meta_xs = gc(meta_xs, node_type, edge_index, edge_type, edge_time)\n",
    "        return meta_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Paper-Field\n",
    "'''\n",
    "paper_ser = {}\n",
    "\n",
    "train_pairs = {}\n",
    "valid_pairs = {}\n",
    "test_pairs  = {}\n",
    "\n",
    "train_papers = {_time: {} for _time in train_range}\n",
    "valid_papers = {_time: {} for _time in valid_range}\n",
    "test_papers  = {_time: {} for _time in test_range}\n",
    "\n",
    "for f_id in graph.edge_list['field']['paper']['PF_in_L2']:\n",
    "    for p_id in graph.edge_list['field']['paper']['PF_in_L2'][f_id]:\n",
    "        _time = graph.edge_list['field']['paper']['PF_in_L2'][f_id][p_id]\n",
    "        if _time in train_range:\n",
    "            if p_id not in train_pairs:\n",
    "                train_pairs[p_id] = []\n",
    "            train_pairs[p_id] += [f_id]\n",
    "            train_papers[_time][p_id] = True\n",
    "        elif _time in valid_range:\n",
    "            if p_id not in valid_pairs:\n",
    "                valid_pairs[p_id] = []\n",
    "            valid_pairs[p_id] += [f_id]\n",
    "            valid_papers[_time][p_id] = True\n",
    "        else:\n",
    "            if p_id not in test_pairs:\n",
    "                test_pairs[p_id] = []\n",
    "            test_pairs[p_id] += [f_id]\n",
    "            test_papers[_time][p_id] = True\n",
    "for _time in list(train_papers.keys()):\n",
    "    if len(train_papers[_time]) < batch_size // 2:\n",
    "        train_papers.pop(_time)\n",
    "    else:\n",
    "        train_papers[_time] = np.array(list(train_papers[_time].keys()))\n",
    "for _time in list(valid_papers.keys()):\n",
    "    if len(valid_papers[_time]) < batch_size // 2:\n",
    "        valid_papers.pop(_time)\n",
    "    else:\n",
    "        valid_papers[_time] = np.array(list(valid_papers[_time].keys()))\n",
    "for _time in list(test_papers.keys()):\n",
    "    if len(test_papers[_time]) < batch_size // 2:\n",
    "        test_papers.pop(_time)\n",
    "    else:\n",
    "        test_papers[_time] = np.array(list(test_papers[_time].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = graph.get_types()\n",
    "cand_list = list(graph.edge_list['field']['paper']['PF_in_L2'])\n",
    "gnn = GNN(in_dim = len(graph.node_feature['paper']['emb'][0]) + 401, n_hid = 256, num_types = len(types), \\\n",
    "          num_relations = len(graph.get_meta_graph()) + 1, n_heads = 8, n_layers = 3).to(device)\n",
    "# gnn = torch.load('../pre-train/save/cpc_model.pt').to(device)\n",
    "classifier = Classifier(256, len(cand_list)).to(device)\n",
    "model = nn.Sequential(gnn, classifier)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 1000, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 44.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (102.2s)  LR: 0.00060 Train Loss: 8.48  Valid Loss: 6.82  Valid NDCG: 0.2055\n",
      "Data Preparation: 2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 (107.8s)  LR: 0.00069 Train Loss: 6.50  Valid Loss: 6.46  Valid NDCG: 0.2064\n",
      "Data Preparation: 2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 (105.4s)  LR: 0.00078 Train Loss: 6.34  Valid Loss: 6.46  Valid NDCG: 0.2179\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 4 (105.6s)  LR: 0.00085 Train Loss: 6.17  Valid Loss: 6.55  Valid NDCG: 0.2124\n",
      "Data Preparation: 2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 (106.7s)  LR: 0.00091 Train Loss: 6.08  Valid Loss: 6.31  Valid NDCG: 0.2215\n",
      "0.2065509787238171\n",
      "Data Preparation: 10.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 (104.8s)  LR: 0.00096 Train Loss: 5.97  Valid Loss: 6.21  Valid NDCG: 0.2252\n",
      "Data Preparation: 2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 (105.1s)  LR: 0.00099 Train Loss: 5.90  Valid Loss: 6.09  Valid NDCG: 0.2390\n",
      "Data Preparation: 2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 (105.1s)  LR: 0.00100 Train Loss: 5.76  Valid Loss: 6.17  Valid NDCG: 0.2411\n",
      "Data Preparation: 2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 (108.3s)  LR: 0.00099 Train Loss: 5.71  Valid Loss: 6.00  Valid NDCG: 0.2501\n",
      "Data Preparation: 2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 (105.1s)  LR: 0.00096 Train Loss: 5.64  Valid Loss: 6.03  Valid NDCG: 0.2534\n",
      "0.24316243173578475\n",
      "Data Preparation: 11.0s\n",
      "Epoch: 11 (105.8s)  LR: 0.00092 Train Loss: 5.66  Valid Loss: 6.04  Valid NDCG: 0.2496\n",
      "Data Preparation: 2.6s\n",
      "Epoch: 12 (102.5s)  LR: 0.00086 Train Loss: 5.44  Valid Loss: 6.05  Valid NDCG: 0.2385\n",
      "Data Preparation: 2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 (105.2s)  LR: 0.00079 Train Loss: 5.48  Valid Loss: 5.89  Valid NDCG: 0.2626\n",
      "Data Preparation: 2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 (104.4s)  LR: 0.00070 Train Loss: 5.53  Valid Loss: 5.80  Valid NDCG: 0.2695\n",
      "Data Preparation: 2.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 (102.5s)  LR: 0.00061 Train Loss: 5.39  Valid Loss: 5.67  Valid NDCG: 0.2711\n",
      "0.24418578938370966\n",
      "Data Preparation: 11.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 (106.3s)  LR: 0.00051 Train Loss: 5.43  Valid Loss: 5.71  Valid NDCG: 0.2736\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 17 (106.1s)  LR: 0.00042 Train Loss: 5.37  Valid Loss: 5.65  Valid NDCG: 0.2727\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 18 (106.3s)  LR: 0.00032 Train Loss: 5.36  Valid Loss: 5.68  Valid NDCG: 0.2639\n",
      "Data Preparation: 2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 (107.0s)  LR: 0.00024 Train Loss: 5.35  Valid Loss: 5.52  Valid NDCG: 0.2889\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 20 (105.5s)  LR: 0.00016 Train Loss: 5.30  Valid Loss: 5.71  Valid NDCG: 0.2654\n",
      "0.26081106024485756\n",
      "Data Preparation: 11.6s\n",
      "Epoch: 21 (100.6s)  LR: 0.00009 Train Loss: 5.23  Valid Loss: 5.60  Valid NDCG: 0.2813\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 22 (105.1s)  LR: 0.00005 Train Loss: 5.26  Valid Loss: 5.71  Valid NDCG: 0.2667\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 23 (104.0s)  LR: 0.00001 Train Loss: 5.28  Valid Loss: 5.63  Valid NDCG: 0.2736\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 24 (102.7s)  LR: 0.00000 Train Loss: 5.29  Valid Loss: 5.71  Valid NDCG: 0.2668\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 25 (104.9s)  LR: 0.00001 Train Loss: 5.32  Valid Loss: 5.75  Valid NDCG: 0.2757\n",
      "0.26879638834007635\n",
      "Data Preparation: 11.8s\n",
      "Epoch: 26 (106.0s)  LR: 0.00003 Train Loss: 5.35  Valid Loss: 5.57  Valid NDCG: 0.2829\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 27 (103.6s)  LR: 0.00007 Train Loss: 5.25  Valid Loss: 5.69  Valid NDCG: 0.2834\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 28 (106.2s)  LR: 0.00013 Train Loss: 5.29  Valid Loss: 5.66  Valid NDCG: 0.2775\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 29 (108.1s)  LR: 0.00020 Train Loss: 5.28  Valid Loss: 5.67  Valid NDCG: 0.2609\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 30 (101.8s)  LR: 0.00029 Train Loss: 5.16  Valid Loss: 5.63  Valid NDCG: 0.2712\n",
      "0.24102179336408783\n",
      "Data Preparation: 11.4s\n",
      "Epoch: 31 (103.6s)  LR: 0.00038 Train Loss: 5.16  Valid Loss: 5.58  Valid NDCG: 0.2820\n",
      "Data Preparation: 2.7s\n",
      "Epoch: 32 (103.9s)  LR: 0.00048 Train Loss: 5.20  Valid Loss: 5.58  Valid NDCG: 0.2841\n",
      "Data Preparation: 2.7s\n",
      "Epoch: 33 (104.4s)  LR: 0.00057 Train Loss: 5.11  Valid Loss: 5.50  Valid NDCG: 0.2834\n",
      "Data Preparation: 3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34 (103.9s)  LR: 0.00067 Train Loss: 5.14  Valid Loss: 5.53  Valid NDCG: 0.3000\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 35 (104.8s)  LR: 0.00075 Train Loss: 5.09  Valid Loss: 5.45  Valid NDCG: 0.2958\n",
      "0.28298487061919647\n",
      "Data Preparation: 11.6s\n",
      "Epoch: 36 (106.4s)  LR: 0.00083 Train Loss: 5.15  Valid Loss: 5.51  Valid NDCG: 0.2867\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 37 (107.4s)  LR: 0.00090 Train Loss: 5.18  Valid Loss: 5.58  Valid NDCG: 0.2953\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 38 (104.3s)  LR: 0.00095 Train Loss: 5.06  Valid Loss: 5.54  Valid NDCG: 0.2922\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 39 (105.5s)  LR: 0.00098 Train Loss: 5.02  Valid Loss: 5.64  Valid NDCG: 0.2861\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 40 (103.7s)  LR: 0.00100 Train Loss: 4.97  Valid Loss: 5.50  Valid NDCG: 0.2739\n",
      "0.29186716247919037\n",
      "Data Preparation: 11.6s\n",
      "Epoch: 41 (100.8s)  LR: 0.00100 Train Loss: 4.83  Valid Loss: 5.45  Valid NDCG: 0.2995\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 42 (103.9s)  LR: 0.00097 Train Loss: 4.95  Valid Loss: 5.55  Valid NDCG: 0.2962\n",
      "Data Preparation: 3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 43 (105.0s)  LR: 0.00093 Train Loss: 4.99  Valid Loss: 5.33  Valid NDCG: 0.3004\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 44 (105.6s)  LR: 0.00088 Train Loss: 4.89  Valid Loss: 5.53  Valid NDCG: 0.2975\n",
      "Data Preparation: 3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 (105.3s)  LR: 0.00081 Train Loss: 4.87  Valid Loss: 5.39  Valid NDCG: 0.3169\n",
      "0.32118589836254946\n",
      "Data Preparation: 11.4s\n",
      "Epoch: 46 (104.7s)  LR: 0.00072 Train Loss: 4.83  Valid Loss: 5.40  Valid NDCG: 0.3006\n",
      "Data Preparation: 2.7s\n",
      "Epoch: 47 (106.4s)  LR: 0.00063 Train Loss: 4.85  Valid Loss: 5.44  Valid NDCG: 0.3072\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 48 (105.0s)  LR: 0.00054 Train Loss: 4.80  Valid Loss: 5.32  Valid NDCG: 0.3123\n",
      "Data Preparation: 3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49 (105.9s)  LR: 0.00044 Train Loss: 4.78  Valid Loss: 5.20  Valid NDCG: 0.3264\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 50 (103.2s)  LR: 0.00035 Train Loss: 4.78  Valid Loss: 5.24  Valid NDCG: 0.3152\n",
      "0.25008213419777886\n",
      "Data Preparation: 11.3s\n",
      "Epoch: 51 (104.9s)  LR: 0.00026 Train Loss: 4.82  Valid Loss: 5.37  Valid NDCG: 0.2968\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 52 (101.9s)  LR: 0.00018 Train Loss: 4.64  Valid Loss: 5.18  Valid NDCG: 0.3189\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 53 (103.9s)  LR: 0.00011 Train Loss: 4.69  Valid Loss: 5.37  Valid NDCG: 0.3018\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 54 (107.8s)  LR: 0.00006 Train Loss: 4.75  Valid Loss: 5.36  Valid NDCG: 0.3187\n",
      "Data Preparation: 3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55 (100.2s)  LR: 0.00002 Train Loss: 4.59  Valid Loss: 5.31  Valid NDCG: 0.3300\n",
      "0.294239288405747\n",
      "Data Preparation: 11.4s\n",
      "Epoch: 56 (103.8s)  LR: 0.00000 Train Loss: 4.77  Valid Loss: 5.20  Valid NDCG: 0.3148\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 57 (104.6s)  LR: 0.00000 Train Loss: 4.66  Valid Loss: 5.22  Valid NDCG: 0.3063\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 58 (103.5s)  LR: 0.00002 Train Loss: 4.72  Valid Loss: 5.22  Valid NDCG: 0.3258\n",
      "Data Preparation: 2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 59 (103.4s)  LR: 0.00006 Train Loss: 4.71  Valid Loss: 4.98  Valid NDCG: 0.3331\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 60 (108.7s)  LR: 0.00012 Train Loss: 4.78  Valid Loss: 5.17  Valid NDCG: 0.3282\n",
      "0.33473263476907605\n",
      "Data Preparation: 11.6s\n",
      "Epoch: 61 (103.2s)  LR: 0.00018 Train Loss: 4.63  Valid Loss: 5.16  Valid NDCG: 0.3219\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 62 (105.4s)  LR: 0.00027 Train Loss: 4.70  Valid Loss: 5.16  Valid NDCG: 0.3278\n",
      "Data Preparation: 2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 63 (104.4s)  LR: 0.00036 Train Loss: 4.61  Valid Loss: 5.10  Valid NDCG: 0.3340\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 64 (108.3s)  LR: 0.00045 Train Loss: 4.81  Valid Loss: 5.26  Valid NDCG: 0.3225\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 65 (104.2s)  LR: 0.00055 Train Loss: 4.68  Valid Loss: 5.14  Valid NDCG: 0.3193\n",
      "0.24243862079081255\n",
      "Data Preparation: 11.1s\n",
      "Epoch: 66 (106.2s)  LR: 0.00064 Train Loss: 4.66  Valid Loss: 5.53  Valid NDCG: 0.3114\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 67 (106.9s)  LR: 0.00073 Train Loss: 4.67  Valid Loss: 5.35  Valid NDCG: 0.3075\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 68 (105.0s)  LR: 0.00081 Train Loss: 4.60  Valid Loss: 5.28  Valid NDCG: 0.3230\n",
      "Data Preparation: 2.7s\n",
      "Epoch: 69 (107.6s)  LR: 0.00088 Train Loss: 4.68  Valid Loss: 5.39  Valid NDCG: 0.3012\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 70 (106.8s)  LR: 0.00094 Train Loss: 4.74  Valid Loss: 5.20  Valid NDCG: 0.3224\n",
      "0.25286838839648934\n",
      "Data Preparation: 11.5s\n",
      "Epoch: 71 (105.0s)  LR: 0.00098 Train Loss: 4.61  Valid Loss: 5.13  Valid NDCG: 0.3199\n",
      "Data Preparation: 2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72 (106.7s)  LR: 0.00100 Train Loss: 4.53  Valid Loss: 5.10  Valid NDCG: 0.3391\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 73 (104.2s)  LR: 0.00100 Train Loss: 4.48  Valid Loss: 5.29  Valid NDCG: 0.3171\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 74 (102.9s)  LR: 0.00098 Train Loss: 4.53  Valid Loss: 5.22  Valid NDCG: 0.3221\n",
      "Data Preparation: 3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 75 (103.9s)  LR: 0.00095 Train Loss: 4.42  Valid Loss: 5.05  Valid NDCG: 0.3449\n",
      "0.2854326859358558\n",
      "Data Preparation: 11.7s\n",
      "Epoch: 76 (104.5s)  LR: 0.00089 Train Loss: 4.48  Valid Loss: 5.25  Valid NDCG: 0.3207\n",
      "Data Preparation: 3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 77 (102.7s)  LR: 0.00083 Train Loss: 4.56  Valid Loss: 4.86  Valid NDCG: 0.3645\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 78 (105.1s)  LR: 0.00075 Train Loss: 4.55  Valid Loss: 5.12  Valid NDCG: 0.3386\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 79 (106.3s)  LR: 0.00066 Train Loss: 4.64  Valid Loss: 4.97  Valid NDCG: 0.3353\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 80 (103.6s)  LR: 0.00056 Train Loss: 4.44  Valid Loss: 5.16  Valid NDCG: 0.3229\n",
      "0.3267764033475853\n",
      "Data Preparation: 11.7s\n",
      "Epoch: 81 (106.6s)  LR: 0.00047 Train Loss: 4.59  Valid Loss: 5.00  Valid NDCG: 0.3358\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 82 (107.0s)  LR: 0.00037 Train Loss: 4.56  Valid Loss: 5.06  Valid NDCG: 0.3317\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 83 (104.0s)  LR: 0.00028 Train Loss: 4.45  Valid Loss: 4.94  Valid NDCG: 0.3492\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 84 (107.0s)  LR: 0.00020 Train Loss: 4.56  Valid Loss: 4.91  Valid NDCG: 0.3363\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 85 (106.4s)  LR: 0.00013 Train Loss: 4.40  Valid Loss: 5.07  Valid NDCG: 0.3379\n",
      "0.30661991991506676\n",
      "Data Preparation: 12.1s\n",
      "Epoch: 86 (101.5s)  LR: 0.00007 Train Loss: 4.36  Valid Loss: 5.05  Valid NDCG: 0.3233\n",
      "Data Preparation: 2.7s\n",
      "Epoch: 87 (102.2s)  LR: 0.00003 Train Loss: 4.28  Valid Loss: 5.07  Valid NDCG: 0.3382\n",
      "Data Preparation: 2.7s\n",
      "Epoch: 88 (103.7s)  LR: 0.00001 Train Loss: 4.36  Valid Loss: 5.04  Valid NDCG: 0.3399\n",
      "Data Preparation: 2.7s\n",
      "Epoch: 89 (104.4s)  LR: 0.00000 Train Loss: 4.45  Valid Loss: 5.08  Valid NDCG: 0.3266\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 90 (106.7s)  LR: 0.00002 Train Loss: 4.49  Valid Loss: 5.19  Valid NDCG: 0.3145\n",
      "0.25115787634613446\n",
      "Data Preparation: 11.4s\n",
      "Epoch: 91 (103.0s)  LR: 0.00005 Train Loss: 4.34  Valid Loss: 4.83  Valid NDCG: 0.3530\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 92 (105.0s)  LR: 0.00010 Train Loss: 4.43  Valid Loss: 5.08  Valid NDCG: 0.3288\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 93 (106.2s)  LR: 0.00017 Train Loss: 4.52  Valid Loss: 5.21  Valid NDCG: 0.3396\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 94 (104.2s)  LR: 0.00024 Train Loss: 4.49  Valid Loss: 5.12  Valid NDCG: 0.3223\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 95 (106.0s)  LR: 0.00033 Train Loss: 4.37  Valid Loss: 5.06  Valid NDCG: 0.3450\n",
      "0.32080515660743003\n",
      "Data Preparation: 11.8s\n",
      "Epoch: 96 (104.8s)  LR: 0.00043 Train Loss: 4.50  Valid Loss: 5.07  Valid NDCG: 0.3309\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 97 (104.6s)  LR: 0.00052 Train Loss: 4.32  Valid Loss: 4.93  Valid NDCG: 0.3437\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 98 (102.0s)  LR: 0.00062 Train Loss: 4.28  Valid Loss: 5.00  Valid NDCG: 0.3490\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 99 (104.8s)  LR: 0.00071 Train Loss: 4.30  Valid Loss: 5.03  Valid NDCG: 0.3330\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 100 (106.2s)  LR: 0.00079 Train Loss: 4.39  Valid Loss: 4.88  Valid NDCG: 0.3580\n",
      "0.3085812047814863\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "pool = mp.Pool(4)\n",
    "process_ids = np.arange(batch_num // 4)\n",
    "st = time.time()\n",
    "jobs = prepare_data(pool, process_ids)\n",
    "train_step = 1500\n",
    "best_val   = 0\n",
    "res = []\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "for epoch in np.arange(epoch_num)+1:\n",
    "    '''\n",
    "        Prepare Training and Validation Data\n",
    "    '''\n",
    "    train_data = [job.get() for job in jobs[:-1]]\n",
    "    valid_data = jobs[-1].get()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    pool = mp.Pool(4)\n",
    "    jobs = prepare_data(pool, process_ids)\n",
    "    et = time.time()\n",
    "    print('Data Preparation: %.1fs' % (et - st))\n",
    "    \n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    torch.cuda.empty_cache()\n",
    "    for batch in np.arange(2):\n",
    "        for node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel in train_data:\n",
    "            node_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                                   edge_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "            res  = classifier.forward(node_rep[paper_ids])\n",
    "            loss = criterion(res, ylabel.to(device))\n",
    "            optimizer.zero_grad() \n",
    "            torch.cuda.empty_cache()\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.2)\n",
    "            optimizer.step()\n",
    "            train_losses += [loss.cpu().detach().tolist()]\n",
    "            train_step += 1\n",
    "            scheduler.step(train_step)\n",
    "            del res, loss\n",
    "    '''\n",
    "        Valid\n",
    "    '''\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel = valid_data\n",
    "        node_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                                   edge_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "        res  = classifier.forward(node_rep[paper_ids])\n",
    "        loss = criterion(res, ylabel.to(device))\n",
    "        valid_res = []\n",
    "\n",
    "        for ai, bi in zip(ylabel, res.argsort(descending = True)):\n",
    "            valid_res += [ai[bi].tolist()]\n",
    "        valid_ndcg = np.average([ndcg_at_k(resi, len(resi)) for resi in valid_res])\n",
    "        if valid_ndcg > best_val:\n",
    "            best_val = valid_ndcg\n",
    "            torch.save(model, './save/rgt_1.pt')\n",
    "        st = time.time()\n",
    "        print((\"Epoch: %d (%.1fs)  LR: %.5f Train Loss: %.2f  Valid Loss: %.2f  Valid NDCG: %.4f\") % \\\n",
    "              (epoch, (st-et), optimizer.param_groups[0]['lr'], np.average(train_losses), loss.cpu().detach().tolist(),\\\n",
    "              valid_ndcg))\n",
    "        stats += [[np.average(train_losses), loss.cpu().detach().tolist()]]\n",
    "        del res, loss\n",
    "        if epoch % 5 == 0:\n",
    "            '''\n",
    "                Test\n",
    "            '''\n",
    "            _time = np.random.choice(list(test_papers.keys()))\n",
    "            node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel = pf_sample(np.random.randint(2 ** 32 - 1), test_papers, \\\n",
    "                                                           test_pairs, test_range, batch_size, test=True)\n",
    "            paper_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                      edge_time.to(device), edge_index.to(device), edge_type.to(device))[paper_ids]\n",
    "            res  = classifier.forward(paper_rep)\n",
    "            test_res = []\n",
    "            for ai, bi in zip(ylabel, res.argsort(descending = True)):\n",
    "                test_res += [ai[bi].tolist()]\n",
    "            test_ndcg = np.average([ndcg_at_k(resi, len(resi)) for resi in test_res])\n",
    "            print(test_ndcg)\n",
    "            del res\n",
    "    del train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = np.array(stats)\n",
    "plt.plot(stats[:,0])\n",
    "plt.plot(stats[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "gnn, classifier = model\n",
    "with torch.no_grad():\n",
    "    test_res = []\n",
    "    for _ in range(10):\n",
    "        _time = np.random.choice(list(test_papers.keys()))\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel = pf_sample(np.random.randint(2 ** 32 - 1), test_papers, \\\n",
    "                                                       test_pairs, test_range, batch_size, test=True)\n",
    "        paper_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                      edge_time.to(device), edge_index.to(device), edge_type.to(device))[paper_ids]\n",
    "        res = classifier.forward(paper_rep)\n",
    "        for ai, bi in zip(ylabel, res.argsort(descending = True)):\n",
    "            test_res += [ai[bi].tolist()]\n",
    "    test_ndcg = [ndcg_at_k(resi, len(resi)) for resi in test_res]\n",
    "    print(np.average(test_ndcg), np.var(test_ndcg))\n",
    "    test_mrr = mean_reciprocal_rank(test_res)\n",
    "    print(np.average(test_mrr), np.var(test_mrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('./save/rgt_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "gnn, classifier = best_model\n",
    "with torch.no_grad():\n",
    "    test_res = []\n",
    "    for _ in range(10):\n",
    "        _time = np.random.choice(list(test_papers.keys()))\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel = pf_sample(np.random.randint(2 ** 32 - 1), test_papers, \\\n",
    "                                                       test_pairs, test_range, batch_size, test=True)\n",
    "        paper_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                      edge_time.to(device), edge_index.to(device), edge_type.to(device))[paper_ids]\n",
    "        res = classifier.forward(paper_rep)\n",
    "        for ai, bi in zip(ylabel, res.argsort(descending = True)):\n",
    "            test_res += [ai[bi].tolist()]\n",
    "    test_ndcg = [ndcg_at_k(resi, len(resi)) for resi in test_res]\n",
    "    print(np.average(test_ndcg), np.var(test_ndcg))\n",
    "    test_mrr = mean_reciprocal_rank(test_res)\n",
    "    print(np.average(test_mrr), np.var(test_mrr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
