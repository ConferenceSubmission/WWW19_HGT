{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/datadrive/data_med/'\n",
    "batch_size = 256\n",
    "batch_num  = 128\n",
    "epoch_num  = 200\n",
    "samp_num   = 7\n",
    "\n",
    "device = torch.device(\"cuda:3\")\n",
    "graph = dill.load(open(data_dir + 'graph.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = {t: True for t in graph.times if t != None and t < 2015}\n",
    "valid_range = {t: True for t in graph.times if t != None and t >= 2015  and t <= 2016}\n",
    "test_range  = {t: True for t in graph.times if t != None and t > 2016}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pf_sample(seed, papers, pairs, t_range, batch_size, test = False):\n",
    "    np.random.seed(seed)\n",
    "    _time = np.random.choice(list(papers.keys()))\n",
    "    sampn = min(len(papers[_time]), batch_size)\n",
    "    pids = np.array(papers[_time])[np.random.choice(len(papers[_time]), sampn, replace = False)]\n",
    "    fids = []\n",
    "    edge = defaultdict(lambda: {})\n",
    "    for x_id, p_id in enumerate(pids):\n",
    "        f_ids = pairs[p_id]\n",
    "        for f_id in f_ids:\n",
    "            if f_id not in fids:\n",
    "                fids += [f_id]\n",
    "            edge[x_id][fids.index(f_id)] = True\n",
    "    pids = np.stack([pids, np.repeat([_time], sampn)]).T\n",
    "    fids = np.stack([fids, np.repeat([_time], len(fids))]).T\n",
    " \n",
    "    feature, times, edge_list, _ = sample_subgraph(graph, t_range, \\\n",
    "                inp = {'paper': pids, 'field': fids}, sampled_depth = 3, sampled_number = 100)\n",
    "\n",
    "    el = []\n",
    "    for i in edge_list['paper']['field']['rev_PF_in_L1']:\n",
    "        if i[0] in edge and i[1] in edge[i[0]]:\n",
    "            continue\n",
    "        el += [i]\n",
    "    edge_list['paper']['field']['rev_PF_in_L1'] = el\n",
    "\n",
    "    el = []\n",
    "    for i in edge_list['field']['paper']['PF_in_L1']:\n",
    "        if i[1] in edge and i[0] in edge[i[1]]:\n",
    "            continue\n",
    "        el += [i]\n",
    "    edge_list['field']['paper']['PF_in_L1'] = el\n",
    "    \n",
    "    \n",
    "    node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict = \\\n",
    "            to_torch(feature, times, edge_list, graph)\n",
    "    '''\n",
    "        Trace the paper_id and field_id by its own index plus the type start index\n",
    "    '''\n",
    "    paper_ids = np.arange(len(pids)) + node_dict['paper'][0]\n",
    "    field_ids = np.arange(len(fids)) + node_dict['field'][0]\n",
    "    ylabel = torch.zeros(sampn, len(cand_list))\n",
    "    for x_id, p_id in enumerate(pids[:,0]):\n",
    "        for f_id in pairs[p_id]:\n",
    "            ylabel[x_id][cand_list.index(f_id)] = 1\n",
    "    ylabel /= ylabel.sum(axis=1).view(-1, 1)\n",
    "    return node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel\n",
    "    \n",
    "def prepare_data(pool, process_ids):\n",
    "    jobs = []\n",
    "    for process_id in process_ids[:-1]:\n",
    "        p = pool.apply_async(pf_sample, args=(np.random.randint(2**32 - 1), train_papers, \\\n",
    "                                               train_pairs, train_range, batch_size))\n",
    "        jobs.append(p)\n",
    "    p = pool.apply_async(pf_sample, args=(np.random.randint(2**32 - 1), valid_papers, \\\n",
    "                                           valid_pairs, valid_range, batch_size))\n",
    "    jobs.append(p)\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HANConv(MessagePassing):\n",
    "    def __init__(self, in_dim, out_dim, num_relations, n_heads, dropout = 0.3, **kwargs):\n",
    "        super(HANConv, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_dim        = in_dim\n",
    "        self.out_dim       = out_dim\n",
    "        self.num_relations = num_relations\n",
    "        self.n_heads       = n_heads\n",
    "        self.d_k           = out_dim // n_heads\n",
    "        \n",
    "        self.aggregat_rws   = nn.ModuleList()\n",
    "        self.interact_rws   = nn.ModuleList()\n",
    "        for t in range(num_relations):\n",
    "            self.interact_rws.append(nn.Linear(2 * self.d_k,   1))\n",
    "            self.aggregat_rws.append(nn.Linear(in_dim,   out_dim))\n",
    "        self.drop            = nn.Dropout(dropout)\n",
    "        self.semantic_q      = nn.Linear(out_dim, out_dim)\n",
    "        self.semantic_v      = nn.Linear(out_dim, 1)\n",
    "    def forward(self, node_inp, edge_index, edge_type):\n",
    "        return self.propagate(edge_index, node_inp=node_inp, edge_type=edge_type)\n",
    "\n",
    "    def message(self, edge_index_i, node_inp_i, node_inp_j, edge_type, num_nodes):\n",
    "        '''\n",
    "            i: target; j: source\n",
    "        '''\n",
    "        data_size = edge_index_i.size(0)\n",
    "        res     = torch.zeros(data_size, self.n_heads, self.d_k).to(node_inp_i.device)\n",
    "        res_att     = torch.zeros(data_size, self.n_heads).to(node_inp_i.device)\n",
    "        res_sem     = torch.zeros(self.num_relations, 1).to(node_inp_i.device)\n",
    "\n",
    "        for relation_id in range(self.num_relations):\n",
    "            idx = (edge_type == relation_id)\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            _node_inp_i = self.aggregat_rws[relation_id](node_inp_i[idx]).view(-1, self.n_heads, self.d_k)\n",
    "            _node_inp_j = self.aggregat_rws[relation_id](node_inp_j[idx]).view(-1, self.n_heads, self.d_k)\n",
    "            \n",
    "            res[idx] = _node_inp_j\n",
    "            s = torch.cat([_node_inp_i, _node_inp_j], dim=-1)\n",
    "            res_att[idx]   = self.interact_rws[relation_id](s).squeeze()\n",
    "        res_att = F.leaky_relu(res_att, 0.2)\n",
    "        res_att = softmax(res_att, edge_index_i, data_size)\n",
    "        res = (res * res_att.view(-1, self.n_heads, 1)).view(-1, self.out_dim)\n",
    "        for relation_id in range(self.num_relations):\n",
    "            idx = (edge_type == relation_id)\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            sem_res = res[idx].mean(dim=0)\n",
    "            sem_att = self.semantic_v(torch.tanh(self.semantic_q(sem_res)))\n",
    "            res_sem[relation_id] = sem_att\n",
    "        res_sem = F.softmax(res_sem, dim=0)\n",
    "        res = res.view(-1, self.n_heads, self.d_k)\n",
    "        ores     = torch.zeros(data_size, self.n_heads, self.d_k).to(node_inp_i.device)\n",
    "        for relation_id in range(self.num_relations):\n",
    "            idx = (edge_type == relation_id)\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            ores[idx] = res[idx] * res_sem[relation_id]\n",
    "        del res, res_att, res_sem\n",
    "        return ores.view(-1, self.out_dim)\n",
    "    \n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(in_dim={}, out_dim={}, num_rel={})'.format(\n",
    "            self.__class__.__name__, self.in_dim, self.out_dim, self.num_relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_dim, n_hid, num_types, num_relations, n_layers, n_heads, dropout = 0.3):\n",
    "        super(GNN, self).__init__()\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.num_types = num_types\n",
    "        self.in_dim    = in_dim\n",
    "        self.n_hid     = n_hid\n",
    "        self.aggregat_ws   = nn.ModuleList()\n",
    "        self.drop          = nn.Dropout(dropout)\n",
    "        for t in range(num_types):\n",
    "            self.aggregat_ws.append(nn.Linear(in_dim, n_hid))\n",
    "        for l in range(n_layers):\n",
    "            self.gcs.append(HANConv(n_hid, n_hid, num_relations, n_heads))\n",
    "    def forward(self, node_feature, node_type, edge_index, edge_type):\n",
    "        res = torch.zeros(node_feature.size(0), self.n_hid).to(node_feature.device)\n",
    "        for t_id in range(self.num_types):\n",
    "            aggregat_w = self.aggregat_ws[t_id]\n",
    "            idx = (node_type == t_id)\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            res[idx] = torch.tanh(aggregat_w(node_feature[idx]))\n",
    "        meta_xs = self.drop(res)\n",
    "        del res\n",
    "        for gc in self.gcs:\n",
    "            meta_xs = self.drop(F.relu(gc(meta_xs, edge_index, edge_type)))\n",
    "        return meta_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GNN' object has no attribute 'n_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8c1d1f2ba10b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    589\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 591\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GNN' object has no attribute 'n_layers'"
     ]
    }
   ],
   "source": [
    "gnn.n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Paper-Field\n",
    "'''\n",
    "paper_ser = {}\n",
    "\n",
    "train_pairs = {}\n",
    "valid_pairs = {}\n",
    "test_pairs  = {}\n",
    "\n",
    "train_papers = {_time: {} for _time in train_range}\n",
    "valid_papers = {_time: {} for _time in valid_range}\n",
    "test_papers  = {_time: {} for _time in test_range}\n",
    "\n",
    "for f_id in graph.edge_list['field']['paper']['PF_in_L1']:\n",
    "    for p_id in graph.edge_list['field']['paper']['PF_in_L1'][f_id]:\n",
    "        _time = graph.edge_list['field']['paper']['PF_in_L1'][f_id][p_id]\n",
    "        if _time in train_range:\n",
    "            if p_id not in train_pairs:\n",
    "                train_pairs[p_id] = []\n",
    "            train_pairs[p_id] += [f_id]\n",
    "            train_papers[_time][p_id] = True\n",
    "        elif _time in valid_range:\n",
    "            if p_id not in valid_pairs:\n",
    "                valid_pairs[p_id] = []\n",
    "            valid_pairs[p_id] += [f_id]\n",
    "            valid_papers[_time][p_id] = True\n",
    "        else:\n",
    "            if p_id not in test_pairs:\n",
    "                test_pairs[p_id] = []\n",
    "            test_pairs[p_id] += [f_id]\n",
    "            test_papers[_time][p_id] = True\n",
    "for _time in list(train_papers.keys()):\n",
    "    if len(train_papers[_time]) < batch_size // 2:\n",
    "        train_papers.pop(_time)\n",
    "    else:\n",
    "        train_papers[_time] = np.array(list(train_papers[_time].keys()))\n",
    "for _time in list(valid_papers.keys()):\n",
    "    if len(valid_papers[_time]) < batch_size // 2:\n",
    "        valid_papers.pop(_time)\n",
    "    else:\n",
    "        valid_papers[_time] = np.array(list(valid_papers[_time].keys()))\n",
    "for _time in list(test_papers.keys()):\n",
    "    if len(test_papers[_time]) < batch_size // 2:\n",
    "        test_papers.pop(_time)\n",
    "    else:\n",
    "        test_papers[_time] = np.array(list(test_papers[_time].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = graph.get_types()\n",
    "cand_list = [k for k in graph.edge_list['field']['paper']['PF_in_L1'] if len(graph.edge_list['field']['paper']['PF_in_L1'][k]) > 0]\n",
    "gnn = GNN(in_dim = len(graph.node_feature['paper']['emb'][0]) + 401, n_hid = 256, num_types = len(types), \\\n",
    "          num_relations = len(graph.get_meta_graph()) + 1, n_heads = 8, n_layers = 3).to(device)\n",
    "# gnn = torch.load('../pre-train/save/cpc_model.pt').to(device)\n",
    "classifier = Classifier(256, len(cand_list)).to(device)\n",
    "model = nn.Sequential(gnn, classifier)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 1000, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 267.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type HANConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (324.2s)  LR: 0.00060 Train Loss: 4.92  Valid Loss: 3.64  Valid NDCG: 0.4868\n",
      "Data Preparation: 5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type HANConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 (320.5s)  LR: 0.00069 Train Loss: 3.45  Valid Loss: 5.37  Valid NDCG: 0.4989\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 3 (311.7s)  LR: 0.00078 Train Loss: 2.50  Valid Loss: 3.39  Valid NDCG: 0.4902\n",
      "Data Preparation: 6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type HANConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 (321.4s)  LR: 0.00085 Train Loss: 2.38  Valid Loss: 2.82  Valid NDCG: 0.5331\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 5 (320.6s)  LR: 0.00091 Train Loss: 2.31  Valid Loss: 2.95  Valid NDCG: 0.4950\n",
      "0.5103756273482876\n",
      "Data Preparation: 26.3s\n",
      "Epoch: 6 (329.2s)  LR: 0.00096 Train Loss: 2.24  Valid Loss: 3.06  Valid NDCG: 0.4894\n",
      "Data Preparation: 6.0s\n",
      "Epoch: 7 (317.0s)  LR: 0.00099 Train Loss: 2.33  Valid Loss: 3.05  Valid NDCG: 0.4926\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 8 (305.7s)  LR: 0.00100 Train Loss: 2.28  Valid Loss: 3.02  Valid NDCG: 0.4745\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 9 (325.3s)  LR: 0.00099 Train Loss: 2.24  Valid Loss: 2.98  Valid NDCG: 0.4945\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 10 (311.8s)  LR: 0.00096 Train Loss: 2.33  Valid Loss: 3.14  Valid NDCG: 0.4960\n",
      "0.5480599308333363\n",
      "Data Preparation: 26.0s\n",
      "Epoch: 11 (303.4s)  LR: 0.00092 Train Loss: 2.29  Valid Loss: 2.95  Valid NDCG: 0.5100\n",
      "Data Preparation: 6.0s\n",
      "Epoch: 12 (313.0s)  LR: 0.00086 Train Loss: 2.24  Valid Loss: 2.97  Valid NDCG: 0.4978\n",
      "Data Preparation: 5.9s\n",
      "Epoch: 13 (313.6s)  LR: 0.00079 Train Loss: 2.25  Valid Loss: 3.37  Valid NDCG: 0.4076\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 14 (308.5s)  LR: 0.00070 Train Loss: 2.26  Valid Loss: 2.87  Valid NDCG: 0.5068\n",
      "Data Preparation: 6.6s\n",
      "Epoch: 15 (315.0s)  LR: 0.00061 Train Loss: 2.16  Valid Loss: 2.90  Valid NDCG: 0.5238\n",
      "0.5332493632665425\n",
      "Data Preparation: 25.9s\n",
      "Epoch: 16 (322.5s)  LR: 0.00051 Train Loss: 2.09  Valid Loss: 2.96  Valid NDCG: 0.4947\n",
      "Data Preparation: 5.8s\n",
      "Epoch: 17 (305.5s)  LR: 0.00042 Train Loss: 2.20  Valid Loss: 2.91  Valid NDCG: 0.4764\n",
      "Data Preparation: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type HANConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 (309.6s)  LR: 0.00032 Train Loss: 2.16  Valid Loss: 2.83  Valid NDCG: 0.5388\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 19 (315.6s)  LR: 0.00024 Train Loss: 2.13  Valid Loss: 2.78  Valid NDCG: 0.5306\n",
      "Data Preparation: 6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type HANConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 (319.0s)  LR: 0.00016 Train Loss: 2.03  Valid Loss: 2.88  Valid NDCG: 0.5437\n",
      "0.5405027767263532\n",
      "Data Preparation: 26.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type HANConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 (317.0s)  LR: 0.00009 Train Loss: 2.09  Valid Loss: 2.86  Valid NDCG: 0.5509\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 22 (313.3s)  LR: 0.00005 Train Loss: 2.13  Valid Loss: 3.15  Valid NDCG: 0.4535\n",
      "Data Preparation: 5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type HANConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 (306.8s)  LR: 0.00001 Train Loss: 2.15  Valid Loss: 2.75  Valid NDCG: 0.5515\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 24 (318.2s)  LR: 0.00000 Train Loss: 2.12  Valid Loss: 2.84  Valid NDCG: 0.5387\n",
      "Data Preparation: 6.3s\n",
      "Epoch: 25 (323.3s)  LR: 0.00001 Train Loss: 2.04  Valid Loss: 2.96  Valid NDCG: 0.5361\n",
      "0.5246873781574383\n",
      "Data Preparation: 26.1s\n",
      "Epoch: 26 (331.5s)  LR: 0.00003 Train Loss: 2.06  Valid Loss: 2.89  Valid NDCG: 0.5506\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 27 (314.5s)  LR: 0.00007 Train Loss: 2.07  Valid Loss: 2.99  Valid NDCG: 0.4831\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 28 (318.1s)  LR: 0.00013 Train Loss: 2.09  Valid Loss: 2.96  Valid NDCG: 0.4801\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 29 (313.1s)  LR: 0.00020 Train Loss: 2.05  Valid Loss: 2.85  Valid NDCG: 0.5344\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 30 (311.7s)  LR: 0.00029 Train Loss: 2.14  Valid Loss: 2.80  Valid NDCG: 0.5224\n",
      "0.5208962696103155\n",
      "Data Preparation: 25.9s\n",
      "Epoch: 31 (314.4s)  LR: 0.00038 Train Loss: 2.08  Valid Loss: 2.76  Valid NDCG: 0.5513\n",
      "Data Preparation: 6.0s\n",
      "Epoch: 32 (323.1s)  LR: 0.00048 Train Loss: 2.08  Valid Loss: 2.90  Valid NDCG: 0.4973\n",
      "Data Preparation: 6.3s\n",
      "Epoch: 33 (304.6s)  LR: 0.00057 Train Loss: 2.20  Valid Loss: 2.96  Valid NDCG: 0.4873\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 34 (326.2s)  LR: 0.00067 Train Loss: 2.02  Valid Loss: 2.89  Valid NDCG: 0.5138\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 35 (316.3s)  LR: 0.00075 Train Loss: 2.06  Valid Loss: 2.93  Valid NDCG: 0.5114\n",
      "0.5126313670890352\n",
      "Data Preparation: 25.7s\n",
      "Epoch: 36 (315.3s)  LR: 0.00083 Train Loss: 2.06  Valid Loss: 2.82  Valid NDCG: 0.5255\n",
      "Data Preparation: 6.0s\n",
      "Epoch: 37 (312.0s)  LR: 0.00090 Train Loss: 2.07  Valid Loss: 2.84  Valid NDCG: 0.4916\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 38 (318.1s)  LR: 0.00095 Train Loss: 2.06  Valid Loss: 2.89  Valid NDCG: 0.5119\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 39 (312.0s)  LR: 0.00098 Train Loss: 2.04  Valid Loss: 2.90  Valid NDCG: 0.4994\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 40 (319.8s)  LR: 0.00100 Train Loss: 2.07  Valid Loss: 2.93  Valid NDCG: 0.5139\n",
      "0.5482799264770586\n",
      "Data Preparation: 27.1s\n",
      "Epoch: 41 (311.9s)  LR: 0.00100 Train Loss: 2.19  Valid Loss: 2.97  Valid NDCG: 0.5295\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 42 (319.8s)  LR: 0.00097 Train Loss: 2.07  Valid Loss: 3.19  Valid NDCG: 0.4599\n",
      "Data Preparation: 6.0s\n",
      "Epoch: 43 (312.6s)  LR: 0.00093 Train Loss: 2.12  Valid Loss: 2.94  Valid NDCG: 0.4669\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 44 (316.1s)  LR: 0.00088 Train Loss: 1.99  Valid Loss: 3.01  Valid NDCG: 0.5303\n",
      "Data Preparation: 6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type HANConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45 (311.4s)  LR: 0.00081 Train Loss: 2.04  Valid Loss: 2.55  Valid NDCG: 0.5807\n",
      "0.5460981849239459\n",
      "Data Preparation: 25.5s\n",
      "Epoch: 46 (304.2s)  LR: 0.00072 Train Loss: 2.06  Valid Loss: 2.87  Valid NDCG: 0.5209\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 47 (324.5s)  LR: 0.00063 Train Loss: 1.92  Valid Loss: 3.00  Valid NDCG: 0.4744\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 48 (310.2s)  LR: 0.00054 Train Loss: 1.96  Valid Loss: 2.75  Valid NDCG: 0.5175\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 49 (299.1s)  LR: 0.00044 Train Loss: 2.05  Valid Loss: 2.77  Valid NDCG: 0.5117\n",
      "Data Preparation: 6.3s\n",
      "Epoch: 50 (309.4s)  LR: 0.00035 Train Loss: 1.95  Valid Loss: 2.93  Valid NDCG: 0.4536\n",
      "0.5695039294826387\n",
      "Data Preparation: 26.0s\n",
      "Epoch: 51 (316.8s)  LR: 0.00026 Train Loss: 1.97  Valid Loss: 2.64  Valid NDCG: 0.5211\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 52 (311.5s)  LR: 0.00018 Train Loss: 1.89  Valid Loss: 2.64  Valid NDCG: 0.5525\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 53 (321.6s)  LR: 0.00011 Train Loss: 1.90  Valid Loss: 2.68  Valid NDCG: 0.5230\n",
      "Data Preparation: 6.5s\n",
      "Epoch: 54 (309.2s)  LR: 0.00006 Train Loss: 1.91  Valid Loss: 2.60  Valid NDCG: 0.5635\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 55 (318.8s)  LR: 0.00002 Train Loss: 1.87  Valid Loss: 2.54  Valid NDCG: 0.5330\n",
      "0.5630807040118024\n",
      "Data Preparation: 26.0s\n",
      "Epoch: 56 (304.8s)  LR: 0.00000 Train Loss: 1.95  Valid Loss: 2.60  Valid NDCG: 0.5585\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 57 (312.8s)  LR: 0.00000 Train Loss: 1.88  Valid Loss: 2.78  Valid NDCG: 0.5524\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 58 (311.2s)  LR: 0.00002 Train Loss: 1.96  Valid Loss: 2.70  Valid NDCG: 0.5159\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 59 (325.5s)  LR: 0.00006 Train Loss: 1.85  Valid Loss: 3.58  Valid NDCG: 0.5059\n",
      "Data Preparation: 6.3s\n",
      "Epoch: 60 (317.4s)  LR: 0.00012 Train Loss: 1.89  Valid Loss: 2.85  Valid NDCG: 0.5188\n",
      "0.5319308962923586\n",
      "Data Preparation: 46.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type HANConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 61 (310.7s)  LR: 0.00018 Train Loss: 1.82  Valid Loss: 2.64  Valid NDCG: 0.5814\n",
      "Data Preparation: 6.0s\n",
      "Epoch: 62 (291.2s)  LR: 0.00027 Train Loss: 1.98  Valid Loss: 2.83  Valid NDCG: 0.5397\n",
      "Data Preparation: 6.0s\n",
      "Epoch: 63 (293.7s)  LR: 0.00036 Train Loss: 1.93  Valid Loss: 2.65  Valid NDCG: 0.5300\n",
      "Data Preparation: 5.9s\n",
      "Epoch: 64 (293.2s)  LR: 0.00045 Train Loss: 1.89  Valid Loss: 2.80  Valid NDCG: 0.5206\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 65 (289.3s)  LR: 0.00055 Train Loss: 1.97  Valid Loss: 3.46  Valid NDCG: 0.4521\n",
      "0.5619095818329054\n",
      "Data Preparation: 25.4s\n",
      "Epoch: 66 (291.9s)  LR: 0.00064 Train Loss: 1.92  Valid Loss: 2.74  Valid NDCG: 0.5566\n",
      "Data Preparation: 6.0s\n",
      "Epoch: 67 (296.3s)  LR: 0.00073 Train Loss: 1.95  Valid Loss: 3.14  Valid NDCG: 0.5439\n",
      "Data Preparation: 6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type HANConv. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 68 (297.1s)  LR: 0.00081 Train Loss: 1.86  Valid Loss: 2.58  Valid NDCG: 0.5822\n",
      "Data Preparation: 5.9s\n",
      "Epoch: 69 (294.6s)  LR: 0.00088 Train Loss: 1.93  Valid Loss: 2.99  Valid NDCG: 0.4978\n",
      "Data Preparation: 5.9s\n",
      "Epoch: 70 (292.7s)  LR: 0.00094 Train Loss: 1.92  Valid Loss: 2.92  Valid NDCG: 0.4718\n",
      "0.5426795047502705\n",
      "Data Preparation: 25.6s\n",
      "Epoch: 71 (291.3s)  LR: 0.00098 Train Loss: 2.01  Valid Loss: 2.96  Valid NDCG: 0.5559\n",
      "Data Preparation: 6.0s\n",
      "Epoch: 72 (288.5s)  LR: 0.00100 Train Loss: 1.93  Valid Loss: 2.72  Valid NDCG: 0.5606\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 73 (293.4s)  LR: 0.00100 Train Loss: 1.85  Valid Loss: 2.88  Valid NDCG: 0.5488\n",
      "Data Preparation: 6.0s\n",
      "Epoch: 74 (296.6s)  LR: 0.00098 Train Loss: 1.84  Valid Loss: 2.64  Valid NDCG: 0.5577\n",
      "Data Preparation: 6.0s\n",
      "Epoch: 75 (299.3s)  LR: 0.00095 Train Loss: 1.81  Valid Loss: 2.80  Valid NDCG: 0.5383\n",
      "0.5154148557714577\n",
      "Data Preparation: 25.7s\n",
      "Epoch: 76 (305.2s)  LR: 0.00089 Train Loss: 1.77  Valid Loss: 3.15  Valid NDCG: 0.4785\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 77 (303.9s)  LR: 0.00083 Train Loss: 1.79  Valid Loss: 2.74  Valid NDCG: 0.5152\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 78 (301.5s)  LR: 0.00075 Train Loss: 1.86  Valid Loss: 2.89  Valid NDCG: 0.5035\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 79 (312.4s)  LR: 0.00066 Train Loss: 1.68  Valid Loss: 2.83  Valid NDCG: 0.5420\n",
      "Data Preparation: 6.0s\n",
      "Epoch: 80 (298.3s)  LR: 0.00056 Train Loss: 1.75  Valid Loss: 2.85  Valid NDCG: 0.5108\n",
      "0.5764040866534945\n",
      "Data Preparation: 26.3s\n",
      "Epoch: 81 (302.1s)  LR: 0.00047 Train Loss: 1.78  Valid Loss: 2.91  Valid NDCG: 0.5004\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 82 (295.4s)  LR: 0.00037 Train Loss: 1.76  Valid Loss: 2.96  Valid NDCG: 0.5012\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 83 (290.0s)  LR: 0.00028 Train Loss: 1.79  Valid Loss: 2.85  Valid NDCG: 0.5042\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 84 (301.7s)  LR: 0.00020 Train Loss: 1.61  Valid Loss: 2.80  Valid NDCG: 0.5291\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 85 (304.8s)  LR: 0.00013 Train Loss: 1.64  Valid Loss: 3.02  Valid NDCG: 0.4954\n",
      "0.5753792353919991\n",
      "Data Preparation: 26.0s\n",
      "Epoch: 86 (306.4s)  LR: 0.00007 Train Loss: 1.59  Valid Loss: 2.88  Valid NDCG: 0.4555\n",
      "Data Preparation: 6.0s\n",
      "Epoch: 87 (292.1s)  LR: 0.00003 Train Loss: 1.72  Valid Loss: 3.07  Valid NDCG: 0.4655\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 88 (297.1s)  LR: 0.00001 Train Loss: 1.59  Valid Loss: 2.88  Valid NDCG: 0.4862\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 89 (300.6s)  LR: 0.00000 Train Loss: 1.67  Valid Loss: 3.03  Valid NDCG: 0.4382\n",
      "Data Preparation: 6.0s\n",
      "Epoch: 90 (305.1s)  LR: 0.00002 Train Loss: 1.62  Valid Loss: 2.87  Valid NDCG: 0.4972\n",
      "0.5483786586527372\n",
      "Data Preparation: 26.2s\n",
      "Epoch: 91 (298.7s)  LR: 0.00005 Train Loss: 1.69  Valid Loss: 2.93  Valid NDCG: 0.5665\n",
      "Data Preparation: 6.0s\n",
      "Epoch: 92 (287.5s)  LR: 0.00010 Train Loss: 1.82  Valid Loss: 3.24  Valid NDCG: 0.5240\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 93 (289.6s)  LR: 0.00017 Train Loss: 1.70  Valid Loss: 2.98  Valid NDCG: 0.4324\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 94 (312.7s)  LR: 0.00024 Train Loss: 1.58  Valid Loss: 2.91  Valid NDCG: 0.4742\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 95 (294.4s)  LR: 0.00033 Train Loss: 1.73  Valid Loss: 3.10  Valid NDCG: 0.4696\n",
      "0.5380495460285641\n",
      "Data Preparation: 26.2s\n",
      "Epoch: 96 (303.9s)  LR: 0.00043 Train Loss: 1.63  Valid Loss: 3.10  Valid NDCG: 0.4760\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 97 (289.7s)  LR: 0.00052 Train Loss: 1.79  Valid Loss: 3.07  Valid NDCG: 0.4688\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 98 (300.1s)  LR: 0.00062 Train Loss: 1.79  Valid Loss: 3.06  Valid NDCG: 0.4937\n",
      "Data Preparation: 6.3s\n",
      "Epoch: 99 (305.4s)  LR: 0.00071 Train Loss: 1.65  Valid Loss: 2.76  Valid NDCG: 0.4900\n",
      "Data Preparation: 6.3s\n",
      "Epoch: 100 (301.0s)  LR: 0.00079 Train Loss: 1.69  Valid Loss: 3.18  Valid NDCG: 0.4510\n",
      "0.532299447470027\n",
      "Data Preparation: 25.9s\n",
      "Epoch: 101 (304.4s)  LR: 0.00087 Train Loss: 1.74  Valid Loss: 2.88  Valid NDCG: 0.5589\n",
      "Data Preparation: 6.3s\n",
      "Epoch: 102 (305.6s)  LR: 0.00093 Train Loss: 1.62  Valid Loss: 2.86  Valid NDCG: 0.5155\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 103 (303.8s)  LR: 0.00097 Train Loss: 1.65  Valid Loss: 3.06  Valid NDCG: 0.4256\n",
      "Data Preparation: 6.0s\n",
      "Epoch: 104 (290.2s)  LR: 0.00099 Train Loss: 1.81  Valid Loss: 2.57  Valid NDCG: 0.5758\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 105 (306.0s)  LR: 0.00100 Train Loss: 1.66  Valid Loss: 3.12  Valid NDCG: 0.4778\n",
      "0.5329169392104887\n",
      "Data Preparation: 25.7s\n",
      "Epoch: 106 (293.9s)  LR: 0.00099 Train Loss: 1.73  Valid Loss: 2.78  Valid NDCG: 0.5298\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 107 (290.1s)  LR: 0.00096 Train Loss: 1.78  Valid Loss: 2.98  Valid NDCG: 0.5287\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 108 (301.5s)  LR: 0.00091 Train Loss: 1.60  Valid Loss: 2.89  Valid NDCG: 0.5280\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 109 (306.1s)  LR: 0.00084 Train Loss: 1.67  Valid Loss: 2.79  Valid NDCG: 0.5358\n",
      "Data Preparation: 6.3s\n",
      "Epoch: 110 (304.3s)  LR: 0.00077 Train Loss: 1.67  Valid Loss: 3.03  Valid NDCG: 0.5089\n",
      "0.5150875281626692\n",
      "Data Preparation: 25.6s\n",
      "Epoch: 111 (292.8s)  LR: 0.00068 Train Loss: 1.76  Valid Loss: 3.00  Valid NDCG: 0.5017\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 112 (294.3s)  LR: 0.00059 Train Loss: 1.67  Valid Loss: 2.87  Valid NDCG: 0.5559\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 113 (295.9s)  LR: 0.00049 Train Loss: 1.60  Valid Loss: 2.87  Valid NDCG: 0.5170\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 114 (301.1s)  LR: 0.00039 Train Loss: 1.66  Valid Loss: 2.87  Valid NDCG: 0.5181\n",
      "Data Preparation: 6.3s\n",
      "Epoch: 115 (304.4s)  LR: 0.00030 Train Loss: 1.62  Valid Loss: 2.79  Valid NDCG: 0.5330\n",
      "0.5353772436562783\n",
      "Data Preparation: 26.6s\n",
      "Epoch: 116 (292.5s)  LR: 0.00022 Train Loss: 1.74  Valid Loss: 2.87  Valid NDCG: 0.5098\n",
      "Data Preparation: 6.5s\n",
      "Epoch: 117 (298.9s)  LR: 0.00014 Train Loss: 1.60  Valid Loss: 3.05  Valid NDCG: 0.5090\n",
      "Data Preparation: 6.2s\n",
      "Epoch: 118 (314.5s)  LR: 0.00008 Train Loss: 1.52  Valid Loss: 3.13  Valid NDCG: 0.4910\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 119 (303.2s)  LR: 0.00004 Train Loss: 1.62  Valid Loss: 2.81  Valid NDCG: 0.5387\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 120 (311.6s)  LR: 0.00001 Train Loss: 1.46  Valid Loss: 3.60  Valid NDCG: 0.4447\n",
      "0.5175297167156361\n",
      "Data Preparation: 26.0s\n",
      "Epoch: 121 (298.3s)  LR: 0.00000 Train Loss: 1.65  Valid Loss: 3.43  Valid NDCG: 0.4395\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 122 (299.2s)  LR: 0.00001 Train Loss: 1.59  Valid Loss: 2.91  Valid NDCG: 0.5350\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 123 (305.7s)  LR: 0.00004 Train Loss: 1.59  Valid Loss: 2.99  Valid NDCG: 0.5298\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 124 (297.0s)  LR: 0.00009 Train Loss: 1.56  Valid Loss: 3.05  Valid NDCG: 0.5138\n",
      "Data Preparation: 6.3s\n",
      "Epoch: 125 (304.4s)  LR: 0.00015 Train Loss: 1.58  Valid Loss: 2.83  Valid NDCG: 0.5183\n",
      "0.6045509809866902\n",
      "Data Preparation: 26.0s\n",
      "Epoch: 126 (302.7s)  LR: 0.00022 Train Loss: 1.53  Valid Loss: 2.73  Valid NDCG: 0.5576\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 127 (307.5s)  LR: 0.00031 Train Loss: 1.50  Valid Loss: 2.92  Valid NDCG: 0.5011\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 128 (300.0s)  LR: 0.00040 Train Loss: 1.62  Valid Loss: 3.05  Valid NDCG: 0.4953\n",
      "Data Preparation: 6.5s\n",
      "Epoch: 129 (295.9s)  LR: 0.00050 Train Loss: 1.68  Valid Loss: 3.04  Valid NDCG: 0.4860\n",
      "Data Preparation: 6.3s\n",
      "Epoch: 130 (301.3s)  LR: 0.00059 Train Loss: 1.61  Valid Loss: 3.40  Valid NDCG: 0.4488\n",
      "0.5451609755931828\n",
      "Data Preparation: 26.8s\n",
      "Epoch: 131 (300.7s)  LR: 0.00069 Train Loss: 1.62  Valid Loss: 2.75  Valid NDCG: 0.5543\n",
      "Data Preparation: 6.5s\n",
      "Epoch: 132 (294.0s)  LR: 0.00077 Train Loss: 1.72  Valid Loss: 3.61  Valid NDCG: 0.4705\n",
      "Data Preparation: 6.3s\n",
      "Epoch: 133 (303.1s)  LR: 0.00085 Train Loss: 1.63  Valid Loss: 2.88  Valid NDCG: 0.5113\n",
      "Data Preparation: 6.5s\n",
      "Epoch: 134 (294.5s)  LR: 0.00091 Train Loss: 1.63  Valid Loss: 2.89  Valid NDCG: 0.5081\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 135 (295.2s)  LR: 0.00096 Train Loss: 1.56  Valid Loss: 3.04  Valid NDCG: 0.4803\n",
      "0.5702987970378468\n",
      "Data Preparation: 26.1s\n",
      "Epoch: 136 (299.5s)  LR: 0.00099 Train Loss: 1.66  Valid Loss: 3.34  Valid NDCG: 0.5144\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 137 (310.9s)  LR: 0.00100 Train Loss: 1.48  Valid Loss: 2.87  Valid NDCG: 0.5336\n",
      "Data Preparation: 6.5s\n",
      "Epoch: 138 (299.0s)  LR: 0.00099 Train Loss: 1.69  Valid Loss: 2.92  Valid NDCG: 0.5334\n",
      "Data Preparation: 7.0s\n",
      "Epoch: 139 (293.3s)  LR: 0.00097 Train Loss: 1.61  Valid Loss: 2.89  Valid NDCG: 0.4935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 6.4s\n",
      "Epoch: 140 (292.8s)  LR: 0.00092 Train Loss: 1.65  Valid Loss: 2.80  Valid NDCG: 0.5281\n",
      "0.5366936586141382\n",
      "Data Preparation: 26.6s\n",
      "Epoch: 141 (300.5s)  LR: 0.00086 Train Loss: 1.57  Valid Loss: 2.80  Valid NDCG: 0.5322\n",
      "Data Preparation: 6.3s\n",
      "Epoch: 142 (300.1s)  LR: 0.00079 Train Loss: 1.58  Valid Loss: 2.98  Valid NDCG: 0.5025\n",
      "Data Preparation: 6.1s\n",
      "Epoch: 143 (305.7s)  LR: 0.00070 Train Loss: 1.58  Valid Loss: 2.86  Valid NDCG: 0.5514\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 144 (296.4s)  LR: 0.00061 Train Loss: 1.57  Valid Loss: 2.89  Valid NDCG: 0.5134\n",
      "Data Preparation: 6.5s\n",
      "Epoch: 145 (299.8s)  LR: 0.00052 Train Loss: 1.47  Valid Loss: 2.93  Valid NDCG: 0.5416\n",
      "0.5690174800342487\n",
      "Data Preparation: 26.3s\n",
      "Epoch: 146 (287.8s)  LR: 0.00042 Train Loss: 1.57  Valid Loss: 2.79  Valid NDCG: 0.5389\n",
      "Data Preparation: 6.4s\n",
      "Epoch: 147 (305.5s)  LR: 0.00033 Train Loss: 1.52  Valid Loss: 2.82  Valid NDCG: 0.5593\n",
      "Data Preparation: 6.5s\n",
      "Epoch: 148 (299.6s)  LR: 0.00024 Train Loss: 1.52  Valid Loss: 2.71  Valid NDCG: 0.5331\n",
      "Data Preparation: 6.3s\n",
      "Epoch: 149 (299.4s)  LR: 0.00016 Train Loss: 1.53  Valid Loss: 2.76  Valid NDCG: 0.5606\n",
      "Data Preparation: 6.3s\n",
      "Epoch: 150 (303.0s)  LR: 0.00010 Train Loss: 1.49  Valid Loss: 2.69  Valid NDCG: 0.5650\n",
      "0.5859440501536182\n",
      "Data Preparation: 26.0s\n",
      "Epoch: 151 (291.7s)  LR: 0.00005 Train Loss: 1.52  Valid Loss: 2.93  Valid NDCG: 0.5046\n",
      "Data Preparation: 6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-618:\n",
      "Process ForkPoolWorker-617:\n",
      "Process ForkPoolWorker-619:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f4fa326c8ce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mtrain_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-620:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-0e32fbada801>\", line 18, in pf_sample\n",
      "    inp = {'paper': pids, 'field': fids}, sampled_depth = 3, sampled_number = 100)\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"../utils.py\", line 294, in sample_subgraph\n",
      "    if decode(source_key)[0] in tesrt:\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-4-0e32fbada801>\", line 18, in pf_sample\n",
      "    inp = {'paper': pids, 'field': fids}, sampled_depth = 3, sampled_number = 100)\n",
      "  File \"<ipython-input-4-0e32fbada801>\", line 18, in pf_sample\n",
      "    inp = {'paper': pids, 'field': fids}, sampled_depth = 3, sampled_number = 100)\n",
      "  File \"../utils.py\", line 185, in decode\n",
      "    return np.array([s[:idx], s[idx+1:]], dtype=float)\n",
      "  File \"<ipython-input-4-0e32fbada801>\", line 18, in pf_sample\n",
      "    inp = {'paper': pids, 'field': fids}, sampled_depth = 3, sampled_number = 100)\n",
      "  File \"../utils.py\", line 294, in sample_subgraph\n",
      "    if decode(source_key)[0] in tesrt:\n",
      "  File \"../utils.py\", line 294, in sample_subgraph\n",
      "    if decode(source_key)[0] in tesrt:\n",
      "KeyboardInterrupt\n",
      "  File \"../utils.py\", line 294, in sample_subgraph\n",
      "    if decode(source_key)[0] in tesrt:\n",
      "  File \"../utils.py\", line 185, in decode\n",
      "    return np.array([s[:idx], s[idx+1:]], dtype=float)\n",
      "KeyboardInterrupt\n",
      "  File \"../utils.py\", line 185, in decode\n",
      "    return np.array([s[:idx], s[idx+1:]], dtype=float)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "pool = mp.Pool(4)\n",
    "process_ids = np.arange(batch_num // 4)\n",
    "st = time.time()\n",
    "jobs = prepare_data(pool, process_ids)\n",
    "train_step = 1500\n",
    "best_val   = 0\n",
    "res = []\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "for epoch in np.arange(epoch_num)+1:\n",
    "    '''\n",
    "        Prepare Training and Validation Data\n",
    "    '''\n",
    "    train_data = [job.get() for job in jobs[:-1]]\n",
    "    valid_data = jobs[-1].get()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    pool = mp.Pool(4)\n",
    "    jobs = prepare_data(pool, process_ids)\n",
    "    et = time.time()\n",
    "    print('Data Preparation: %.1fs' % (et - st))\n",
    "    \n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    torch.cuda.empty_cache()\n",
    "    for batch in np.arange(2):\n",
    "        for node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel in train_data:\n",
    "            node_rep = gnn.forward(node_feature.to(device), node_type.to(device), edge_index.to(device), edge_type.to(device))\n",
    "            res  = classifier.forward(node_rep[paper_ids])\n",
    "            loss = criterion(res, ylabel.to(device))\n",
    "            optimizer.zero_grad() \n",
    "            torch.cuda.empty_cache()\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.2)\n",
    "            optimizer.step()\n",
    "            train_losses += [loss.cpu().detach().tolist()]\n",
    "            train_step += 1\n",
    "            scheduler.step(train_step)\n",
    "            del res, loss\n",
    "    '''\n",
    "        Valid\n",
    "    '''\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel = valid_data\n",
    "        node_rep = gnn.forward(node_feature.to(device), node_type.to(device), edge_index.to(device), edge_type.to(device))\n",
    "        res  = classifier.forward(node_rep[paper_ids])\n",
    "        loss = criterion(res, ylabel.to(device))\n",
    "        valid_res = []\n",
    "\n",
    "        for ai, bi in zip(ylabel, res.argsort(descending = True)):\n",
    "            valid_res += [ai[bi].tolist()]\n",
    "        valid_ndcg = np.average([ndcg_at_k(resi, len(resi)) for resi in valid_res])\n",
    "        if valid_ndcg > best_val:\n",
    "            best_val = valid_ndcg\n",
    "            torch.save(model, './save/rgt_1.pt')\n",
    "        st = time.time()\n",
    "        print((\"Epoch: %d (%.1fs)  LR: %.5f Train Loss: %.2f  Valid Loss: %.2f  Valid NDCG: %.4f\") % \\\n",
    "              (epoch, (st-et), optimizer.param_groups[0]['lr'], np.average(train_losses), loss.cpu().detach().tolist(),\\\n",
    "              valid_ndcg))\n",
    "        stats += [[np.average(train_losses), loss.cpu().detach().tolist()]]\n",
    "        del res, loss\n",
    "        if epoch % 5 == 0:\n",
    "            '''\n",
    "                Test\n",
    "            '''\n",
    "            _time = np.random.choice(list(test_papers.keys()))\n",
    "            node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel = pf_sample(np.random.randint(2 ** 32 - 1), test_papers, \\\n",
    "                                                           test_pairs, test_range, batch_size, test=True)\n",
    "            paper_rep = gnn.forward(node_feature.to(device), node_type.to(device), edge_index.to(device), edge_type.to(device))[paper_ids]\n",
    "            res  = classifier.forward(paper_rep)\n",
    "            test_res = []\n",
    "            for ai, bi in zip(ylabel, res.argsort(descending = True)):\n",
    "                test_res += [ai[bi].tolist()]\n",
    "            test_ndcg = np.average([ndcg_at_k(resi, len(resi)) for resi in test_res])\n",
    "            print(test_ndcg)\n",
    "            del res\n",
    "    del train_data, valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = np.array(stats)\n",
    "plt.plot(stats[:,0])\n",
    "plt.plot(stats[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "gnn, classifier = model\n",
    "with torch.no_grad():\n",
    "    test_res = []\n",
    "    for _ in range(10):\n",
    "        _time = np.random.choice(list(test_papers.keys()))\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel = pf_sample(np.random.randint(2 ** 32 - 1), test_papers, \\\n",
    "                                                       test_pairs, test_range, batch_size, test=True)\n",
    "        paper_rep = gnn.forward(node_feature.to(device), node_type.to(device), edge_index.to(device))[paper_ids]\n",
    "        res = classifier.forward(paper_rep)\n",
    "        for ai, bi in zip(ylabel, res.argsort(descending = True)):\n",
    "            test_res += [ai[bi].tolist()]\n",
    "    test_ndcg = [ndcg_at_k(resi, len(resi)) for resi in test_res]\n",
    "    print(np.average(test_ndcg), np.var(test_ndcg))\n",
    "    test_mrr = mean_reciprocal_rank(test_res)\n",
    "    print(np.average(test_mrr), np.var(test_mrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('./save/rgt_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "gnn, classifier = best_model\n",
    "with torch.no_grad():\n",
    "    test_res = []\n",
    "    for _ in range(10):\n",
    "        _time = np.random.choice(list(test_papers.keys()))\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel = pf_sample(np.random.randint(2 ** 32 - 1), test_papers, \\\n",
    "                                                       test_pairs, test_range, batch_size, test=True)\n",
    "        paper_rep = gnn.forward(node_feature.to(device), node_type.to(device), edge_index.to(device))[paper_ids]\n",
    "        res = classifier.forward(paper_rep)\n",
    "        for ai, bi in zip(ylabel, res.argsort(descending = True)):\n",
    "            test_res += [ai[bi].tolist()]\n",
    "    test_ndcg = [ndcg_at_k(resi, len(resi)) for resi in test_res]\n",
    "    print(np.average(test_ndcg), np.var(test_ndcg))\n",
    "    test_mrr = mean_reciprocal_rank(test_res)\n",
    "    print(np.average(test_mrr), np.var(test_mrr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
