{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/datadrive/data_cs/'\n",
    "batch_size = 64\n",
    "batch_num  = 128\n",
    "epoch_num  = 100\n",
    "samp_num   = 3\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "graph = dill.load(open(data_dir + 'graph.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = {t: True for t in graph.times if t != None and t < 2015}\n",
    "valid_range = {t: True for t in graph.times if t != None and t >= 2015  and t <= 2016}\n",
    "test_range  = {t: True for t in graph.times if t != None and t > 2016}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c1bcc166f84e8aa039fc6cb0794c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=510189), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "name_count = defaultdict(lambda: [])\n",
    "for i, j in tqdm(graph.node_feature['author'].iterrows(), total = len(graph.node_feature['author'])):\n",
    "    name_count[j['name']] += [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author Disambiguation\n",
    "'''\n",
    "train_pairs = {}\n",
    "valid_pairs = {}\n",
    "test_pairs  = {}\n",
    "\n",
    "train_papers = {_time: {} for _time in train_range}\n",
    "valid_papers = {_time: {} for _time in valid_range}\n",
    "test_papers  = {_time: {} for _time in test_range}\n",
    "\n",
    "for name in name_count:\n",
    "    same_name_author_list = np.array(name_count[name])\n",
    "    if len(same_name_author_list) > 4:\n",
    "        for author_id, author in enumerate(same_name_author_list):\n",
    "            pem_ids = list(range(len(same_name_author_list)))\n",
    "            pem_ids.remove(author_id)\n",
    "            for p_id in graph.edge_list['author']['paper']['rev_AP_write_first'][author]:\n",
    "                _time = graph.edge_list['author']['paper']['rev_AP_write_first'][author][p_id]\n",
    "                if type(_time) != int:\n",
    "                    continue\n",
    "                al = same_name_author_list[np.array([author_id] + pem_ids)]\n",
    "                if _time in train_range:\n",
    "                    if p_id not in train_pairs:\n",
    "                        train_pairs[p_id] = []\n",
    "                    train_pairs[p_id] += [al]\n",
    "                    train_papers[_time][p_id] = True\n",
    "                elif _time in valid_range:\n",
    "                    if p_id not in valid_pairs:\n",
    "                        valid_pairs[p_id] = []\n",
    "                    valid_pairs[p_id] += [al]\n",
    "                    valid_papers[_time][p_id] = True\n",
    "                else:\n",
    "                    if p_id not in test_pairs:\n",
    "                        test_pairs[p_id] = []\n",
    "                    test_pairs[p_id] += [al]\n",
    "                    test_papers[_time][p_id] = True\n",
    "train_papers = {k:list(train_papers[k].keys()) for k in train_papers if len(train_papers[k]) >= batch_size}\n",
    "valid_papers = {k:list(valid_papers[k].keys()) for k in valid_papers if len(valid_papers[k]) >= batch_size}\n",
    "test_papers  = {k:list(test_papers[k].keys()) for k in test_papers if len(test_papers[k]) >= batch_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ad_sample(seed, papers, pairs, t_range, batch_size, test = False):\n",
    "    np.random.seed(seed)\n",
    "    _time = np.random.choice(list(papers.keys()))\n",
    "    pids = np.array(papers[_time])[np.random.choice(len(papers[_time]), batch_size, replace = False)]\n",
    "    aids = []\n",
    "    edge = defaultdict(lambda: {})\n",
    "    eals = []\n",
    "    for x_id, p_id in enumerate(pids):\n",
    "        als = pairs[p_id]\n",
    "        for al in als:\n",
    "            for a_id in al:\n",
    "                if a_id not in aids:\n",
    "                    aids += [a_id]\n",
    "            eal = [aids.index(al[0])]\n",
    "            edge[x_id][eal[-1]] = 1\n",
    "            for a_id in al[1:]:\n",
    "                eal += [aids.index(a_id)]\n",
    "                edge[x_id][eal[-1]] = 0\n",
    "            eals += [[x_id, eal]]\n",
    "    pids = np.stack([pids, np.repeat([_time], batch_size)]).T\n",
    "    aids = np.stack([aids, np.repeat([_time], len(aids))]).T\n",
    " \n",
    "    feature, times, edge_list, _ = sample_subgraph(graph, t_range, \\\n",
    "                inp = {'paper': pids, 'author': aids}, sampled_depth = 3, sampled_number = 100)\n",
    "\n",
    "    el = []\n",
    "    for i in edge_list['paper']['author']['AP_write_first']:\n",
    "        if i[0] in edge and i[1] in edge[i[0]]:\n",
    "            continue\n",
    "        el += [i]\n",
    "    edge_list['paper']['author']['AP_write_first'] = el\n",
    "\n",
    "    el = []\n",
    "    for i in edge_list['author']['paper']['rev_AP_write_first']:\n",
    "        if i[1] in edge and i[0] in edge[i[1]]:\n",
    "            continue\n",
    "        el += [i]\n",
    "    edge_list['author']['paper']['rev_AP_write_first'] = el\n",
    "    \n",
    "    \n",
    "    node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict = \\\n",
    "            to_torch(feature, times, edge_list, graph)\n",
    "    '''\n",
    "        Trace the paper_id and field_id by its own index plus the type start index\n",
    "    '''\n",
    "    paper_ids = np.arange(len(pids)) + node_dict['paper'][0]\n",
    "    author_ids = np.arange(len(aids)) + node_dict['author'][0]\n",
    "    ylabel = {}\n",
    "    for x_id, eal in eals:\n",
    "        ylabel[x_id + node_dict['paper'][0]] = np.array(eal) + node_dict['author'][0]\n",
    "    return node_feature, node_type, edge_time, edge_index, edge_type, author_ids, paper_ids, ylabel\n",
    "    \n",
    "def prepare_data(pool, process_ids):\n",
    "    jobs = []\n",
    "    for process_id in process_ids[:-1]:\n",
    "        p = pool.apply_async(ad_sample, args=(np.random.randint(2**32 - 1), train_papers, \\\n",
    "                                               train_pairs, train_range, batch_size))\n",
    "        jobs.append(p)\n",
    "    p = pool.apply_async(ad_sample, args=(np.random.randint(2**32 - 1), valid_papers, \\\n",
    "                                           valid_pairs, valid_range, batch_size))\n",
    "    jobs.append(p)\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_dim, n_hid, num_types, num_relations, n_heads, n_layers, dropout = 0.3):\n",
    "        super(GNN, self).__init__()\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.num_types = num_types\n",
    "        self.in_dim    = in_dim\n",
    "        self.n_hid     = n_hid\n",
    "        self.aggregat_ws   = nn.ModuleList()\n",
    "        self.drop          = nn.Dropout(dropout)\n",
    "        for t in range(num_types):\n",
    "            self.aggregat_ws.append(nn.Linear(in_dim, n_hid))\n",
    "        for l in range(n_layers):\n",
    "            self.gcs.append(RAGCNConv(n_hid, n_hid, num_types, num_relations, n_heads, dropout))\n",
    "    def set_device(self, device):\n",
    "        self.device = device\n",
    "        for gc in self.gcs:\n",
    "            gc.device = device\n",
    "    def forward(self, node_feature, node_type, edge_time, edge_index, edge_type):\n",
    "        res = torch.zeros(node_feature.size(0), self.n_hid).to(node_feature.device)\n",
    "        for t_id in range(self.num_types):\n",
    "            aggregat_w = self.aggregat_ws[t_id]\n",
    "            idx = (node_type == t_id)\n",
    "            if idx.sum() == 0:\n",
    "                continue\n",
    "            res[idx] = torch.tanh(aggregat_w(node_feature[idx]))\n",
    "        meta_xs = self.drop(res)\n",
    "        del res\n",
    "        for gc in self.gcs:\n",
    "            meta_xs = gc(meta_xs, node_type, edge_index, edge_type, edge_time)\n",
    "        return meta_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = graph.get_types()\n",
    "gnn = GNN(in_dim = len(graph.node_feature['paper']['emb'][0]) + 401, n_hid = 256, num_types = len(types), \\\n",
    "          num_relations = len(graph.get_meta_graph()) + 1, n_heads = 8, n_layers = 3).to(device)\n",
    "matcher = Matcher(256, n_heads = 8).to(device)\n",
    "model = nn.Sequential(gnn, matcher)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 1000, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_softmax(pred, size):\n",
    "    loss = 0\n",
    "    stx = 0\n",
    "    for l in size:\n",
    "        loss += torch.log_softmax(pred[stx: stx + l], dim=-1)[0] / l\n",
    "        stx += l\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 272.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 (642.9s)  LR: 0.00069 Train Loss: 25.74  Valid Loss: 8.57  Valid NDCG: 0.7529\n",
      "Data Preparation: 2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 (670.7s)  LR: 0.00085 Train Loss: 12.01  Valid Loss: 6.40  Valid NDCG: 0.8881\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 3 (671.2s)  LR: 0.00096 Train Loss: 5.18  Valid Loss: 3.98  Valid NDCG: 0.8709\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 4 (673.6s)  LR: 0.00100 Train Loss: 4.05  Valid Loss: 3.13  Valid NDCG: 0.8695\n",
      "Data Preparation: 2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 (671.0s)  LR: 0.00096 Train Loss: 3.74  Valid Loss: 3.37  Valid NDCG: 0.9019\n",
      "Data Preparation: 2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 (664.1s)  LR: 0.00086 Train Loss: 3.52  Valid Loss: 2.71  Valid NDCG: 0.9366\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 7 (663.8s)  LR: 0.00070 Train Loss: 3.49  Valid Loss: 5.29  Valid NDCG: 0.8908\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 8 (665.4s)  LR: 0.00051 Train Loss: 3.37  Valid Loss: 2.59  Valid NDCG: 0.9273\n",
      "Data Preparation: 2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 (672.6s)  LR: 0.00032 Train Loss: 3.25  Valid Loss: 2.89  Valid NDCG: 0.9731\n",
      "Data Preparation: 2.6s\n",
      "Epoch: 10 (672.8s)  LR: 0.00016 Train Loss: 3.03  Valid Loss: 2.61  Valid NDCG: 0.9225\n",
      "Data Preparation: 3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 (661.5s)  LR: 0.00004 Train Loss: 2.82  Valid Loss: 2.06  Valid NDCG: 0.9786\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 12 (621.1s)  LR: 0.00000 Train Loss: 2.60  Valid Loss: 2.63  Valid NDCG: 0.9219\n",
      "Data Preparation: 2.7s\n",
      "Epoch: 13 (618.5s)  LR: 0.00003 Train Loss: 2.66  Valid Loss: 3.71  Valid NDCG: 0.9174\n",
      "Data Preparation: 2.7s\n",
      "Epoch: 14 (611.5s)  LR: 0.00013 Train Loss: 2.71  Valid Loss: 2.01  Valid NDCG: 0.9462\n",
      "Data Preparation: 2.7s\n",
      "Epoch: 15 (610.3s)  LR: 0.00029 Train Loss: 2.65  Valid Loss: 2.22  Valid NDCG: 0.9295\n",
      "Data Preparation: 2.7s\n",
      "Epoch: 16 (615.7s)  LR: 0.00047 Train Loss: 2.60  Valid Loss: 2.60  Valid NDCG: 0.9652\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 17 (611.9s)  LR: 0.00067 Train Loss: 2.87  Valid Loss: 2.42  Valid NDCG: 0.9679\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 18 (615.2s)  LR: 0.00083 Train Loss: 3.21  Valid Loss: 1.91  Valid NDCG: 0.9610\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 19 (613.8s)  LR: 0.00095 Train Loss: 2.64  Valid Loss: 2.44  Valid NDCG: 0.9457\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 20 (613.9s)  LR: 0.00100 Train Loss: 3.09  Valid Loss: 3.10  Valid NDCG: 0.9229\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 21 (639.3s)  LR: 0.00097 Train Loss: 2.92  Valid Loss: 3.39  Valid NDCG: 0.9413\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 22 (635.8s)  LR: 0.00088 Train Loss: 2.91  Valid Loss: 2.84  Valid NDCG: 0.9628\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 23 (620.6s)  LR: 0.00072 Train Loss: 3.02  Valid Loss: 2.84  Valid NDCG: 0.9319\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 24 (633.9s)  LR: 0.00054 Train Loss: 2.83  Valid Loss: 2.13  Valid NDCG: 0.9504\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 25 (631.7s)  LR: 0.00035 Train Loss: 2.48  Valid Loss: 2.56  Valid NDCG: 0.9439\n",
      "Data Preparation: 3.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ziniu/anaconda3/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type GNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 (628.8s)  LR: 0.00018 Train Loss: 2.34  Valid Loss: 1.96  Valid NDCG: 0.9922\n",
      "Data Preparation: 2.8s\n",
      "Epoch: 27 (630.7s)  LR: 0.00006 Train Loss: 2.57  Valid Loss: 2.56  Valid NDCG: 0.9204\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 28 (624.7s)  LR: 0.00000 Train Loss: 2.53  Valid Loss: 2.23  Valid NDCG: 0.9571\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 29 (617.7s)  LR: 0.00002 Train Loss: 2.52  Valid Loss: 2.28  Valid NDCG: 0.9634\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 30 (621.6s)  LR: 0.00011 Train Loss: 2.10  Valid Loss: 2.80  Valid NDCG: 0.9864\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 31 (628.5s)  LR: 0.00026 Train Loss: 2.40  Valid Loss: 2.67  Valid NDCG: 0.9501\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 32 (625.7s)  LR: 0.00045 Train Loss: 2.45  Valid Loss: 1.78  Valid NDCG: 0.9452\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 33 (628.5s)  LR: 0.00064 Train Loss: 2.24  Valid Loss: 1.81  Valid NDCG: 0.9749\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 34 (631.0s)  LR: 0.00081 Train Loss: 2.33  Valid Loss: 1.43  Valid NDCG: 0.9731\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 35 (628.8s)  LR: 0.00094 Train Loss: 2.41  Valid Loss: 2.38  Valid NDCG: 0.9405\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 36 (619.5s)  LR: 0.00100 Train Loss: 3.01  Valid Loss: 3.15  Valid NDCG: 0.9533\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 37 (624.7s)  LR: 0.00098 Train Loss: 2.99  Valid Loss: 2.65  Valid NDCG: 0.9229\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 38 (623.3s)  LR: 0.00089 Train Loss: 3.06  Valid Loss: 3.31  Valid NDCG: 0.9593\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 39 (622.0s)  LR: 0.00075 Train Loss: 2.86  Valid Loss: 3.53  Valid NDCG: 0.9044\n",
      "Data Preparation: 3.3s\n",
      "Epoch: 40 (612.2s)  LR: 0.00056 Train Loss: 2.62  Valid Loss: 3.73  Valid NDCG: 0.8896\n",
      "Data Preparation: 3.4s\n",
      "Epoch: 41 (619.1s)  LR: 0.00037 Train Loss: 2.50  Valid Loss: 3.61  Valid NDCG: 0.9398\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 42 (608.1s)  LR: 0.00020 Train Loss: 2.63  Valid Loss: 3.09  Valid NDCG: 0.9439\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 43 (614.2s)  LR: 0.00007 Train Loss: 2.71  Valid Loss: 2.63  Valid NDCG: 0.9413\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 44 (620.4s)  LR: 0.00000 Train Loss: 2.66  Valid Loss: 3.40  Valid NDCG: 0.8942\n",
      "Data Preparation: 2.9s\n",
      "Epoch: 45 (632.2s)  LR: 0.00002 Train Loss: 2.50  Valid Loss: 3.25  Valid NDCG: 0.9025\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 46 (616.1s)  LR: 0.00010 Train Loss: 2.26  Valid Loss: 1.38  Valid NDCG: 0.9768\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 47 (612.7s)  LR: 0.00024 Train Loss: 2.41  Valid Loss: 2.64  Valid NDCG: 0.9355\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 48 (613.1s)  LR: 0.00042 Train Loss: 2.36  Valid Loss: 3.12  Valid NDCG: 0.9300\n",
      "Data Preparation: 3.2s\n",
      "Epoch: 49 (621.3s)  LR: 0.00062 Train Loss: 2.17  Valid Loss: 3.04  Valid NDCG: 0.9526\n",
      "Data Preparation: 3.1s\n",
      "Epoch: 50 (613.4s)  LR: 0.00079 Train Loss: 2.22  Valid Loss: 2.70  Valid NDCG: 0.9478\n",
      "Data Preparation: 3.0s\n",
      "Epoch: 51 (623.9s)  LR: 0.00093 Train Loss: 2.46  Valid Loss: 2.26  Valid NDCG: 0.9601\n",
      "Data Preparation: 2.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-209:\n",
      "Process ForkPoolWorker-210:\n",
      "Process ForkPoolWorker-211:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Process ForkPoolWorker-212:\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-6-2fb815af380e>\", line 24, in ad_sample\n",
      "    inp = {'paper': pids, 'author': aids}, sampled_depth = 3, sampled_number = 100)\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"../utils.py\", line 294, in sample_subgraph\n",
      "    if decode(source_key)[0] in tesrt:\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-6-2fb815af380e>\", line 24, in ad_sample\n",
      "    inp = {'paper': pids, 'author': aids}, sampled_depth = 3, sampled_number = 100)\n",
      "  File \"../utils.py\", line 185, in decode\n",
      "    return np.array([s[:idx], s[idx+1:]], dtype=float)\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"../utils.py\", line 248, in sample_subgraph\n",
      "    add_budget(te, int(source_id), int(source_time), layer_data, budget)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"../utils.py\", line 177, in add_budget\n",
      "    budget[source_type][k] += 1. / len(sampled_ids)\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"../utils.py\", line 142, in <lambda>\n",
      "    lambda: 0. #sampled_score\n",
      "  File \"<ipython-input-6-2fb815af380e>\", line 24, in ad_sample\n",
      "    inp = {'paper': pids, 'author': aids}, sampled_depth = 3, sampled_number = 100)\n",
      "  File \"../utils.py\", line 294, in sample_subgraph\n",
      "    if decode(source_key)[0] in tesrt:\n",
      "  File \"/home/ziniu/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "KeyboardInterrupt\n",
      "  File \"../utils.py\", line 185, in decode\n",
      "    return np.array([s[:idx], s[idx+1:]], dtype=float)\n",
      "  File \"<ipython-input-6-2fb815af380e>\", line 42, in ad_sample\n",
      "    to_torch(feature, times, edge_list, graph)\n",
      "KeyboardInterrupt\n",
      "  File \"../utils.py\", line 341, in to_torch\n",
      "    node_feature = torch.FloatTensor(node_feature)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a381c8b2210e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaper_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             node_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n\u001b[0;32m---> 30\u001b[0;31m                                    edge_time.to(device), edge_index.to(device), edge_type.to(device))\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mtrain_paper_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtrain_author_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d33d05f851f9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, node_feature, node_type, edge_time, edge_index, edge_type)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mmeta_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmeta_xs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/heter/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, node_inp, node_type, edge_index, edge_type, edge_time)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         return self.propagate(edge_index, node_inp=node_inp, node_type=node_type, \\\n\u001b[0;32m--> 114\u001b[0;31m                               edge_type=edge_type, edge_time = edge_time)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_inp_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_inp_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_type_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_type_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch_geometric/nn/conv/message_passing.py\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mupdate_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__update_args__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmessage_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mupdate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/heter/model.py\u001b[0m in \u001b[0;36mmessage\u001b[0;34m(self, edge_index_i, node_inp_i, node_inp_j, node_type_i, node_type_j, edge_type, edge_time, num_nodes)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0matts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mres_att\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_inp_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mres_val\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_inp_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msource_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stats = []\n",
    "pool = mp.Pool(4)\n",
    "process_ids = np.arange(batch_num // 4)\n",
    "st = time.time()\n",
    "jobs = prepare_data(pool, process_ids)\n",
    "train_step = 1500\n",
    "best_val   = 0\n",
    "res = []\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "for epoch in np.arange(epoch_num)+1:\n",
    "    '''\n",
    "        Prepare Training and Validation Data\n",
    "    '''\n",
    "    train_data = [job.get() for job in jobs[:-1]]\n",
    "    valid_data = jobs[-1].get()\n",
    "    pool.terminate()\n",
    "    pool.join()\n",
    "    pool = mp.Pool(4)\n",
    "    jobs = prepare_data(pool, process_ids)\n",
    "    et = time.time()\n",
    "    print('Data Preparation: %.1fs' % (et - st))\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    torch.cuda.empty_cache()\n",
    "    for batch in np.arange(4):\n",
    "        for node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel in train_data:\n",
    "            node_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                                   edge_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "            train_paper_vecs = []\n",
    "            train_author_vecs = []\n",
    "            train_size  = []\n",
    "            for p_id in ylabel:\n",
    "                al = ylabel[p_id]\n",
    "                train_paper_vecs +=  [node_rep[p_id].repeat(len(al), 1)]\n",
    "                train_author_vecs += [node_rep[al]]\n",
    "                train_size  += [len(al)]\n",
    "            train_paper_vecs  = torch.cat(train_paper_vecs).to(device)\n",
    "            train_author_vecs = torch.cat(train_author_vecs).to(device)\n",
    "            res = matcher.forward(train_author_vecs, train_paper_vecs, pair=True)\n",
    "            loss = mask_softmax(res, train_size)\n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            torch.cuda.empty_cache()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses += [loss.cpu().detach().tolist()]\n",
    "            train_step += 1\n",
    "            scheduler.step(train_step)\n",
    "            del loss, res, node_rep\n",
    "            \n",
    "    '''\n",
    "        Valid\n",
    "    '''\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel = valid_data\n",
    "        node_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "                                   edge_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "        valid_paper_vecs = []\n",
    "        valid_author_vecs = []\n",
    "        valid_size  = []\n",
    "        valid_label = []\n",
    "        for p_id in ylabel:\n",
    "            al = ylabel[p_id]\n",
    "            valid_paper_vecs +=  [node_rep[p_id].repeat(len(al), 1)]\n",
    "            valid_author_vecs += [node_rep[al]]\n",
    "            valid_size  += [len(al)]\n",
    "            label = torch.zeros(len(al))\n",
    "            label[0] = 1\n",
    "            valid_label += [label]\n",
    "        valid_paper_vecs  = torch.cat(valid_paper_vecs).to(device)\n",
    "        valid_author_vecs = torch.cat(valid_author_vecs).to(device)\n",
    "        res = matcher.forward(valid_author_vecs, valid_paper_vecs, pair=True)\n",
    "        loss = mask_softmax(res, valid_size)\n",
    "        valid_res = []\n",
    "        ser = 0\n",
    "        for s, l in zip(valid_size, valid_label):\n",
    "            p = res[ser: ser + s]\n",
    "            r = l[p.argsort(descending = True)]\n",
    "            valid_res += [r.cpu().detach().tolist()]\n",
    "            ser += s\n",
    "        valid_ndcg = np.average([ndcg_at_k(resi, len(resi)) for resi in valid_res])\n",
    "        if valid_ndcg > best_val:\n",
    "            best_val = valid_ndcg\n",
    "            torch.save(model, './save/gat.pt')\n",
    "        st = time.time()\n",
    "        print((\"Epoch: %d (%.1fs)  LR: %.5f Train Loss: %.2f  Valid Loss: %.2f  Valid NDCG: %.4f\") % \\\n",
    "              (epoch, (st-et), optimizer.param_groups[0]['lr'], np.average(train_losses), loss.cpu().detach().tolist(),\\\n",
    "              valid_ndcg))\n",
    "\n",
    "#         if epoch % 5 == 0:\n",
    "#             '''\n",
    "#                 Test\n",
    "#             '''\n",
    "#             node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel = test_data\n",
    "#             node_rep = gnn.forward(node_feature.to(device), node_type.to(device), \\\n",
    "#                                    edge_time.to(device), edge_index.to(device), edge_type.to(device))\n",
    "#             test_paper_vecs = []\n",
    "#             test_author_vecs = []\n",
    "#             test_size  = []\n",
    "#             test_label = []\n",
    "#             for p_id in ylabel:\n",
    "#                 al = ylabel[p_id]\n",
    "#                 test_paper_vecs +=  [node_rep[p_id].repeat(len(al), 1)]\n",
    "#                 test_author_vecs += [node_rep[al]]\n",
    "#                 test_size  += [len(al)]\n",
    "#                 label = torch.zeros(len(al))\n",
    "#                 label[0] = 1\n",
    "#                 test_label += [label]\n",
    "#             test_paper_vecs  = torch.cat(test_paper_vecs).to(device)\n",
    "#             test_author_vecs = torch.cat(test_author_vecs).to(device)\n",
    "#             res = matcher.forward(test_author_vecs, test_paper_vecs, pair=True)\n",
    "#             loss = mask_softmax(res, test_size)\n",
    "#             test_res = []\n",
    "#             ser = 0\n",
    "#             for s, l in zip(test_size, test_label):\n",
    "#                 p = res[ser: ser + s]\n",
    "#                 r = l[p.argsort(descending = True)]\n",
    "#                 test_res += [r.cpu().detach().tolist()]\n",
    "#                 ser += s\n",
    "#             test_ndcg = np.average([ndcg_at_k(resi, len(resi)) for resi in test_res])\n",
    "#             print(test_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_label), len(valid_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
