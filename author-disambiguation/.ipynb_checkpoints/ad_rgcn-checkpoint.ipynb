{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/datadrive/data/'\n",
    "batch_size = 512\n",
    "batch_num  = 128\n",
    "epoch_num  = 100\n",
    "samp_num   = 7\n",
    "\n",
    "device = torch.device(\"cuda:2\")\n",
    "graph = dill.load(open(data_dir + 'graph.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = {t: True for t in graph.times if t != None and t <= 2015}\n",
    "valid_range = {t: True for t in graph.times if t != None and (t > 2015) & (t < 2018)}\n",
    "test_range  = {t: True for t in graph.times if t != None and t >= 2018}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author Disambiguation\n",
    "'''\n",
    "author_dict = dill.load(open(data_dir + 'author_dict.pk', 'rb'))\n",
    "ds_authors  = [[graph.node_forward['author'][ai] for ai in author_dict[k]] \\\n",
    "                   for k in author_dict if len(author_dict[k]) > 2]\n",
    "\n",
    "len_author = [len(author_dict[k]) for k in author_dict if len(author_dict[k]) > 1]\n",
    "# sb.distplot(np.log(len_author) / np.log(10))\n",
    "# plt.xticks(np.arange(4), [1, 10, 100, 1000])\n",
    "# plt.xlabel('Same-name Author Number', fontsize = 15)\n",
    "# plt.show()\n",
    "train_pairs = {}\n",
    "valid_pairs = {}\n",
    "test_pairs  = {}\n",
    "\n",
    "train_papers = {_time: {} for _time in train_range}\n",
    "valid_papers = {_time: {} for _time in valid_range}\n",
    "test_papers  = {_time: {} for _time in test_range}\n",
    "\n",
    "for ser, same_name_author_list in tqdm(enumerate(ds_authors), total = len(ds_authors)):\n",
    "    same_name_author_list = np.array(same_name_author_list)\n",
    "    for author_id, author in enumerate(same_name_author_list):\n",
    "        pem_ids = list(range(len(same_name_author_list)))\n",
    "        pem_ids.remove(author_id)\n",
    "        for p_id in graph.edge_list['author']['paper']['rev_AP_write'][author]:\n",
    "            _time = graph.edge_list['author']['paper']['rev_AP_write'][author][paper]\n",
    "            al = same_name_author_list[np.array([author_id] + pem_ids)]\n",
    "            if _time in train_range:\n",
    "                if p_id not in train_pairs:\n",
    "                    train_pairs[p_id] = []\n",
    "                train_pairs[p_id] += [al]\n",
    "                train_papers[_time][p_id] = True\n",
    "            elif _time in valid_range:\n",
    "                if p_id not in valid_pairs:\n",
    "                    valid_pairs[p_id] = []\n",
    "                valid_pairs[p_id] += [al]\n",
    "                valid_papers[_time][p_id] = True\n",
    "            else:\n",
    "                if p_id not in test_pairs:\n",
    "                    test_pairs[p_id] = []\n",
    "                test_pairs[p_id] += [al]\n",
    "                test_papers[_time][p_id] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ad_sample(seed, papers, pairs, t_range, batch_size, test = False):\n",
    "    np.random.seed(seed)\n",
    "    _time = np.random.choice(list(papers.keys()))\n",
    "    pids = np.array(papers[_time])[np.random.choice(len(papers[_time]), batch_size, replace = False)]\n",
    "    aids = []\n",
    "    edge = defaultdict(lambda: {})\n",
    "    eals = []\n",
    "    for x_id, p_id in enumerate(pids):\n",
    "        als = pairs[p_id]\n",
    "        for al in als:\n",
    "            for a_id in al:\n",
    "                if a_id not in aids:\n",
    "                    aids += [a_id]\n",
    "            eal = [fids.index(al[0])]\n",
    "            edge[x_id][eal[-1]] = 1\n",
    "            for a_id in al[1:]:\n",
    "                eal += [fids.index(a_id)]\n",
    "                edge[x_id][eal[-1]] = 0\n",
    "            eals += [[x_id, eal]]\n",
    "    pids = np.stack([pids, np.repeat([_time], batch_size)]).T\n",
    "    aids = np.stack([aids, np.repeat([_time], len(aids))]).T\n",
    " \n",
    "    feature, times, edge_list = sample_subgraph(graph, t_range, \\\n",
    "                inp = {'paper': pids, 'author': fids}, sampled_depth = 4, sampled_number = 128)\n",
    "\n",
    "    el = []\n",
    "    for i in edge_list['paper']['author']['AP_write']:\n",
    "        if i[0] in edge and i[1] in edge[i[0]]:\n",
    "            continue\n",
    "        el += [i]\n",
    "    edge_list['paper']['author']['AP_write'] = el\n",
    "\n",
    "    el = []\n",
    "    for i in edge_list['author']['paper']['rev_AP_write']:\n",
    "        if i[1] in edge and i[0] in edge[i[1]]:\n",
    "            continue\n",
    "        el += [i]\n",
    "    edge_list['author']['paper']['rev_AP_write'] = el\n",
    "    \n",
    "    \n",
    "    node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict = \\\n",
    "            to_torch(feature, times, edge_list)\n",
    "    '''\n",
    "        Trace the paper_id and field_id by its own index plus the type start index\n",
    "    '''\n",
    "    paper_ids = np.arange(len(pids)) + node_dict['paper'][0]\n",
    "    author_ids = np.arange(len(aids)) + node_dict['author'][0]\n",
    "    ylabel = {}\n",
    "    for x_id in eals:\n",
    "        ylabel[x_id + node_dict['paper'][0]] = np.array(eals[x_id]) + node_dict['paper'][0]\n",
    "    return node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel\n",
    "    \n",
    "def prepare_data(pool, process_ids):\n",
    "    jobs = []\n",
    "    for process_id in process_ids[:-1]:\n",
    "        p = pool.apply_async(ad_sample, args=(np.random.randint(2**32 - 1), train_papers, \\\n",
    "                                               train_pairs, train_range, batch_size))\n",
    "        jobs.append(p)\n",
    "    p = pool.apply_async(ad_sample, args=(np.random.randint(2**32 - 1), valid_papers, \\\n",
    "                                           valid_pairs, valid_range, batch_size))\n",
    "    jobs.append(p)\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import RGCNConv\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, n_hid, n_layers, num_relations, dropout = 0.5):\n",
    "        super(GNN, self).__init__()\n",
    "        self.gcs = nn.ModuleList()\n",
    "        self.adapt = nn.Linear(n_hid, n_hid // 2)\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "        for l in range(n_layers):\n",
    "            self.gcs.append(RGCNConv(n_hid // 2, n_hid // 2, num_relations, num_bases = int(math.sqrt(n_hid))))\n",
    "    def set_device(self, device):\n",
    "        self.device = device\n",
    "        for gc in self.gcs:\n",
    "            gc.device = device\n",
    "    def forward(self, node_feature, edge_index, edge_type):\n",
    "        meta_xs = self.drop(F.elu(self.adapt(node_feature)))\n",
    "        for gc in self.gcs:\n",
    "            meta_xs = self.drop(gc(meta_xs, edge_index, edge_type))\n",
    "        return meta_xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = GNN(400).to(device)\n",
    "matcher = Matcher(200, n_heads = 8).to(device)\n",
    "model = nn.Sequential(gnn, matcher)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 2000, eta_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_softmax(pred, size):\n",
    "    loss = 0\n",
    "    stx = 0\n",
    "    for l in size:\n",
    "        loss += torch.log_softmax(pred[stx: stx + l], dim=-1)[0] / l\n",
    "        stx += l\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "pool = mp.Pool(16)\n",
    "process_ids = np.arange(batch_num // 8)\n",
    "st = time.time()\n",
    "jobs = prepare_data(pool, process_ids)\n",
    "train_step = 3000\n",
    "best_val   = 0\n",
    "res = []\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "for epoch in np.arange(epoch_num)+1:\n",
    "    '''\n",
    "        Prepare Training and Validation Data\n",
    "    '''\n",
    "    train_data = [job.get() for job in jobs[:-1]]\n",
    "    valid_data = jobs[-1].get()\n",
    "    pool.terminate()\n",
    "    pool.join()\n",
    "    pool = mp.Pool(16)\n",
    "    jobs = prepare_data(pool, process_ids)\n",
    "    et = time.time()\n",
    "    print('Data Preparation: %.1fs' % (et - st))\n",
    "    \n",
    "    \n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for batch in np.arange(8):\n",
    "        for node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel in train_data:\n",
    "            node_rep = gnn.forward(node_feature.to(device), edge_index.to(device), edge_type.to(device))\n",
    "            train_paper_vecs = []\n",
    "            train_author_vecs = []\n",
    "            train_size  = []\n",
    "            for x_id in ylabel:\n",
    "                for al in ylabel[x_id]:\n",
    "                    al = train_pairs[_id]\n",
    "                    train_paper_vecs +=  [node_rep[x_id].repeat(len(al), 1)]\n",
    "                    train_author_vecs += [node_rep[al]]\n",
    "                    train_size  += [len(al)]\n",
    "            train_paper_vecs  = torch.cat(train_paper_vecs).to(device)\n",
    "            train_author_vecs = torch.cat(train_author_vecs).to(device)\n",
    "            res = matcher.forward(train_author_vecs, train_paper_vecs, pair=True)\n",
    "            loss = mask_softmax(res, train_size)\n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses += [loss.cpu().detach().tolist()]\n",
    "            train_step += 1\n",
    "            scheduler.step(train_step)\n",
    "    '''\n",
    "        Valid\n",
    "    '''\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel = valid_data\n",
    "        node_rep = gnn.forward(node_feature.to(device), edge_index.to(device), edge_type.to(device))\n",
    "        valid_paper_vecs = []\n",
    "        valid_author_vecs = []\n",
    "        valid_size  = []\n",
    "        valid_label = []\n",
    "        for x_id in ylabel:\n",
    "            for al in ylabel[x_id]:\n",
    "                al = valid_pairs[_id]\n",
    "                valid_paper_vecs +=  [node_rep[x_id].repeat(len(al), 1)]\n",
    "                valid_author_vecs += [node_rep[al]]\n",
    "                valid_size  += [len(al)]\n",
    "                label = torch.zeros(len(al))\n",
    "                label[0] = 1\n",
    "                valid_label += [label]\n",
    "        valid_paper_vecs  = torch.cat(valid_paper_vecs).to(device)\n",
    "        valid_author_vecs = torch.cat(valid_author_vecs).to(device)\n",
    "        res = matcher.forward(valid_author_vecs, valid_paper_vecs, pair=True)\n",
    "        loss = mask_softmax(res, valid_size)\n",
    "        valid_res = []\n",
    "        ser = 0\n",
    "        for s in valid_size:\n",
    "            p = res[ser: ser + s]\n",
    "            l = valid_label[ser: ser + s]\n",
    "            r = l[p.argsort(descending = True)]\n",
    "            valid_res += [r.cpu().detach().tolist()]\n",
    "            ser += s\n",
    "        valid_ndcg = np.average([ndcg_at_k(resi, len(resi)) for resi in valid_res])\n",
    "        if valid_ndcg > best_val:\n",
    "            best_val = valid_ndcg\n",
    "            torch.save(model, './save/gat.pt')\n",
    "        st = time.time()\n",
    "        print((\"Epoch: %d (%.1fs)  LR: %.5f Train Loss: %.2f  Valid Loss: %.2f  Valid NDCG: %.4f\") % \\\n",
    "              (epoch, (st-et), optimizer.param_groups[0]['lr'], np.average(train_losses), loss.cpu().detach().tolist(),\\\n",
    "              valid_ndcg))\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            '''\n",
    "                Test\n",
    "            '''\n",
    "            node_feature, node_type, edge_time, edge_index, edge_type, field_ids, paper_ids, ylabel = test_data\n",
    "            node_rep = gnn.forward(node_feature.to(device), edge_index.to(device))\n",
    "            test_paper_vecs = []\n",
    "            test_author_vecs = []\n",
    "            test_size  = []\n",
    "            test_label = []\n",
    "            for x_id in ylabel:\n",
    "                for al in ylabel[x_id]:\n",
    "                    al = test_pairs[_id]\n",
    "                    test_paper_vecs +=  [node_rep[x_id].repeat(len(al), 1)]\n",
    "                    test_author_vecs += [node_rep[al]]\n",
    "                    test_size  += [len(al)]\n",
    "                    label = torch.zeros(len(al))\n",
    "                    label[0] = 1\n",
    "                    test_label += [label]\n",
    "            test_paper_vecs  = torch.cat(test_paper_vecs).to(device)\n",
    "            test_author_vecs = torch.cat(test_author_vecs).to(device)\n",
    "            res = matcher.forward(test_author_vecs, test_paper_vecs, pair=True)\n",
    "            loss = mask_softmax(res, test_size)\n",
    "            test_res = []\n",
    "            ser = 0\n",
    "            for s in test_size:\n",
    "                p = res[ser: ser + s]\n",
    "                l = test_label[ser: ser + s]\n",
    "                r = l[p.argsort(descending = True)]\n",
    "                test_res += [r.cpu().detach().tolist()]\n",
    "                ser += s\n",
    "            test_ndcg = np.average([ndcg_at_k(resi, len(resi)) for resi in test_res])\n",
    "            print(test_ndcg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
